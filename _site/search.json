[
  {
    "objectID": "instructions/overview.html",
    "href": "instructions/overview.html",
    "title": "Project instruction:",
    "section": "",
    "text": "Author(s): Dr. H and Gerard Pendleton Thurston the 4th\nNote: You can delete this folder and remove it from the project website once you have read and understood the instructions. It shouldn’t be part of your final submission.\nAudio instructions:\nIf you want, you can listen to the instructions:\n\n\n\n Source: Text-to-speech conversion done with Amazon Polly on AWS \nNote: These audio instructions should not be included in your final submission or repository, once you are done wiht them, please delete the files and remove them from the website."
  },
  {
    "objectID": "instructions/overview.html#python-package-optional",
    "href": "instructions/overview.html#python-package-optional",
    "title": "Project instruction:",
    "section": "Python package (optional)",
    "text": "Python package (optional)\nThis is an optional component of the project, if you’d like, you can create a dedicated Python package for your project. The source folder for this package should be included in the root of your repository, and it can be imported into your processing scripts used in the various technical-details sections. If you create a package, it should be well-documented, with an additional tab on the navigation bar for the package documentation.\nWhile a package would typically have its own GitHub repository, for this project, please include it within the same repository.\nThe skeleton for the package is not provided in the repo, but you can recycle what you created in past assignments."
  },
  {
    "objectID": "instructions/overview.html#select-a-broad-topic-area",
    "href": "instructions/overview.html#select-a-broad-topic-area",
    "title": "Project instruction:",
    "section": "Select a broad topic area",
    "text": "Select a broad topic area\nStart by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/overview.html#narrow-your-focus",
    "href": "instructions/overview.html#narrow-your-focus",
    "title": "Project instruction:",
    "section": "Narrow your focus",
    "text": "Narrow your focus\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/overview.html#data-science-questions",
    "href": "instructions/overview.html#data-science-questions",
    "title": "Project instruction:",
    "section": "Data Science Questions",
    "text": "Data Science Questions\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/overview.html#repository-setup",
    "href": "instructions/overview.html#repository-setup",
    "title": "Project instruction:",
    "section": "Repository Setup",
    "text": "Repository Setup\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/overview.html#expectations-for-github-usage",
    "href": "instructions/overview.html#expectations-for-github-usage",
    "title": "Project instruction:",
    "section": "Expectations for GitHub Usage",
    "text": "Expectations for GitHub Usage\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n1. Use a Logical Repository Structure\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n2. Commit Regularly\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n3. Data Storage\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n4. Syncing with GU Domains\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n5. Code Documentation\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "href": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "title": "Project instruction:",
    "section": "Collaboration in Groups (If Applicable)",
    "text": "Collaboration in Groups (If Applicable)\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/overview.html#website-development",
    "href": "instructions/overview.html#website-development",
    "title": "Project instruction:",
    "section": "Website Development",
    "text": "Website Development\nIt is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/overview.html#website-hosting",
    "href": "instructions/overview.html#website-hosting",
    "title": "Project instruction:",
    "section": "Website Hosting",
    "text": "Website Hosting\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/overview.html#the-two-audiences",
    "href": "instructions/overview.html#the-two-audiences",
    "title": "Project instruction:",
    "section": "The two audiences",
    "text": "The two audiences\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/overview.html#get-started-early",
    "href": "instructions/overview.html#get-started-early",
    "title": "Project instruction:",
    "section": "Get started early",
    "text": "Get started early\nRemember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/overview.html#graduate-level-work",
    "href": "instructions/overview.html#graduate-level-work",
    "title": "Project instruction:",
    "section": "Graduate level work",
    "text": "Graduate level work\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "Project instruction:",
    "section": "The intersection of skills and domain knowledge",
    "text": "The intersection of skills and domain knowledge\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/overview.html#what-is-impact",
    "href": "instructions/overview.html#what-is-impact",
    "title": "Project instruction:",
    "section": "What is “impact”?",
    "text": "What is “impact”?\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/overview.html#what-makes-a-good-research-project",
    "href": "instructions/overview.html#what-makes-a-good-research-project",
    "title": "Project instruction:",
    "section": "What makes a good research project?",
    "text": "What makes a good research project?\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/overview.html#time-management",
    "href": "instructions/overview.html#time-management",
    "title": "Project instruction:",
    "section": "Time management",
    "text": "Time management\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/overview.html#visualization-guidelines",
    "href": "instructions/overview.html#visualization-guidelines",
    "title": "Project instruction:",
    "section": "Visualization guidelines",
    "text": "Visualization guidelines\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/overview.html#coding",
    "href": "instructions/overview.html#coding",
    "title": "Project instruction:",
    "section": "Coding",
    "text": "Coding\n\nPractice First\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\nPrototype and develop on a small data set\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/overview.html#debugging",
    "href": "instructions/overview.html#debugging",
    "title": "Project instruction:",
    "section": "Debugging",
    "text": "Debugging\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/overview.html#file-types",
    "href": "instructions/overview.html#file-types",
    "title": "Project instruction:",
    "section": "File Types",
    "text": "File Types\nYou can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/overview.html#quarto-includes",
    "href": "instructions/overview.html#quarto-includes",
    "title": "Project instruction:",
    "section": "Quarto Includes",
    "text": "Quarto Includes\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\nWhy Use Quarto Includes?\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/overview.html#citation",
    "href": "instructions/overview.html#citation",
    "title": "Project instruction:",
    "section": "Citation",
    "text": "Citation\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/overview.html#acceptable-use-cases",
    "href": "instructions/overview.html#acceptable-use-cases",
    "title": "Project instruction:",
    "section": "Acceptable use cases",
    "text": "Acceptable use cases\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/overview.html#unacceptable-use-cases",
    "href": "instructions/overview.html#unacceptable-use-cases",
    "title": "Project instruction:",
    "section": "Unacceptable use cases",
    "text": "Unacceptable use cases\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\nEffect on grade\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\nPlagiarism investigation\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cell Block Tango:  Steps of Misconduct, Over-Policing, and Cycles of Racial Injustice in Chicago’s Criminal Justice System",
    "section": "",
    "text": "Chicago serves as both a microcosm and a magnifying glass for the systemic failures embedded in the U.S. criminal justice system. For five consecutive years, the city has led the nation in exonerations, a grim testament to entrenched racial bias, police misconduct, and flawed policing practices1. Yet, Chicago is more than an anomaly—it is a focal point within a broader, statewide crisis, and a reflection of nationwide patterns.\nThis project expands the lens to examine Illinois as a whole, using Chicago as a critical case study to investigate how wrongful convictions and over-policing disproportionately devastate Black and Latino communities. By analyzing exoneration patterns, arrest rates, incarceration data, and demographics at the county level, this work uncovers the ways systemic injustices ripple across Illinois. Chicago’s staggering share of exonerations exposes the extreme, but it also reflects a deeper, structural issue that transcends city limits."
  },
  {
    "objectID": "index.html#introduction-motivation",
    "href": "index.html#introduction-motivation",
    "title": "Cell Block Tango:  Steps of Misconduct, Over-Policing, and Cycles of Racial Injustice in Chicago’s Criminal Justice System",
    "section": "",
    "text": "Chicago serves as both a microcosm and a magnifying glass for the systemic failures embedded in the U.S. criminal justice system. For five consecutive years, the city has led the nation in exonerations, a grim testament to entrenched racial bias, police misconduct, and flawed policing practices1. Yet, Chicago is more than an anomaly—it is a focal point within a broader, statewide crisis, and a reflection of nationwide patterns.\nThis project expands the lens to examine Illinois as a whole, using Chicago as a critical case study to investigate how wrongful convictions and over-policing disproportionately devastate Black and Latino communities. By analyzing exoneration patterns, arrest rates, incarceration data, and demographics at the county level, this work uncovers the ways systemic injustices ripple across Illinois. Chicago’s staggering share of exonerations exposes the extreme, but it also reflects a deeper, structural issue that transcends city limits."
  },
  {
    "objectID": "index.html#research-questions",
    "href": "index.html#research-questions",
    "title": "Cell Block Tango:  Steps of Misconduct, Over-Policing, and Cycles of Racial Injustice in Chicago’s Criminal Justice System",
    "section": "Research Questions",
    "text": "Research Questions\n\nWhat patterns emerge in exoneration data across Illinois counties, and how do they reflect systemic failures in the justice system?\n\nWhy does Chicago account for such a disproportionate share of wrongful convictions in Illinois?\n\nDoes over-policing disproportionately target marginalized communities, particularly Black and Latino populations?\n\nWhat role does police misconduct play in wrongful convictions across Illinois? Is there a relationship between misconduct and race?\n\nCan data-driven models identify predictive patterns of wrongful convictions and over-policing across Illinois counties?"
  },
  {
    "objectID": "index.html#visualizing-injustice-exonerations-and-arrests-across-illinois",
    "href": "index.html#visualizing-injustice-exonerations-and-arrests-across-illinois",
    "title": "Cell Block Tango:  Steps of Misconduct, Over-Policing, and Cycles of Racial Injustice in Chicago’s Criminal Justice System",
    "section": "Visualizing Injustice: Exonerations and Arrests Across Illinois",
    "text": "Visualizing Injustice: Exonerations and Arrests Across Illinois\n\nFigure 1: Log-Scaled Exonerations by County (including and excluding Cook County).\n\nFigure 2: Log-Scaled Arrests by County (including and excluding Cook County)."
  },
  {
    "objectID": "index.html#citizens-perceptions-of-over-and-under-policing-a-look-at-race-ethnicity-and-community-characteristics",
    "href": "index.html#citizens-perceptions-of-over-and-under-policing-a-look-at-race-ethnicity-and-community-characteristics",
    "title": "Cell Block Tango:  Steps of Misconduct, Over-Policing, and Cycles of Racial Injustice in Chicago’s Criminal Justice System",
    "section": "Citizens’ Perceptions of Over-and Under-Policing: A Look at Race, Ethnicity, and Community Characteristics",
    "text": "Citizens’ Perceptions of Over-and Under-Policing: A Look at Race, Ethnicity, and Community Characteristics\nBoehme, Cann, and Isom (2020) examine how race, ethnicity, and community characteristics influence public perceptions of policing, shedding light on the over-under-policing paradox. The authors demonstrate how marginalized communities—particularly Black and Latino neighborhoods—experience aggressive over-policing through surveillance and frequent police encounters, while simultaneously being under-protected when it comes to safety and support. This dual dynamic fosters a deep sense of distrust, isolation, and legal cynicism, perpetuating systemic divides. Importantly, their findings emphasize that race remains a consistent predictor of these perceptions, even after accounting for economic conditions and neighborhood characteristics like concentrated disadvantage and ethnic diversity2.\nUsing data from the PHDCN Community Survey, which captures responses from 8,782 participants across 343 Chicago neighborhoods, the authors employ Hierarchical Linear Modeling (HLM) to analyze how individual- and community-level factors shape perceptions of policing. Their results reveal that while neighborhood context plays a significant role, racialized policing persists, with Black and Latino residents disproportionately reporting over-policing experiences regardless of socioeconomic conditions2.\nThe study is particularly important to this project for two key reasons. First, it highlights how structural factors—such as poverty, race, and neighborhood composition—intersect to produce divergent policing experiences. This intersectionality aligns with the project’s focus on exploring the racialized and systemic roots of policing disparities in Chicago and Illinois as a whole. Second, the research exposes a feedback loop in which over-surveillance and under-service deepen distrust and isolation within marginalized communities. This insight reinforces the project’s aim to uncover how systemic racial biases and inequities sustain harmful policing cycles, particularly in Black and Latino neighborhoods across Chicago2."
  },
  {
    "objectID": "index.html#over-policing-black-bodies-the-need-for-multidimensional-and-transformative-reforms",
    "href": "index.html#over-policing-black-bodies-the-need-for-multidimensional-and-transformative-reforms",
    "title": "Cell Block Tango:  Steps of Misconduct, Over-Policing, and Cycles of Racial Injustice in Chicago’s Criminal Justice System",
    "section": "Over-Policing Black Bodies: The Need for Multidimensional and Transformative Reforms",
    "text": "Over-Policing Black Bodies: The Need for Multidimensional and Transformative Reforms\nJones-Brown and Williams (2021) provide a critical analysis of over-policing in Black communities, demonstrating how racial disparities in police interactions—such as traffic stops, consent searches, and arrests—are rooted in systemic biases that criminalize Black identity. Drawing on both quantitative data and qualitative accounts, the authors reveal the psychological toll of aggressive policing, which fosters a feedback loop of fear, mistrust, and isolation. High-profile cases like the deaths of George Floyd and Breonna Taylor serve as stark representations of this broader issue, amplified by social media as a tool for documenting police brutality. Meanwhile, mainstream media often mischaracterizes Black-led protests, further reinforcing harmful stereotypes. By situating these patterns within a historical context—referencing the 1967 Katzenbach Report and the War on Drugs—the authors expose the longstanding structural roots of racialized policing, particularly relevant to cities like Chicago3.\nJones-Brown and Williams’ study is integral context for the research for several reasons. First, it underscores how structural racism and historical policy decisions continue to drive over-policing in Black communities, a phenomenon reflected in Chicago’s policing data. Second, it highlights the dual perspective of quantitative evidence and qualitative lived experiences—an approach I aim to incorporate into my analysis to capture both the scope and human impact of over-policing. Lastly, their reform recommendations offer a foundation for addressing systemic disparities, aligning with my goal of examining how community-driven solutions can disrupt harmful policing cycles3. By situating over-policing within a historical and structural framework, this study provides a comprehensive understanding of the issue and offers pathways for transformative change. These insights directly support the project’s examination of how racialized policing practices and systemic corruption perpetuate injustice in Chicago’s Black communities3."
  },
  {
    "objectID": "index.html#chicago-ranks-no.-1-in-exonerations-for-5th-year-in-a-row-accounting-for-more-than-half-of-national-total",
    "href": "index.html#chicago-ranks-no.-1-in-exonerations-for-5th-year-in-a-row-accounting-for-more-than-half-of-national-total",
    "title": "Cell Block Tango:  Steps of Misconduct, Over-Policing, and Cycles of Racial Injustice in Chicago’s Criminal Justice System",
    "section": "Chicago Ranks No. 1 in Exonerations for 5th Year in a Row, Accounting for More Than Half of National Total",
    "text": "Chicago Ranks No. 1 in Exonerations for 5th Year in a Row, Accounting for More Than Half of National Total\nHeather Cherone (2023) highlights Chicago’s troubling distinction as the nation’s leader in wrongful convictions, with Cook County alone accounting for over half of all U.S. exonerations in 2022. Drawing on data from the National Registry of Exonerations, the analysis uncovers systemic patterns of police misconduct, racial disparities, and institutional resistance to reform—issues central to this project’s examination of injustice within Illinois’ criminal justice system1.\nThe article reveals the disproportionate toll of wrongful convictions on Black and Latino communities, underscoring the racialized nature of policing in Chicago. At the core of this crisis is police misconduct, exemplified by officers like Ronald Watts and Reynaldo Guevara, whose corrupt practices—ranging from extortion to framing suspects—are linked to a significant share of exonerations. These patterns not only devastate individuals and their communities but also impose a staggering financial burden on the city. In 2022, Chicago spent $98 million settling police misconduct cases, with the 2023 budget allocating another $82 million, signaling the immense economic cost of systemic failures. Despite the federal consent decree issued in 2017 to address these issues, the Chicago Police Department remains only 3% compliant, reflecting deep institutional resistance to change1.\nThis analysis provides a critical foundation for the project. By examining exoneration data, it highlights the role of racialized policing practices and systemic misconduct in perpetuating wrongful convictions. Chicago’s prominence in these statistics offers a unique lens through which to explore broader patterns of injustice across Illinois, serving as both a focal point and a microcosm of nationwide systemic failures. Additionally, the data-driven approach of the National Registry of Exonerations aligns closely with this project’s methodology. While not replicating the study, this project builds upon its findings by conducting further analysis of Illinois exoneration trends to uncover how race, police misconduct, and institutional barriers contribute to these systemic injustices."
  },
  {
    "objectID": "technical-details/llm-usage-log.html",
    "href": "technical-details/llm-usage-log.html",
    "title": "LLM Usage Log",
    "section": "",
    "text": "This page serves as a “catch-all” for LLM use cases that don’t involve direct content creation but support the project in other capacities, such as reformatting ideas, commenting code, summarizing sources, and geospatial analysis.\nLLM tools were used in the following ways for the tasks below:"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#brainstorming",
    "href": "technical-details/llm-usage-log.html#brainstorming",
    "title": "LLM Usage Log",
    "section": "Brainstorming",
    "text": "Brainstorming\n\nI initially conceived my project idea focusing on over-policing and the intersection of race and poverty, a topic I’ve always been passionate about. LLM tools helped me refine and narrow the scope to a specific locality—Chicago—to create a more actionable and measurable project."
  },
  {
    "objectID": "technical-details/llm-usage-log.html#literature-review",
    "href": "technical-details/llm-usage-log.html#literature-review",
    "title": "LLM Usage Log",
    "section": "Literature Review",
    "text": "Literature Review\nLLM tools significantly enhanced my literature review process by allowing me to:\n\nSummarize key academic and news sources: ChatGPT assisted in extracting and summarizing core insights from research papers, ensuring the integration of multiple perspectives on racial disparities, socioeconomic factors, and over-policing.\nHighlight key findings: I utilized LLM tools to identify relevant data on wrongful convictions, income inequality, and police misconduct.\nStructure the literature review: The LLM helped reorganize summaries and synthesize the literature into a cohesive narrative, making complex studies more accessible for integration into my project.\nManage citations: LLM tools supported the creation of structured citation summaries, facilitating the development of the references.bib file.\n\nThese applications enabled me to streamline the review process and focus on critical themes like systemic racial biases, socioeconomic vulnerabilities, and justice system reform."
  },
  {
    "objectID": "technical-details/llm-usage-log.html#writing",
    "href": "technical-details/llm-usage-log.html#writing",
    "title": "LLM Usage Log",
    "section": "Writing",
    "text": "Writing\nLLM tools supported the writing process in acceptable use cases:\n\nProofreading and grammar: Grammarly reviewed my text for clarity, grammar, and coherence to ensure professional and academic standards were met.\nVoice consistency: Initially, I wrote much of my prose in first person. I used LLM tools to convert explanations and analyses into the third person for a formal research tone.\nReformatting content: ChatGPT helped restructure lengthy or disjointed sections to improve readability and logical flow."
  },
  {
    "objectID": "technical-details/llm-usage-log.html#code",
    "href": "technical-details/llm-usage-log.html#code",
    "title": "LLM Usage Log",
    "section": "Code",
    "text": "Code\nLLM tools were utilized for technical tasks, including:\n\nCode commenting and documentation: I used LLM tools to generate clear explanations for code sections and functions.\nGeospatial EDA: Since geospatial analysis was not covered in class, LLM tools assisted in guiding me through exploratory data analysis for mapping wrongful convictions and arrest patterns.\nReformatting and explaining code: ChatGPT translated technical code logic into descriptive prose, making it easier to incorporate into the project narrative.\nDebugging support: LLM tools were occasionally used to identify and resolve errors in my Python code.\n\nThese applications ensured that my code was well-documented, accessible, and accurate, particularly in unfamiliar areas like GIS-based EDA."
  },
  {
    "objectID": "technical-details/llm-usage-log.html#design",
    "href": "technical-details/llm-usage-log.html#design",
    "title": "LLM Usage Log",
    "section": "Design",
    "text": "Design\nLLM tools were used for designing and developing the project website, including:\n\nHTML and CSS: ChatGPT supported the creation and refinement of the SCSS and CSS files required for the website design.\nStructural layout: I received guidance on structuring the site for clarity, ensuring it aligned with research presentation goals."
  },
  {
    "objectID": "technical-details/llm-usage-log.html#summary-of-use",
    "href": "technical-details/llm-usage-log.html#summary-of-use",
    "title": "LLM Usage Log",
    "section": "Summary of Use",
    "text": "Summary of Use\nOverall, LLM tools were leveraged in acceptable and responsible ways to:\n\nBrainstorm and refine project scope.\nStreamline the literature review.\nProofread and reformat project writing.\nComment and debug code, including geospatial analyses.\nSupport website design, particularly in areas outside the curriculum.\n\nBy using LLM tools to complement my work, I was able to focus more on critical thinking, analysis, and research outcomes while ensuring a polished and well-documented final project."
  },
  {
    "objectID": "technical-details/data-collection/methods.html",
    "href": "technical-details/data-collection/methods.html",
    "title": "Methods",
    "section": "",
    "text": "The methods combined direct downloads, web scraping, and API-based geocoding to assemble a robust dataset for analysis. The exoneration and arrest datasets served as the foundation, while geocoding and additional scraping added valuable spatial and demographic context.\n\n\nThe Illinois arrest dataset was sourced from the Illinois Criminal Justice Information Authority’s (ICJIA) Arrest Explorer, a platform providing aggregate arrest data from the Criminal History Record Information (CHRI) system—a statewide resource for demographic and offense-related variables.1\nTo ensure privacy and confidentiality, ICJIA applied the following modifications:\n- Counts under 10 are approximated (e.g., 1 for counts 0–4, 6 for counts 5–9),\n- Subtotals, such as arrests by race or county, are accurate within +1/-1, and\n- Statewide totals align exactly with the CHRI database at the time of retrieval, which occurs twice annually.\nFurther, the dataset excludes juvenile arrests, class C misdemeanors, and cases with missing demographic details. For this project, the data was first filtered by race, county, and year, and then downloaded directly to examine patterns relevant to my analysis.2\n\n\n\nThe exoneration dataset was downloaded directly from the National Registry of Exonerations, which collects and publishes searchable, online statistical data and case details for known exonerations of innocent criminal defendants in the United States from 1989 to the present.3\nThe Registry defines exonerations as cases where a person, following new evidence of innocence, is officially cleared through actions like factual declarations of innocence, pardons, or the dismissal/acquittal of charges.4\nTo access the data, a spreadsheet request form had to be submitted, and the dataset was provided under specific conditions to ensure its proper use:\n\nNo retransmission: The spreadsheet or any substantial portion of it cannot be shared with anyone who has not agreed to the conditions.\n\nAdvance notice: The National Registry must be informed in advance of any publication or distribution of data derived from the spreadsheet.\n\nCorrections and additions: Recipients agree to report any errors or missing data they identify to the Registry.\n\n\n\n\nThe population and incarceration data for Illinois counties were obtained by scraping Prison Policy Initiative’s website which provides information on total population and incarcerated populations broken down by race for counties across the United States5.\nTo extract the data, the requests library was used to retrieve the webpage’s HTML content, and BeautifulSoup was employed to parse the HTML and locate the relevant table which was then converted into a Pandas DataFrame for cleaning and analysis. Here is the code used:"
  },
  {
    "objectID": "technical-details/data-collection/methods.html#illinois-arrest-data",
    "href": "technical-details/data-collection/methods.html#illinois-arrest-data",
    "title": "Methods",
    "section": "",
    "text": "The Illinois arrest dataset was sourced from the Illinois Criminal Justice Information Authority’s (ICJIA) Arrest Explorer, a platform providing aggregate arrest data from the Criminal History Record Information (CHRI) system—a statewide resource for demographic and offense-related variables.1\nTo ensure privacy and confidentiality, ICJIA applied the following modifications:\n- Counts under 10 are approximated (e.g., 1 for counts 0–4, 6 for counts 5–9),\n- Subtotals, such as arrests by race or county, are accurate within +1/-1, and\n- Statewide totals align exactly with the CHRI database at the time of retrieval, which occurs twice annually.\nFurther, the dataset excludes juvenile arrests, class C misdemeanors, and cases with missing demographic details. For this project, the data was first filtered by race, county, and year, and then downloaded directly to examine patterns relevant to my analysis.2"
  },
  {
    "objectID": "technical-details/data-collection/methods.html#exoneration-data",
    "href": "technical-details/data-collection/methods.html#exoneration-data",
    "title": "Methods",
    "section": "",
    "text": "The exoneration dataset was downloaded directly from the National Registry of Exonerations, which collects and publishes searchable, online statistical data and case details for known exonerations of innocent criminal defendants in the United States from 1989 to the present.3\nThe Registry defines exonerations as cases where a person, following new evidence of innocence, is officially cleared through actions like factual declarations of innocence, pardons, or the dismissal/acquittal of charges.4\nTo access the data, a spreadsheet request form had to be submitted, and the dataset was provided under specific conditions to ensure its proper use:\n\nNo retransmission: The spreadsheet or any substantial portion of it cannot be shared with anyone who has not agreed to the conditions.\n\nAdvance notice: The National Registry must be informed in advance of any publication or distribution of data derived from the spreadsheet.\n\nCorrections and additions: Recipients agree to report any errors or missing data they identify to the Registry."
  },
  {
    "objectID": "technical-details/data-collection/methods.html#mass-incarceration-racial-geography",
    "href": "technical-details/data-collection/methods.html#mass-incarceration-racial-geography",
    "title": "Methods",
    "section": "",
    "text": "The population and incarceration data for Illinois counties were obtained by scraping Prison Policy Initiative’s website which provides information on total population and incarcerated populations broken down by race for counties across the United States5.\nTo extract the data, the requests library was used to retrieve the webpage’s HTML content, and BeautifulSoup was employed to parse the HTML and locate the relevant table which was then converted into a Pandas DataFrame for cleaning and analysis. Here is the code used:"
  },
  {
    "objectID": "bibliography.html",
    "href": "bibliography.html",
    "title": "Bibliography",
    "section": "",
    "text": "References\n\n1. Jones-Brown, D., & Williams, J. M. (2021). Over-policing Black bodies: The need for multidimensional and transformative reforms. Journal of Ethnicity in Criminal Justice, 19(3-4), 181–187. https://doi.org/10.1080/15377938.2021.1992326\n\n\n2. Coates, T.-N. (2015). The Black Family in the Age of Mass Incarceration. The Atlantic. https://www.theatlantic.com/magazine/archive/2015/10/the-black-family-in-the-age-of-mass-incarceration/403246/\n\n\n3. Boehme, H. M., Cann, D., & Isom, D. A. (2020). Citizens’ Perceptions of Over- and Under-Policing: A Look at Race, Ethnicity, and Community Characteristics. Sage, 32. https://doi.org/10.1177/0011128720974309\n\n\n4. Gesin, J. (2014). Socioeconomic determinants of violent crime rates in the US. Empirical Economic Bulletin, An Undergraduate Journal, 7(1), 1.\n\n\n5. Cherone, H. (2023). Chicago ranks no. 1 in exonerations for 5th year in a row, accounting for more than half of national total: report. In WTTW. \"https://news.wttw.com/2023/05/16/chicago-ranks-no-1-exonerations-5th-year-row-accounting-more-half-national-total-report\"\n\n\n6. University of California Irvine Newkirk Center for Science & Society, & Michigan State University College of Law, U. of M. L. S. (2024). Exoneration Detail List. In The National Registry of Exonerations. https://www.law.umich.edu/special/exoneration/Pages/detaillist.aspx\n\n\n7. University of California Irvine Newkirk Center for Science & Society, & Michigan State University College of Law, U. of M. L. S. (2024). The National Registry of Exonerations - Exoneration Registry. In The National Registry of Exonerations. https://www.law.umich.edu/special/exoneration/Pages/about.aspx\n\n\n8. University of California Irvine Newkirk Center for Science & Society, & Michigan State University College of Law, U. of M. L. S. (2024). The National Registry of Exonerations - Glossary. In The National Registry of Exonerations. https://www.law.umich.edu/special/exoneration/Pages/glossary.aspx\n\n\n9. University of California Irvine Newkirk Center for Science & Society, & Michigan State University College of Law, U. of M. L. S. (2024). The National Registry of Exonerations - Our Mission. In The National Registry of Exonerations. https://www.law.umich.edu/special/exoneration/Pages/mission.aspx\n\n\n10. Initiative, P. P. (2024). Appendix A. Counties – Ratios of Overrepresentation. https://www.prisonpolicy.org/racialgeography/counties.html\n\n\n11. Illinois Criminal Justice Information Authority. (2024). Arrests by race, county, and year. In Arrest Explorer. https://icjia.illinois.gov/arrestexplorer/\n\n\n12. Illinois Criminal Justice Information Authority. (2024). Overview  arrest explorer. In Arrest Explorer. https://icjia.illinois.gov/arrestexplorer/docs/#what-data-is-available\n\n\n13. Survey, I. S. G. (1984). Illinois county boundaries (2.0 ed.). Illinois State Geological Survey. https://clearinghouse.isgs.illinois.edu/data/reference/illinois-county-boundaries-polygons-and-lines\n\n\n14. Li, W., & Ricard, J. (2023). Many Large U.S. Police Agencies Are Missing from FBI Crime Data. In The Marshall Project. https://www.themarshallproject.org/2023/07/13/fbi-crime-rates-data-gap-nibrs\n\n\n15. Woodard, P. L., & Belair, R. R. (1993). Use and Management of Criminal History Record Information: A Comprehensive Report  Bureau of Justice Statistics [.gov]. In Bureau of Justice Statistics. https://bjs.ojp.gov/library/publications/use-and-management-criminal-history-record-information-comprehensive-report\n\n\n16. Pearl, J. (2016). Counterfactuals and their applications. In Causal inference in statistics: A primer. John Wiley & Sons. https://bayes.cs.ucla.edu/PRIMER/ch4-preview.pdf\n\n\n17. Illinois Primary Health Care Association (IPHCA). (2020). Illinois counties by rural/urban classification. https://dph.illinois.gov/content/dam/soi/en/web/idph/files/rur-urb-2021.pdf\n\n\n18. Corporation for Supportive Housing (CSH). (2024). Racial Disparities and Disproportionality Index. Racial Disparities and Disproportionality Index Report. https://www.csh.org/wp-content/uploads/2020/04/RDDI_OverviewHowTo.pdf"
  },
  {
    "objectID": "instructions/website-structure.html",
    "href": "instructions/website-structure.html",
    "title": "Website project structure",
    "section": "",
    "text": "Please at miniumum include the following pages in your website:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\nSupervised-learning\nLLM-usage\nProgress-log\n\n\nPlease adhere closely to this structure, for consistency accross projects.\nSub-sections can be handles as markdown headers in the respective pages.\nYou can add more pages,and if you want, you can merge EDA and unsupervised learning into one page, since the are similar. Or make these section headers in the dropdown menu, for further sub-sections creation.\nFor example:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\n\nClustering\nDimensionality Reduction\n\nSupervised-learning\n\nFeature selection\n\nregression\nclassification\n\nClassification\n\nBinary classification\nMulti-class classification\n\nRegression\n\nLLM-usage\nProgress-log\n\n\nImportant: Exactly what you put on these pages will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\nA skeleton of the recommended version of the website is provided in the github classroom repository.\n./\n├── README.md\n├── _quarto.yml\n├── assets\n│   ├── gu-logo.png\n│   ├── nature.csl\n│   └── references.bib\n├── build.sh\n├── data\n│   ├── processed-data\n│   │   └── countries_population.csv\n│   └── raw-data\n│       └── countries_population.csv\n├── index.qmd\n├── instructions\n│   ├── expectations.qmd\n│   ├── github-usage.qmd\n│   ├── llm-usage.qmd\n│   ├── overview.qmd\n│   ├── quarto-tips.qmd\n│   ├── topic-selection.qmd\n│   └── website-structure.qmd\n├── report\n│   └── report.qmd\n└── technical-details\n    ├── data-cleaning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── data-collection\n    │   ├── closing.qmd\n    │   ├── instructions.qmd\n    │   ├── main.ipynb\n    │   ├── methods.qmd\n    │   └── overview.qmd\n    ├── eda\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── llm-usage-log.qmd\n    ├── progress-log.qmd\n    ├── supervised-learning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    └── unsupervised-learning\n        ├── instructions.qmd\n        └── main.ipynb\nAlways strive to incorporate the following:\n\nStructure: Use clear headings and subheadings to break down each section of your EDA.\nClarity: Provide concise explanations for all tables and visualizations, ensuring they are easy to interpret.\nCode Links: Link to relevant code (e.g., GitHub) or embed code snippets for transparency and reproducibility.\nReproducibility: Make your EDA reproducible by providing access to the dataset, scripts, and tools you used.\nVisualization: Use visualizations to convey key insights\n\n\n\nIt is required that you build your website with Quarto.\n\n\n\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO.\n\n\n\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/website-structure.html#website-development",
    "href": "instructions/website-structure.html#website-development",
    "title": "Website project structure",
    "section": "",
    "text": "It is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/website-structure.html#website-hosting",
    "href": "instructions/website-structure.html#website-hosting",
    "title": "Website project structure",
    "section": "",
    "text": "You MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/website-structure.html#the-two-audiences",
    "href": "instructions/website-structure.html#the-two-audiences",
    "title": "Website project structure",
    "section": "",
    "text": "Knowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/quarto-tips.html",
    "href": "instructions/quarto-tips.html",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton.\n\n\n\n\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/quarto-tips.html#file-types",
    "href": "instructions/quarto-tips.html#file-types",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/quarto-tips.html#quarto-includes",
    "href": "instructions/quarto-tips.html#quarto-includes",
    "title": "Quarto Tips",
    "section": "",
    "text": "Quarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/github-usage.html",
    "href": "instructions/github-usage.html",
    "title": "GitHub",
    "section": "",
    "text": "Your project will be fully transparent, with all source code hosted on GitHub. This platform will serve as the main repository for your project code, documentation, and website. Proper organization and regular updates are key for effective collaboration and project management.\n\nIMPORTANT: Proficiency in GitHub for collaboration is a valuable addition to your resume. Being able to join a team and immediately contribute by solving problems and adding value is a highly sought-after skill. Now is the time to develop this expertise—embrace Git fully, become proficient, and graduate with a critical skill for your future career.\n\n\n\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure.\n\n\n\n\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies.\n\n\n\n\n\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/github-usage.html#repository-setup",
    "href": "instructions/github-usage.html#repository-setup",
    "title": "GitHub",
    "section": "",
    "text": "You MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/github-usage.html#expectations-for-github-usage",
    "href": "instructions/github-usage.html#expectations-for-github-usage",
    "title": "GitHub",
    "section": "",
    "text": "Your grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "href": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "title": "GitHub",
    "section": "",
    "text": "If you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html",
    "href": "technical-details/unsupervised-learning/main.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "This analysis employs unsupervised learning techniques—including Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), K-Means, DBSCAN, and Hierarchical Clustering—to examine the Illinois exoneration dataset. The primary objective is to identify patterns and hidden structures within the data, particularly focusing on how case characteristics, demographic variables (such as race and county), and the number of years lost to wrongful convictions intersect.\nThe analysis is structured as follows:\n1. Dimensionality Reduction: Methods such as PCA and t-SNE are utilized to project high-dimensional data into lower-dimensional spaces, simplifying the visualization of complex relationships while preserving key structural and variance-based insights.\n\nClustering: Clustering techniques—K-Means, DBSCAN, and Hierarchical Clustering—are applied to uncover natural groupings within the dataset and assess whether these clusters align with demographic features like race or case-related factors.\nEvaluation and Interpretation: The performance of each method is evaluated, and clustering results are compared to draw meaningful interpretations. Visualizations are integrated throughout the analysis to enhance clarity and support findings.\n\nThe motivation for this analysis stems from the critical need to uncover systemic patterns in wrongful conviction data. By applying unsupervised learning methods, the investigation aims to reveal relationships and disparities between demographic factors and case outcomes that are not immediately apparent. These insights contribute to a deeper understanding of biases and inequities within exoneration cases and support broader efforts for justice system reform."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#pca-principal-component-analysis",
    "href": "technical-details/unsupervised-learning/main.html#pca-principal-component-analysis",
    "title": "Unsupervised Learning",
    "section": "PCA (Principal Component Analysis)",
    "text": "PCA (Principal Component Analysis)\nPrincipal Component Analysis (PCA) is a statistical technique used to reduce the dimensionality of high-dimensional datasets. It achieves this by identifying the most significant features, known as principal components, through linear transformations. These components capture the maximum variance in the data, allowing for a simplified yet informative representation of complex datasets.\n\n# ---------------------------------------------------------------\n# Unsupervised Learning Implementation\n# Principal Component Analysis (PCA) and clustering methods used\n# in this code are based on demos and labs provided by:\n#\n# Hickman, J. (2024.). Principal Components Analysis and clustering.\n# Georgetown University Centralized Lecture Content.\n# Retrieved from: https://jfh.georgetown.domains/centralized-lecture-content/content/machine-learning/unsupervised-learning/dimensionality-reduction/PCA/notes.html#pca-vs-clustering\n# ---------------------------------------------------------------\n\n# Define utility plotting function\ndef plot_2D(X,color_vector, plot_title):\n    fig, ax = plt.subplots()\n    ax.scatter(X[:,0], X[:,1],c=color_vector, alpha=0.5) #, c=y\n    ax.set(xlabel='PC-1 ', ylabel='PC-2',\n    title= plot_title)\n    ax.grid()\n    # fig.savefig(\"test.png\")\n    plt.show()\n\n# Define variance plot function to visualize variance explained by PCA components\ndef plot_variance_explained(pca):\n    explained_variance_ratio = pca.explained_variance_ratio_\n\n    # Plot explained variance ratio\n    plt.figure(figsize=(8, 6))\n    plt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, marker='o')\n    plt.xlabel('Number of components')\n    plt.ylabel('Explained variance ratio')\n    plt.title('Explained Variance Ratio by Component')\n    plt.show()\n\n    # Plot cumulative explained variance\n    cumulative_variance = np.cumsum(explained_variance_ratio)\n    plt.figure(figsize=(8, 6))\n    plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\n    plt.xlabel('Number of components')\n    plt.ylabel('Cumulative explained variance')\n    plt.title('Cumulative Explained Variance by Component')\n    plt.show()\n\n\nExplained Variance Ratio\nThe explained variance ratio in PCA indicates the proportion of the total variance in the dataset that is captured by each principal component (PC). This metric is essential for determining the optimal number of components to retain during dimensionality reduction. Each principal component captures a fraction of the total variance, with the first principal component (PC-1) explaining the largest share, followed by the second component (PC-2), and so forth. By analyzing the distribution of variance across the components, it becomes possible to identify which components contribute the most meaningful information to the dataset. The cumulative variance** is calculated by summing the explained variance ratios of successive components. This cumulative measure helps determine how many components are necessary to retain a significant portion of the total variance, such as 90% or 95%. Retaining fewer components simplifies the data representation, making it more computationally efficient, while still preserving most of the underlying structure and variability of the original dataset.\n\n# Apply PCA\npca = PCA()\nX_pca = pca.fit_transform(X)\n\n# Print variance explained and cumulative variance by each principal component\nprint(\"Variance explained by each principal component:\")\nprint(pca.explained_variance_ratio_[:10])\n\nprint(\"\\nCumulative variance explained by each principal component:\")\nprint(np.cumsum(pca.explained_variance_ratio_)[:10])\n\n# Plot the variance explained\nplot_variance_explained(pca)\n\nVariance explained by each principal component:\n[0.59465697 0.29997716 0.10536587]\n\nCumulative variance explained by each principal component:\n[0.59465697 0.89463413 1.        ]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first plot, “Explained Variance Ratio by Component,” shows the proportion of variance captured by each principal component. The steep decline in this plot indicates that the first principal component (PC-1) explains the largest portion of the variance, followed by the second component (PC-2). After these first two components, the additional variance explained by subsequent components decreases significantly. This behavior suggests that the majority of the dataset’s structure can be captured by the first two components.\nThe second plot, “Cumulative Explained Variance by Component,” illustrates the total variance explained as additional components are added. The curve rises sharply at the start, with the first two components capturing approximately 90% of the total variance. Beyond the second component, the curve begins to flatten, indicating diminishing returns. This flattening demonstrates that including more components contributes little new information to the overall representation of the data.\nTogether, these plots emphasize the significance of the first two principal components. By focusing on these components, the dimensionality of the data can be effectively reduced while retaining most of its variance. This reduction simplifies computations, decreases model complexity, and enhances the interpretability of visualizations, all without sacrificing critical information.\n\npca = PCA(n_components=2)\nX_pca_2 = pca.fit_transform(X)\n\n# Step 5: Plot 2D results\nplot_2D(X_pca_2, df['race_encoded'], 'Principal Component Analysis Results')\n\n\n\n\n\n\n\n\nThe plot, “Principal Component Analysis Results,” presents the dataset reduced to two dimensions—PC-1 and PC-2—the two principal components that capture the most variance. Each point represents an individual data observation, with colors corresponding to the race_encoded variable.\n\nPC-1 (x-axis) captures the largest portion of the variance, approximately 60%.\n\nPC-2 (y-axis) explains the next largest portion, around 30%.\n\nThe distribution of points shows that much of the variance is concentrated along the PC-1 axis, suggesting that this direction captures the most meaningful structure in the dataset. The spread along the PC-2 axis provides additional separation, though to a lesser extent. The presence of vertical “striping” and overlapping points indicates that the race_encoded variable (color-coded) does not perfectly align with the variance explained by the first two components. This observation suggests that the numerical features—age, sentence_in_years, and years_lost—alone may not fully differentiate racial categories.\nIn sum, PCA effectively reduces the dataset to two dimensions, capturing a significant portion of the variance. However, the clustering patterns observed suggest that race-based groupings may not be strongly linear within the numerical features."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#t-sne-t-distributed-stochastic-neighbor-embedding",
    "href": "technical-details/unsupervised-learning/main.html#t-sne-t-distributed-stochastic-neighbor-embedding",
    "title": "Unsupervised Learning",
    "section": "t-SNE (t-distributed Stochastic Neighbor Embedding)",
    "text": "t-SNE (t-distributed Stochastic Neighbor Embedding)\nT-distributed Stochastic Neighbor Embedding (t-SNE) is a non-linear dimensionality reduction technique designed for visualizing high-dimensional data in a low-dimensional space. It preserves local relationships within the data, making it particularly effective for identifying clusters and patterns that may not be visible in higher dimensions.\n\n# Test different perplexity values\nperplexity_values = [5, 30, 50, 100]\n\nfor perplexity in perplexity_values:\n    print(f\"Running t-SNE with perplexity={perplexity}\")\n    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n    X_tsne = tsne.fit_transform(X)\n    plot_2D(X_tsne, df['race_encoded'], f't-SNE Results (Perplexity={perplexity})')\n\nRunning t-SNE with perplexity=5\n\n\n\n\n\n\n\n\n\nRunning t-SNE with perplexity=30\n\n\n\n\n\n\n\n\n\nRunning t-SNE with perplexity=50\n\n\n\n\n\n\n\n\n\nRunning t-SNE with perplexity=100\n\n\n\n\n\n\n\n\n\nThe t-SNE visualizations were generated using perplexity values of 5, 30, 50, and 100 to examine how this parameter influences the clustering structure. Perplexity determines the balance between local and global relationships within the data, where lower values emphasize small neighborhoods and higher values capture broader patterns.\nAt perplexity = 5, the plot reveals fragmented and overly localized clusters. While small neighborhoods are highlighted, the data appears disjointed, making it difficult to identify coherent global groupings. This behavior suggests that a perplexity of 5 is too low to capture meaningful structure.\nAt perplexity = 30, the visualization becomes more organized, striking a balance between local and global structure. Clear regional groupings emerge, with smaller clusters visible alongside broader trends. This representation provides an interpretable and balanced view of the data.\nAt perplexity = 50, the clustering appears more cohesive and distinct. The structure of the data is well-defined, and groupings are clearer compared to perplexity = 30. This value maintains a strong balance between fine-grained patterns and global structure, making it ideal for visualizing race-based patterns in the data.\nAt perplexity = 100, the plot emphasizes global structure but sacrifices local details. Clusters become stretched horizontally, and smaller, fine-grained groupings are smoothed out. While broad relationships are highlighted, important insights from localized clusters are diminished.\nIn conclusion, a perplexity value of 50 was selected as it produces the clearest and most cohesive clusters. This value preserves both local details and global structure, providing the optimal balance for identifying meaningful groupings and visualizing patterns related to race.\n\nComparison of PCA and t-SNE Results\n\n# Apply t-SNE with perplexity=50\ntsne = TSNE(n_components=2, perplexity=50, random_state=42)\nX_tsne = tsne.fit_transform(X)  # X is your standardized input data\n\n# Side-by-side comparison: PCA vs t-SNE\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# PCA Plot\naxes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=df['race_encoded'], alpha=0.5)\naxes[0].set_title('PCA Results')\naxes[0].set_xlabel('PC-1')\naxes[0].set_ylabel('PC-2')\n\n# t-SNE Plot\naxes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=df['race_encoded'], alpha=0.5)\naxes[1].set_title('t-SNE Results')\naxes[1].set_xlabel('t-SNE Component 1')\naxes[1].set_ylabel('t-SNE Component 2')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe two plots above compare the results of Principal Component Analysis (PCA) and t-SNE (perplexity = 50) when applied to the same dataset. In the PCA Results, the data is reduced to two principal components that capture the directions of maximum variance. The points appear somewhat aligned along the vertical axis (PC-1), indicating that the majority of the variance lies in that direction. However, the plot does not reveal clear or well-defined clusters, suggesting that PCA effectively captures global variance patterns but struggles to preserve local neighborhood structures. The visual separation by race (color-coded) is not particularly distinct in the PCA output. Alternatively, t-SNE Results provide a more nuanced and detailed visualization. By balancing local and global relationships, t-SNE produces more distinct clusters and patterns with greater cohesion. The clusters are clearer and better separated, indicating that t-SNE excels at preserving the local structure of the data. Although some overlap remains, the t-SNE output reveals a structure that is significantly clearer compared to PCA, particularly when visualized using race-based color encoding. Overall, t-SNE (with perplexity = 50) outperforms PCA in uncovering patterns and potential clusters within the dataset. While PCA captures the global variance effectively, it fails to separate groups as clearly. This comparison highlights the advantage of t-SNE for visualizing complex, high-dimensional data where local relationships are particularly important."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#k-means",
    "href": "technical-details/unsupervised-learning/main.html#k-means",
    "title": "Unsupervised Learning",
    "section": "K-Means",
    "text": "K-Means\nK-Means is a foundational unsupervised clustering algorithm that partitions data into a predefined number of clusters, denoted as K. K-Means begins by randomly selecting K initial cluster centroids and assigns each data point to the closest centroid based on the Euclidean distance. The centroids are then recalculated as the mean of all data points within their respective clusters. This process of assignment and centroid adjustment repeats until the centroids stabilize or a convergence criterion is reached. The goal of K-Means is to group data into clusters that are both cohesive and well-separated—where distances within each cluster are minimized, and distances between clusters are maximized. While K-Means performs effectively when clusters are spherical and well-defined, it requires specifying the number of clusters (K) in advance, which introduces the need for hyperparameter tuning.\n\nElbow Method\nTo identify the optimal number of clusters, the Elbow Method and Silhouette Score are employed. The Elbow Method involves plotting the point where the rate of inertia reduction slows, indicating the optimal number of clusters, as adding more clusters beyond it provides diminishing returns in variance reduction. The Silhouette Score offers an additional measure to assess clustering quality by evaluating how similar each data point is to its own cluster compared to other clusters. Higher Silhouette Scores indicate clusters that are well-defined and more cohesive, providing a clearer structure within the data.\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# Range for number of clusters\nk_values = range(2, 10)\nwcss = []\nsilhouette_scores = []\n\n# Find optimal K using WCSS and silhouette score\nfor k in k_values:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    labels = kmeans.fit_predict(X)  # Assuming X is the 2D data (e.g., PCA or t-SNE)\n    wcss.append(kmeans.inertia_)\n    silhouette_scores.append(silhouette_score(X, labels))\n\n# Plot Elbow Curve\nplt.figure(figsize=(8, 5))\nplt.plot(k_values, wcss, '-o')\nplt.title(\"Elbow Method for K-Means\")\nplt.xlabel(\"Number of Clusters (K)\")\nplt.ylabel(\"Within-Cluster Sum of Squares (WCSS)\")\nplt.show()\n\n\n\n\n\n\n\n\nIn the Elbow Method plot above, the inertia decreases sharply up to around K = 4 or K = 5, after which the curve begins to flatten. This behavior suggests that the optimal number of clusters lies between 4 and 5. By combining insights from both the Elbow Method and the Silhouette Scores, the best value for K can be confidently selected. Visualizing the clustering results further allows for assessing their alignment with meaningful patterns within the data.\n\n\nVisualize Clusters for Optimal K\n\n# Visualize Clusters for Optimal K\noptimal_k = 4  # Replace with the K identified using the elbow point\nkmeans = KMeans(n_clusters=optimal_k, random_state=42)\nlabels_kmeans = kmeans.fit_predict(X)\nplot_2D(X, labels_kmeans, 'K-Means Clustering Results')\n\n\n\n\n\n\n\n\nThe visualization above displays the results of K-Means clustering applied with an optimal value of K = 4, as determined using the Elbow Method. The data has been projected onto the first two principal components (PC-1 and PC-2) for visualization, with each point color-coded based on its cluster assignment.\nThe clusters appear well-separated and exhibit distinct patterns along the two principal components:\n\nThe yellow cluster occupies the far right of the PC-1 axis, indicating that this group has unique characteristics that distinguish it from the others.\n\nThe purple cluster is concentrated at the bottom of the plot, suggesting that it shares common features that set it apart, particularly along PC-2.\n\nThe teal and blue clusters are more centered with slight overlap, reflecting some shared attributes while still maintaining discernible boundaries.\n\nThe choice of K = 4 aligns well with the structure of the exoneration dataset, where race emerged as a key factor during Exploratory Data Analysis (EDA). The racial groups initially considered included Black, Hispanic, White, Native American, and Asian. However, the Asian group was excluded due to its negligible presence in the dataset, making four clusters a logical and meaningful choice. This outcome mirrors the underlying data distribution, where the remaining racial groups are clearly represented in the clustering results.\nThese findings suggest that K-Means effectively partitions the data into four meaningful clusters based on the selected features. The clear separation of clusters along PC-1 and PC-2 highlights that these principal components successfully capture the variance in the data, enabling the differentiation of racial groupings. The slight overlap between clusters may stem from shared attributes across groups or limitations in the chosen features."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#dbscan-density-based-spatial-clustering-of-applications-with-noise",
    "href": "technical-details/unsupervised-learning/main.html#dbscan-density-based-spatial-clustering-of-applications-with-noise",
    "title": "Unsupervised Learning",
    "section": "DBSCAN (Density-Based Spatial Clustering of Applications with Noise)",
    "text": "DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\nDBSCAN is an unsupervised clustering algorithm that groups data points based on density. Unlike K-Means, which requires specifying the number of clusters in advance, DBSCAN identifies clusters by locating dense regions in the data while marking points in less dense areas as noise. This makes it particularly effective for identifying clusters of arbitrary shapes and handling outliers. DBSCAN relies on two key parameters:\n\neps: The maximum distance between two points for them to be considered part of the same neighborhood.\n\nmin_samples: The minimum number of points required to form a dense region (a cluster).\n\nDBSCAN begins with an unvisited point and determines its neighborhood within the radius eps. If the number of points in the neighborhood meets or exceeds min_samples, a cluster is initiated. The cluster is then expanded by iteratively including points within eps distance of other points already in the cluster. Points that do not meet the density requirement are labeled as noise (outliers).\nOne of the key strengths of DBSCAN is its ability to detect clusters of varying shapes and handle datasets with noisy or irregular boundaries. Additionally, it does not force every point into a cluster, which allows for the identification of outliers—a feature particularly useful for understanding anomalies within the data. However, selecting appropriate values for eps and min_samples is critical to DBSCAN’s performance. A common strategy involves experimenting with different eps values and assessing the results using metrics such as the Silhouette Score.\nBy leveraging DBSCAN, the analysis can uncover nuanced structures in the dataset that may not be apparent with algorithms like K-Means, offering deeper insights into hidden patterns and systemic disparities.\n\nEps Hyper-Parameter Tuning\n\nfrom sklearn.cluster import DBSCAN\n\n# Test different eps values for DBSCAN\neps_values = [0.5, 1.0, 1.5, 2.0]\nfor eps in eps_values:\n    dbscan = DBSCAN(eps=eps, min_samples=5)\n    labels_dbscan = dbscan.fit_predict(X)\n    try:\n        sil_score = silhouette_score(X, labels_dbscan)\n        print(f\"EPS: {eps}, Silhouette Score: {sil_score}\")\n    except:\n        print(f\"EPS: {eps}, Silhouette Score: Undefined (noise present)\")\n\n    # Visualize Clusters\n    plot_2D(X, labels_dbscan, f'DBSCAN Clustering (EPS={eps})')\n\nEPS: 0.5, Silhouette Score: 0.1970450363722957\n\n\n\n\n\n\n\n\n\nEPS: 1.0, Silhouette Score: 0.7808945570083328\n\n\n\n\n\n\n\n\n\nEPS: 1.5, Silhouette Score: 0.7808945570083328\n\n\n\n\n\n\n\n\n\nEPS: 2.0, Silhouette Score: 0.7808945570083328\n\n\n\n\n\n\n\n\n\nThe eps values for DBSCAN were selected by testing a range of values (0.5, 1.0, 1.5, and 2.0) to analyze the algorithm’s sensitivity to this key parameter. DBSCAN uses eps (neighborhood radius) and min_samples to define dense regions, where eps controls the size of these regions. Smaller values of eps result in fragmented clusters, while larger values produce fewer, broader clusters. The chosen values allow for an incremental evaluation of how clustering changes to identify the best balance between fragmentation and cohesion.\n\nEPS = 0.5\n\nAt eps = 0.5, the clusters are highly fragmented, with a significant number of points labeled as noise (not assigned to any cluster). The neighborhood radius is too small to form large, cohesive clusters.\n\nThe Silhouette Score is 0.197, indicating poor clustering performance with high intra-cluster variance. The results lack meaningful structure and cohesion.\n\nEPS = 1.0\n\nWith eps = 1.0, clustering performance improves significantly. More points are grouped into clusters, dense regions become apparent, and the number of noise points is reduced.\n\nThe Silhouette Score rises to 0.78, reflecting well-defined clusters and clear separation between groups. At this value, the algorithm strikes a good balance between cohesive clusters and noise reduction.\n\nEPS = 1.5\n\nAt eps = 1.5, the clustering results remain largely similar to those observed at eps = 1.0. Most data points are grouped into a single large cluster, with only a small number of points remaining on the periphery.\n\nThe Silhouette Score remains stable at 0.78, but expanding the neighborhood radius further does not uncover additional structure in the data.\n\nEPS = 2.0\n\nWhen eps = 2.0, nearly all points are assigned to a single large cluster. While this eliminates noise points, it oversimplifies the data and removes meaningful structural separation.\n\nAlthough the Silhouette Score remains consistent, the clustering lacks distinct groupings, indicating that eps = 2.0 is too large for this dataset.\n\n\nThe analysis of DBSCAN results demonstrates that eps = 1.0 provides the best clustering performance. At this value, the clusters are well-defined, noise is minimized, and the Silhouette Score achieves a peak of 0.78. Smaller eps values, such as 0.5, lead to fragmented clusters with excessive noise, while larger values, such as 1.5 and 2.0, smooth the data excessively, reducing meaningful separation. Thus, eps = 1.0 emerges as the optimal choice, balancing noise reduction with cohesive and interpretable clustering results."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#hierarchical-clustering",
    "href": "technical-details/unsupervised-learning/main.html#hierarchical-clustering",
    "title": "Unsupervised Learning",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nHierarchical Clustering is an unsupervised machine learning algorithm that builds a hierarchy of clusters through an iterative process. Unlike K-Means, which requires specifying the number of clusters in advance, hierarchical clustering produces a dendrogram—a tree-like structure that shows how data points are grouped at different levels of granularity. The algorithm can follow two main approaches: agglomerative (bottom-up) and divisive (top-down). In agglomerative clustering, each data point starts as its own cluster. Clusters are progressively merged based on their similarity (distance), defined by the linkage method. This analysis uses Ward’s linkage, which minimizes intra-cluster variance at each step, resulting in well-balanced and cohesive clusters. The dendrogram serves as a visual tool to identify the optimal number of clusters by “cutting” the tree at a height where clusters are most distinct. This makes hierarchical clustering particularly effective for EDA, as it does not require prior knowledge of the number of clusters.\nTo determine the optimal clusters, the dendrogram was examined for large vertical distances, indicating well-separated groups. Ward’s linkage ensures minimal intra-cluster variance, producing compact and interpretable groupings—an essential property for datasets like the exoneration data, where clear separations between groups are critical. Hierarchical clustering with Ward’s linkage offers a flexible and interpretable approach for uncovering the dataset’s structure. By analyzing the dendrogram and selecting the appropriate height, meaningful clusters were identified. Compared to K-Means and DBSCAN, hierarchical clustering provides the additional benefit of visualizing relationships between clusters, making it a valuable method for validating and interpreting clustering results.\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Create Dendrogram\nlinkage_matrix = linkage(X, method='ward')\nplt.figure(figsize=(10, 7))\ndendrogram(linkage_matrix)\nplt.title(\"Hierarchical Clustering Dendrogram\")\nplt.xlabel(\"Samples\")\nplt.ylabel(\"Distance\")\nplt.show()\n\n# Apply Agglomerative Clustering for chosen number of clusters\nn_clusters = 4  # Replace with the number identified using the dendrogram\nagglom = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\nlabels_agglom = agglom.fit_predict(X)\n\n# Visualize Clusters\nplot_2D(X, labels_agglom, 'Hierarchical Clustering Results')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHierarchical Clustering Results Interpretation\nThe dendrogram above visually represents the clustering hierarchy generated using Ward’s linkage in hierarchical clustering. The dendrogram shows how data points are progressively merged into clusters at varying distances. By cutting the dendrogram at an appropriate height, four clusters are identified, which align with the patterns observed in the earlier K-Means results.\nThe Hierarchical Clustering Results plot projects these clusters onto the first two principal components (PC-1 and PC-2), with data points color-coded based on their cluster assignments:\n1. Purple Cluster: Positioned at the top of the PC-2 axis, this group stands out distinctly, indicating unique features that strongly separate it along PC-2.\n2. Yellow Cluster: Spread vertically across the middle region of PC-1, this cluster shows moderate cohesion while spanning a wide range along PC-2.\n3. Teal Cluster: Concentrated toward the bottom-right, this group is more compact and well-defined along PC-1, suggesting strong intra-cluster similarity.\n4. Blue Cluster: Located in the bottom-left region, this group appears dense and cohesive, with relatively low variation along PC-2.\nThe dendrogram supports the selection of four clusters, evidenced by clear separations into branches at a distance of approximately 20 units. These clusters are well-defined and consistent with the natural groupings observed in the data, particularly along PC-1 and PC-2. Compared to other clustering methods, hierarchical clustering offers a notable advantage by providing a hierarchical structure for cluster exploration. This allows for the analysis of groupings at multiple levels of granularity, enhancing the interpretability of the results. Overall, hierarchical clustering confirms the patterns identified in the K-Means results, while providing additional insights through the dendrogram. The use of Ward’s linkage ensures minimal intra-cluster variance, resulting in cohesive and interpretable clusters. These clusters align closely with the underlying structure of the data, reinforcing the conclusions drawn from the PCA and K-Means analyses."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#discussion",
    "href": "technical-details/unsupervised-learning/main.html#discussion",
    "title": "Unsupervised Learning",
    "section": "Discussion",
    "text": "Discussion\nThe clustering analysis using K-Means, DBSCAN, and Hierarchical Clustering reveals meaningful patterns within the exoneration dataset. Each method identifies groupings that align with the underlying racial categories—Black, Hispanic, White, and Native American—highlighted in the earlier exploratory data analysis (EDA).\nK-Means effectively grouped the data into four clusters, which correspond closely with the primary racial groups observed. The Elbow Method confirmed that four clusters were optimal, with clear separations visible along PC-1 and PC-2. This method produced cohesive and well-defined clusters with minimal noise, reflecting the structure of the data. The results demonstrate a strong alignment with the racial breakdown, as the groupings were particularly distinct along PC-1, capturing critical variance in the dataset.\nDBSCAN yielded varying results depending on the value of the eps parameter. At eps = 1.0, the method achieved the best balance between cluster cohesion and noise reduction, with a Silhouette Score of 0.78. At this setting, the clusters were distinguishable, and the algorithm successfully identified dense regions within the data. However, increasing eps beyond 1.0 caused most data points to merge into a single cluster, diminishing DBSCAN’s ability to detect finer groupings. Despite this limitation, DBSCAN proved particularly useful for identifying sparse regions and outliers, which may correspond to underrepresented racial groups or anomalies within the dataset.\nHierarchical Clustering using Ward’s linkage provided additional insights through its dendrogram, offering a clear visualization of the clustering process. The dendrogram supported the selection of four clusters, which closely matched the groupings identified by K-Means. Projecting the clusters onto the first two principal components revealed well-defined and interpretable separations. Ward’s linkage ensured minimal intra-cluster variance, producing compact clusters that effectively reflect the structure of the data.\nWhile all three clustering methods revealed meaningful patterns, K-Means and Hierarchical Clustering produced the clearest and most interpretable results. Both consistently identified four clusters that align with the racial categories analyzed earlier. K-Means offered computational efficiency and well-separated groupings, while Hierarchical Clustering provided the added benefit of a dendrogram, which validated the results by illustrating the relationships between clusters. DBSCAN, while effective for detecting outliers and handling irregular data distributions, struggled to produce distinct groupings beyond specific parameter settings.\nThe clustering results demonstrate that the exoneration data can be effectively grouped into four distinct clusters that align with the racial categories of Black, Hispanic, White, and Native American. These findings reinforce the observations from the EDA, where negligible representation of certain racial groups, such as Asians, led to their exclusion from the analysis. By revealing clear and consistent patterns within the data, this analysis highlights systemic disparities in exoneration outcomes that are closely tied to racial identity. Understanding these patterns provides critical insights into racial biases within the criminal justice system, emphasizing the need for informed policy changes and further investigations."
  },
  {
    "objectID": "technical-details/eda/main.html",
    "href": "technical-details/eda/main.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "This Exploratory Data Analysis (EDA) aims to uncover critical patterns and disparities within the dataset, focusing on geographic and demographic distributions of wrongful convictions, arrests, and official misconduct. The analysis examines relationships across key variables, such as race, location, crime type, and sentencing, to highlight systemic inequities and structural flaws in the criminal justice system. By summarizing, visualizing, and analyzing the data, this EDA provides a comprehensive foundation for the following stages of the project, informing both unsupervised learning approaches, such as clustering, and supervised learning, which will predict the probability of wrongful incarceration across race, counties, and other factors. Ultimately, this work seeks to highlight systemic failures, contribute to a deeper understanding of Illinois Crime data, and underscore the significant and urgent need for data-driven reforms in our criminal justice system.\n\n\nAlthough the dataset does not explicitly label counties as urban or rural, external classification data from the Illinois Primary Health Care Association (IPHCA) was incorporated to provide important context for analyzing patterns in arrests, exonerations, and over-policing across Illinois counties. This visualization will be refrencfed multiple times in the following EDA, for further details, refer to Appendix A in the Appendix tab.1\n\n\n\nIllinois Counties by Rural/Urban Classification"
  },
  {
    "objectID": "technical-details/eda/main.html#categorical-variables",
    "href": "technical-details/eda/main.html#categorical-variables",
    "title": "Exploratory Data Analysis",
    "section": "Categorical Variables",
    "text": "Categorical Variables\n\nDistribution of Exonerations by Race\n\n\nCode\nplt.figure(figsize=(8, 4))\nsns.countplot(y=exon_df['race'], order=exon_df['race'].value_counts().index, palette='viridis')\nplt.title('Distribution of Exonerations by Race')\nplt.xlabel('Count')\nplt.ylabel('Race')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nrace_counts = exon_df['race'].value_counts().reset_index()\nrace_counts.columns = ['Race', 'Count']\nprint(\"Table: Distribution of Race by Exoneration\")\nrace_counts.head()\n\n\nTable: Distribution of Race by Exoneration\n\n\n\n\n\n\n\n\n\nRace\nCount\n\n\n\n\n0\nBlack\n418\n\n\n1\nHispanic\n81\n\n\n2\nWhite\n47\n\n\n3\nAsian\n1\n\n\n4\nNative American\n1\n\n\n\n\n\n\n\nBlack individuals account for the largest group of exonerees in Illinois, with 418 cases, followed by Hispanic individuals with 81 cases and White individuals with 47. Contrastingly, Asian and Native American groups each have only 1 case. The extremely low counts for Asian and Native American individuals are essential to note because while these groups are part of the dataset, their inclusion in further analyses—such as statistical tests or group comparisons—could skew results** or lead to unreliable interpretations due to the disproportionately small sample sizes. To ensure the validity and accuracy of the analysis, these groups may be excluded or grouped under an “Other” category where necessary to maintain statistical integrity and avoid drawing misleading conclusions. These base findings highlight apparent disparities in exoneration rates across racial groups—a pattern that will be examined in greater detail in the following analysis.\n\n\nFrequency of Worst Crime Display\n\n\nCode\nplt.figure(figsize=(8, 4))\nsns.countplot(y=exon_df['worst_crime_display'], \n                order=exon_df['worst_crime_display'].value_counts().index, \n                palette='viridis')\nplt.title('Frequency of Worst Crimes')\nplt.xlabel('Count')\nplt.ylabel('Worst Crime Display')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe bar chart above shows the distribution of “worst crimes” associated with exonerees. Murder overwhelmingly accounts for the largest share, followed closely by Drug Possession or Sale and Sexual Assault. Beyond these top categories, the remaining crimes—such as Assault, Attempted Murder, and Weapon Possession or Sale—appear far less frequently, with counts dropping off significantly. The dominance of murder cases highlights a troubling trend: the most severe crimes are also the most common among wrongful convictions. This could potentially reflect systemic pressures, including the heightened scrutiny and urgency associated with solving violent crimes, which could possibly contribute to rushed investigations, false accusations, or coerced confessions.\nThe presence of Drug Possession or Sale as the second-highest category is equally notable, as it points to broader issues within the criminal justice system surrounding drug-related offenses, particularly the targeting of specific communities through over-policing and harsh sentencing policies. These findings reveal clear patterns in wrongful convictions that disproportionately affect individuals accused of high-stakes or heavily policed offenses.\n\n\nDistribution of Exonerees by Sex\n\n\nCode\nplt.figure(figsize=(8, 4))\nsns.countplot(y=exon_df['sex'], \n                order=exon_df['sex'].value_counts().index, \n                palette='viridis')\nplt.title('Distribution of Exonerees by Sex')\nplt.xlabel('Count')\nplt.ylabel('Sex')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe bar chart above illustrates the distribution of exonerees by sex, demonstrating a significant disparity, with male exonerees (greater than 500) vastly outnumbering female exonerees, accounting for only a tiny fraction of the total. This imbalance highlights that wrongful convictions overwhelmingly impact men. While this could reflect broader patterns in arrest and incarceration rates, it also raises questions about how gender influences systemic interactions within the justice system, including policing, prosecution, and sentencing. The stark contrast underscores the need to explore whether wrongful conviction processes disproportionately affect men due to their overrepresentation in the criminal justice system or if there are other structural factors contributing to this disparity.\n\nDistribution of Exonerees’ Sex by Race\n\n\nCode\nplt.figure(figsize=(8, 6))\nsns.countplot(data=exon_df, \n              y='sex', \n              hue='race', \n              palette='viridis')\nplt.title('Distribution of Sex by Race')\nplt.xlabel('Count')\nplt.ylabel('Sex')\nplt.legend(title='Race', bbox_to_anchor=(1.05, 1), loc='upper left')  # Adjust legend position\nplt.show()\n\n\n\n\n\n\n\n\n\nThe chart above highlights the distribution of exonerees’ sex across racial groups. Male exonerees dominate in every racial category, with Black men making up the largest share. This reflects well-documented systemic racial disparities in the criminal justice system, where Black individuals are over-policed, over-arrested, and disproportionately wrongfully convicted.2\nThe counts for female exonerees are strikingly low across all racial groups, a trend that is consistent with the overall underrepresentation of women in the criminal justice system. Women are less likely to be arrested and incarcerated, which partially explains their near invisibility in exoneration data. However, it is important to note that while the overall numbers for women are small, Black women make up a larger portion of female exonerees compared to other racial groups, demonstrating that Black women, existing at the intersection of racial and gender biases, face compounded vulnerabilities within the justice system."
  },
  {
    "objectID": "technical-details/eda/main.html#numerical-variables",
    "href": "technical-details/eda/main.html#numerical-variables",
    "title": "Exploratory Data Analysis",
    "section": "Numerical Variables",
    "text": "Numerical Variables\n\nAge Distribution of Exonerees\n\n\nCode\nplt.figure(figsize=(8, 4))\nsns.histplot(exon_df['age'], kde=True, bins=30, color='blue')\nplt.title('Age Distribution of Exonerees')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe age distribution of exonerees is visualized in the histogram above, which includes a kernel density estimate (KDE) to illustrate the overall shape of the data. The distribution is right-skewed, with most exonerees concentrated in the younger age range, particularly between 20 and 30 years old. The frequency begins to taper off as age increases, with far fewer exonerees above the age of 40. This skewed pattern suggests that younger individuals are disproportionately represented among Illinois’ exonerees, which may reflect broader systemic trends in arrest rates, convictions, or vulnerabilities within the justice system that disproportionately affect younger populations. The KDE line further highlights the sharp peak in the younger age range, emphasizing the concentration of exonerees in this group. These findings raise important questions about age-specific factors contributing to wrongful convictions, such as policing practices, access to legal resources, or biases that may disproportionately impact younger individuals. Further research could help unpack these trends and identify critical points of intervention.\n\nAge Distribution of Exonerees by Race\n\n\nCode\n# Bin the ages into groups\nage_bins = [10, 20, 30, 40, 50, 60, 70]  # Adjust bins as needed\nexon_df['age_group'] = pd.cut(exon_df['age'], bins=age_bins, right=False)\n\n# Group the data by age group and race\nage_race_counts = exon_df.groupby(['age_group', 'race']).size().unstack(fill_value=0)\nage_race_counts.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')\n\nplt.title(\"Age Distribution of Exonerees by Race\")\nplt.xlabel(\"Age Group\")\nplt.ylabel(\"Number of Exonerees\")\nplt.xticks(rotation=45)  # Rotate x-axis labels for readability\nplt.legend(title=\"Race\", bbox_to_anchor=(1.05, 1), loc='upper left')  \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe visualization enhances the age analysis and highlights a critical finding: young Black, in the 20-30 age group, men are disproportionately represented among exonerees, reflecting systemic failures within the criminal justice system, where young Black men face heightened vulnerability to wrongful convictions due to over-policing, racial bias, and systemic inequities.2\n\n\n\nDistribution of Years Wrongfully Lost by Exonerees\n\n\nCode\n# Filter out invalid years_lost values (e.g., negative or null values)\nexon_df_clean = exon_df[exon_df['years_lost'] &gt;= 0]\n\n# Calculate total years lost\ntotal_years_lost = round(exon_df_clean['years_lost'].sum(), 2)  # Round to 2 decimal places\nprint(f\"Total Years Lost by Exonerees: {total_years_lost}\")\n\nplt.figure(figsize=(8, 4))\n\nsns.histplot(exon_df_clean['years_lost'], kde=True, bins=30, color='purple', alpha=0.6)\n\nplt.title(f'Distribution of Years Wrongfully Lost by Exonerees\\n(Total: {total_years_lost:,} Years)')\nplt.xlabel('Years Lost')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\nTotal Years Lost by Exonerees: 4625.81\n\n\n\n\n\n\n\n\n\nThe histogram shows the distribution of years lost by exonerees due to wrongful convictions, with a total of 4,625.81 years lost collectively. The data is heavily skewed to the left, with most exonerees losing fewer than 5 years, likely representing individuals who were sentenced to probation or short-term incarceration but were still wrongfully convicted. However, the presence of a long tail reveals that a significant number of individuals spent 10, 20, or even 40+ years wrongfully imprisoned. While these longer terms are less frequent, their profound impact cannot be overstated, representing irreparable harm where decades of freedom were unjustly taken. This visualization emphasizes both the immense collective burden and the devastating individual human cost of wrongful convictions.\n\nDistribution of Years Wrongfully Lost by Exonerees by Race\n\n\nCode\nplt.figure(figsize=(10, 6))\n\nraces = exon_df_clean['race'].unique()\ndata_by_race = [exon_df_clean[exon_df_clean['race'] == race]['years_lost'] for race in races]\n\nplt.hist(data_by_race, bins=30, stacked=True, color=plt.cm.viridis(np.linspace(0, 1, len(races))), label=races, alpha=0.9)\n\nplt.title(f\"Distribution of Years Wrongfully Lost by Exonerees by Race)\\n(Total: {total_years_lost:,} Years)\")\nplt.xlabel(\"Years Lost\")\nplt.ylabel(\"Frequency\")\nplt.legend(title=\"Race\", loc='upper right')  # Add a legend for clarity\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe visualization underscores a critical disparity: Black individuals account for the vast majority of years wrongfully lost, reflecting systemic failures that disproportionately target Black communities. This burden extends far beyond individual exonerees—decades of stolen freedom amplify generational harm, perpetuating cycles of instability and deepening systemic inequities within the criminal justice system."
  },
  {
    "objectID": "technical-details/eda/main.html#implications",
    "href": "technical-details/eda/main.html#implications",
    "title": "Exploratory Data Analysis",
    "section": "Implications",
    "text": "Implications\nThe univariate analysis highlights deeply entrenched systemic disparities within wrongful convictions across race, gender, and age. Black individuals account for the overwhelming majority of exonerees, underscoring racialized patterns of disproportionate targeting within the criminal justice system. Hispanic individuals follow, particularly in drug-related cases, while White individuals remain underrepresented. While gender disparities in exonerations reflect broader trends of men being overrepresented in the justice system, the disproportionate presence of Black women among female exonerees reveals a unique intersection of biases. Women are often perceived as less likely to commit crimes due to societal stereotypes that frame them as less harmful or less threatening, which partially explains their overall underrepresentation. However, this perception does not shield Black women from racial biases, which place them at a heightened risk of wrongful conviction relative to their female counterparts. The result is a compounded vulnerability: while women as a whole are underrepresented among Illinois exonerees, Black women remain disproportionately overrepresented within this subset, illustrating how overlapping racial and gender biases converge to uniquely impact women of color.\nAge-specific patterns reveal that younger individuals, mainly those aged 20 to 30, are disproportionately represented among exonerees. However, the data makes clear that this is not just an issue of age—young Black men are overwhelmingly central to this trend. The analysis of years lost further underscores the devastating consequences, with many exonerees losing fewer than 5 years but some enduring 10, 20, or even 40+ years of unjust imprisonment. This disproportionate targeting of young Black men reflects a justice system that fails at every level, perpetuating racialized harm that fractures lives, families, and entire communities."
  },
  {
    "objectID": "technical-details/eda/main.html#worst-crime-display-by-race",
    "href": "technical-details/eda/main.html#worst-crime-display-by-race",
    "title": "Exploratory Data Analysis",
    "section": "Worst Crime Display by Race",
    "text": "Worst Crime Display by Race\n\n\nCode\nplt.figure(figsize=(14, 8))  # Increase figure size for better readability\n\n# Subset the dataframe for most frequent worst crime displays, excluding outliers\nsubset_crimes = exon_df['worst_crime_display'].isin([\n    'Murder', 'Drug Possession or Sale', 'Sexual Assault',\n    'Assault', 'Attempted Murder', 'Weapon Possession or Sale',\n    'Child Sex Abuse', 'Robbery', 'Theft', 'Sex Offender Registration',\n    'Burglary/Unlawful Entry', 'Other Nonviolent Felony', 'Manslaughter'\n])\nexon_df_subset = exon_df[subset_crimes]\n\nsns.countplot(\n    data=exon_df_subset,\n    y='Worst Crime Display',\n    hue='race',\n    order=exon_df_subset['worst_crime_display'].value_counts().index, \n    palette='viridis',\n    linewidth=0.5 \n)\n\nplt.title('Worst Crime Display by Race (Excluding Outliers)')\nplt.xlabel('Count')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe bar chart underscores wrongful conviction data categorized by race and the most serious crimes committed. Murder and Drug Possession or Sale dominate as the leading causes of wrongful convictions, with Black individuals disproportionately represented in these categories. Black individuals face significantly higher rates of wrongful convictions compared to other racial groups, with Hispanic individuals following closely—predominately in cases involving drug-related charges. In contrast, wrongful convictions among White individuals appear less frequent for these crimes. While wrongful convictions for nonviolent offenses like theft or burglary are relatively rare across racial groups, the overrepresentation of Black and Hispanic individuals in the most severe crime categories is indicative of pervasive biases embedded within judicial processes."
  },
  {
    "objectID": "technical-details/eda/main.html#exoneree-sentence-length-by-race",
    "href": "technical-details/eda/main.html#exoneree-sentence-length-by-race",
    "title": "Exploratory Data Analysis",
    "section": "Exoneree Sentence Length by Race",
    "text": "Exoneree Sentence Length by Race\nRows where sentence_in_years equaled 100 were excluded from the analysis to account for life sentences and death penalties, which had been coded as “100 years” during data cleaning. Including these extreme values likely would have skewed the distribution and created misleading interpretations of the results. By removing these outliers, the analysis focuses on finite sentence lengths, offering a clearer and more accurate representation of sentencing disparities among exonerees across racial groups.\n\n\nCode\n# Drop rows where sentence_in_years equals 100 -&gt; life sentence and death penalty\ndf_filtered = exon_df[exon_df['sentence_in_years'] != 100]\n\n# Plot Sentence in Years by Race\nplt.figure(figsize=(12, 6))\nsns.boxplot(data=df_filtered, x='race', y='sentence_in_years', palette='viridis')\nplt.title('Sentence Length in Years by Race (Excluding Life Sentence and Death Penalty)')\nplt.xlabel('Race')\nplt.ylabel('Sentence Length (Years)')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe boxplot highlights critical disparities in sentence lengths for exonerees across racial categories where Black individuals show shorter median sentence lengths overall, yet their data contains a significant number of outliers, indicating that some individuals served exceptionally long sentences. Hispanic individuals display the widest range in sentence lengths, with many values extending toward the higher end. White individuals follow a similar pattern but exhibit a slightly lower spread compared to Hispanic individuals. For Asian and Native American groups, the limited number of observations makes it difficult to identify definitive trends. These findings emphasize that racial disparities in wrongful convictions extend beyond the conviction itself, revealing inconsistencies in the length of sentences served prior to exoneration.\n\nNormalized Sentence Length in Years by Race\nThe following boxplot applies Z-score normalization to sentence lengths, adjusting values within each racial group to have a mean of zero and a standard deviation of one. This transformation ensures that sentence lengths are evaluated relative to their group’s internal distribution, eliminating distortions caused by variations in scale or sample size.\n\n\nCode\n# Calculate sample size per race\nrace_counts = df_filtered['race'].value_counts()\n\n#   Z-Score normalization\ndf_filtered['normalized_sentence_length'] = (\n    df_filtered.groupby('race')['sentence_in_years']\n    .transform(lambda x: (x - x.mean()) / x.std())\n)\n\n# Plot the normalized sentence lengths\nplt.figure(figsize=(12, 6))\nsns.boxplot(data=df_filtered, x='race', y='normalized_sentence_length', palette='viridis')\nplt.title('Normalized Sentence Length in Years by Race')\nplt.xlabel('Race')\nplt.ylabel('Normalized Sentence Length')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe boxplot reveals persistent disparities across racial groups even after normalization. Black individuals exhibit a concentration of outliers at the higher end, reflecting extreme sentence lengths relative to their group average. Hispanic and White individuals show a similar spread, with Hispanic individuals slightly skewed toward higher values. Asian and Native American groups lack sufficient data for definitive interpretation, as their categories show minimal or no variance. While normalization adjusts for group-level differences, the persistence of extreme values among Black individuals highlights systemic sentencing inequities that cannot be erased through statistical transformation alone.\n\n\nImplications\nThe bivariate analysis highlights systemic racial disparities in wrongful convictions, particularly for Black and Hispanic individuals. Black exonerees are disproportionately represented in severe crime categories like murder and drug-related offenses, reflecting the long-standing impact of over-policing and biased law enforcement practices. Hispanic individuals also face elevated wrongful convictions in drug-related crimes, while White individuals are underrepresented across these categories. Sentencing disparities further deepen these inequities. Black individuals experience extreme outliers in sentence lengths, with many serving exceptionally long terms before exoneration. Hispanic exonerees show the broadest range in sentence lengths, pointing to inconsistencies in sentencing practices that disproportionately impact minority groups. Even after normalization, Black individuals continue to exhibit extreme sentence values relative to their group averages, underscoring that systemic biases persist despite statistical adjustments. These findings reveal a troubling pattern of racialized injustice within the criminal legal system. From arrest to conviction to sentencing, Black and Hispanic individuals face disproportionate harm, driven by structural biases that demand urgent reform."
  },
  {
    "objectID": "technical-details/eda/main.html#geospatial-workflow",
    "href": "technical-details/eda/main.html#geospatial-workflow",
    "title": "Exploratory Data Analysis",
    "section": "Geospatial Workflow",
    "text": "Geospatial Workflow\nThe following analyses involved a sequence of steps to prepare and visualize the exoneration and arrest data:\n\nGeometry Creation\nUsing shapely.geometry, latitude and longitude values were converted into geometric points to map exoneration cases spatially. This allowed for the dataset to be transformed into a GeoDataFrame using GeoPandas.\n\n\nCode\n# Create geometry column for points based on latitude and longitude\ngeometry = [Point(xy) for xy in zip(exon_df['longitude'], exon_df['latitude'])]\ngeo_df = gpd.GeoDataFrame(exon_df, geometry=geometry)\n\n\n\n\nLoading Shapefiles\nThe Illinois county boundary shapefile was loaded using GeoPandas (gpd.read_file). This file provided the geographic outlines needed to visualize data at the county level.\n\n\nCode\n# Load Illinois county shapefile \nillinois_counties = gpd.read_file('../../data/geospatial/IL_BNDY_COUNTY_Py.shp')\n\n\n\n\nAggregating Exoneration Data\nExoneration counts were grouped by county and race to produce a summarized table of exonerations (groupby in pandas). This data was then merged with the Illinois shapefile to align geographic boundaries with exoneration counts.\n\n\nCode\n# Aggregate exoneration counts by county and race\nexoneration_counts = exon_df.groupby(['county', 'race'], as_index=False).size()\nexoneration_counts.rename(columns={'size': 'num_exonerations'}, inplace=True)\n\nillinois_counties['county'] = illinois_counties['COUNTY_NAM'].str.title()  # Match casing\n\n# Merge exoneration data with shapefile\nmerged_counties = illinois_counties.merge(exoneration_counts, on='county', how='left')\n\n\n\n\nAggregating Arrest Data\nArrest counts were grouped by county and race to produce a summarized table of arrests (groupby in pandas). This data was then merged with the Illinois shapefile to align geographic boundaries with arrest totals.\n\n\nCode\n# Load aggregated arrests data\naggregated_data = pd.read_csv('../../data/processed-data/aggregated_arrests_2001_to_2021.csv')\naggregated_data_melted = aggregated_data.melt(id_vars='race', var_name='county', value_name='total_arrests')\n\n# Aggregate total arrests by county for all races\ntotal_arrests_by_county = aggregated_data_melted.groupby('county')['total_arrests'].sum().reset_index()\n\n# Ensure county names are consistent\ntotal_arrests_by_county['county'] = total_arrests_by_county['county'].str.strip().str.title()\n\n# Merge total arrests data with the shapefile\nmerged_total_arrests = illinois_counties.merge(total_arrests_by_county, on='county', how='left')\n\n\n\n\nHandling Missing Data\nCounties without recorded exonerations or arrests were assigned a count of 0 to ensure completeness in the visualizations.\n\n\nCode\n# Replace NaN values with 0 for counties without exonerations\nmerged_counties['num_exonerations'] = merged_counties['num_exonerations'].fillna(0)\n\n# Fill NaN values with 0 for counties without data\nmerged_total_arrests['total_arrests'] = merged_total_arrests['total_arrests'].fillna(0)\n\n\n\n\nLogarithmic Transformation\nThe geospatial analysis visualizes exoneration and arrest patterns across Illinois counties. Cook County’s numbers are orders of magnitude higher than those in other counties, which skews the results and makes it difficult to identify trends in smaller counties on a linear scale. To address this, two scaling approaches were used:\n\nLogarithmic Scaling (Including Cook County): Log transformations (np.log1p) compress the data range, making patterns in smaller counties clearer while still preserving Cook County’s contribution.\n\nLinear Scaling (Excluding Cook County): Removing Cook County allows linear scaling to highlight smaller variations across the remaining counties, ensuring their natural distribution is preserved.\n\nWith log scaling, patterns across smaller counties become visible without Cook County completely dominating the visualization. Even on a linear scale, exonerations and arrests remain heavily concentrated in urban areas like Cook County and its surrounding regions.\nThis dual approach highlights the systemic concentration of wrongful convictions and arrests in high-population areas while ensuring patterns in smaller counties are not obscured. By combining log scaling and linear scaling, the visualizations strike a balance between emphasizing Cook County’s impact and uncovering broader geographic trends.\n\n\nCode\n# Exonerations Log transformation to handle the exponential scale of Cook County data\nmerged_counties['log_num_exonerations'] = np.log1p(merged_counties['num_exonerations'])\n\n# Arrests Log transformation to handle the exponential scale of Cook County data\nmerged_total_arrests['log_total_arrests'] = np.log10(merged_total_arrests['total_arrests'] + 1)  # Add 1 to avoid log(0)\n\n# Filter out Cook County \nfiltered_counties = merged_counties[merged_counties['county'] != 'Cook']\nfiltered_total_arrests = merged_total_arrests[merged_total_arrests['county'] != 'Cook']"
  },
  {
    "objectID": "technical-details/eda/main.html#exonerations-by-county",
    "href": "technical-details/eda/main.html#exonerations-by-county",
    "title": "Exploratory Data Analysis",
    "section": "Exonerations by County",
    "text": "Exonerations by County\n\n\nCode\nplt.figure(figsize=(8, 4))\nsns.countplot(y=exon_df['county'], order=exon_df['county'].value_counts().index, palette='viridis')\nplt.title('Distribution of Exonerations by County')\nplt.xlabel('Count')\nplt.ylabel('County')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFor context, Chicago is located within Cook County.\n\n\nLog-Scaled Number of Exonerations by County in Illinois\n\n\nCode\n# Creates side-by-side plots\nfig, axes = plt.subplots(1, 2, figsize=(20, 10), constrained_layout=True)\n\nmerged_counties.plot(\n    column='log_num_exonerations',  # Use the log-transformed column\n    cmap='Blues',\n    linewidth=0.5,\n    edgecolor='black',\n    ax=axes[0],\n    legend=True\n)\naxes[0].set_title(\"Log-Scaled Number of Exonerations by County in Illinois (Including Cook)\", fontsize=14)\naxes[0].set_xlabel(\"Longitude\")\naxes[0].set_ylabel(\"Latitude\")\naxes[0].grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n\n# Plot the gradient map excluding Cook County, keeping linear scale\nfiltered_counties.plot(\n    column='num_exonerations',  \n    cmap='Blues',\n    linewidth=0.5,\n    edgecolor='black',\n    ax=axes[1],\n    legend=True\n)\naxes[1].set_title(\"Number of Exonerations by County (Excluding Cook)\", fontsize=14)\naxes[1].set_xlabel(\"Longitude\")\naxes[1].set_ylabel(\"Latitude\")\naxes[1].grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nThe maps reveal critical patterns in both exonerations and arrests across Illinois counties. Cook County (Chicago), as expected, dominates the data with orders of magnitude higher counts, which skews the results and makes it challenging to identify trends in smaller counties. To address this, a logarithmic transformation (np.log1p) was applied to compress the data range, ensuring that smaller counties remain visible without flattening Cook County’s impact. When Cook County is excluded, linear scaling highlights natural variations across the remaining counties. While smaller differences become more interpretable, the visualizations still emphasize the concentration of arrests and exonerations in urban regions outside Cook County. This dual scaling approach—logarithmic for overall visibility and linear for regional comparisons—strikes a balance, allowing geographic disparities across Illinois to emerge clearly. By visualizing both exonerations and arrests, the maps underscore systemic trends: wrongful convictions are disproportionately concentrated in high-population areas, particularly urban hubs. At the same time, significant activity persists in smaller counties, reinforcing the need for statewide examination of these issues."
  },
  {
    "objectID": "technical-details/eda/main.html#arrests-by-county",
    "href": "technical-details/eda/main.html#arrests-by-county",
    "title": "Exploratory Data Analysis",
    "section": "Arrests by County",
    "text": "Arrests by County\n\nLog-Scaled Total Arrests by County in Illinois\n\n\nCode\n# Create side-by-side plots\nfig, axes = plt.subplots(1, 2, figsize=(20, 10), constrained_layout=True)\n\n# Plot the gradient map for total arrests by county (including Cook County with log scale)\nmerged_total_arrests.plot(\n    column='log_total_arrests',  # Use log-scaled data\n    cmap='Reds',\n    linewidth=0.5,\n    edgecolor='black',\n    ax=axes[0],\n    legend=True\n)\naxes[0].set_title(\"Log-Scaled Total Arrests by County in Illinois (Including Cook)\", fontsize=14)\naxes[0].set_xlabel(\"Longitude\")\naxes[0].set_ylabel(\"Latitude\")\naxes[0].grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n\n# Plot the gradient map for total arrests by county (excluding Cook County with linear scale)\nfiltered_total_arrests.plot(\n    column='total_arrests',  # Use linear-scaled data\n    cmap='Reds',\n    linewidth=0.5,\n    edgecolor='black',\n    ax=axes[1],\n    legend=True\n)\naxes[1].set_title(\"Total Arrests by County (Excluding Cook) in Illinois\", fontsize=14)\naxes[1].set_xlabel(\"Longitude\")\naxes[1].set_ylabel(\"Latitude\")\naxes[1].grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n\n# Show the side-by-side maps\nplt.show()\n\n\n\n\n\n\n\n\n\nThe maps highlight clear geographic disparities in arrest patterns across Illinois. Urban counties, particularly Cook County, dominate the arrest totals, even when using a logarithmic scale to compress the data. Cook County remains a significant outlier, with arrest counts orders of magnitude higher than in rural areas, driving the statewide totals. When Cook County is excluded and linear scaling is applied, the smaller counties’ arrest patterns become more visible. While some rural counties show modest variations, their arrest totals remain consistently lower than those in urban regions.To provide further context, Illinois counties’ rural and urban classifications (see Appendix A) align with these trends. Rural counties—defined by lower population densities—consistently exhibit fewer arrests, while urban counties, like Cook, DuPage, and Lake, demonstrate the systemic concentration of policing and arrests in high-population areas."
  },
  {
    "objectID": "technical-details/eda/main.html#total-arrests-by-race-and-county",
    "href": "technical-details/eda/main.html#total-arrests-by-race-and-county",
    "title": "Exploratory Data Analysis",
    "section": "Total Arrests by Race and County",
    "text": "Total Arrests by Race and County\n\nLog-Scaled Total Arrests by Race and County\n\n\nCode\nmerged_total_arrests = illinois_counties.merge(\n    aggregated_data_melted, on='county', how='left'\n)\n\nmerged_total_arrests['total_arrests'] = merged_total_arrests['total_arrests'].fillna(0)\n\n# Log-transform total arrests for maps including Cook County\nmerged_total_arrests['log_total_arrests'] = np.log10(merged_total_arrests['total_arrests'] + 1)  # Add 1 to avoid log(0)\n\n# Filter out Cook County\nmerged_data_excluding_cook = merged_total_arrests[merged_total_arrests['county'] != 'Cook']\n\n# Define the races to plot\nraces_to_plot = [\"Black\", \"White\", \"Hispanic\", \"Asian\"]\n\n# Create subplots\nfig, axes = plt.subplots(2, len(races_to_plot), figsize=(20, 20), constrained_layout=True)\n\n# Plot each race's data (including Cook County with log scale) on the first row\nfor i, race in enumerate(races_to_plot):\n    race_data = merged_total_arrests[merged_total_arrests['race'] == race]\n\n    # Ensure data is not empty\n    if race_data.empty:\n        print(f\"No data available for the selected race: {race}\")\n        continue\n\n    # Plot the gradient map for the current race\n    race_data.plot(\n        column='log_total_arrests',  # Use log-transformed data\n        cmap='Reds',\n        linewidth=0.5,\n        edgecolor='black',\n        ax=axes[0, i],\n        legend=(i == len(races_to_plot) - 1)  # Show legend only on the last subplot\n    )\n    axes[0, i].set_title(f\"Log-Scaled Total Arrests (Including Cook): {race}\")\n    axes[0, i].set_xlabel(\"Longitude\")\n    axes[0, i].set_ylabel(\"Latitude\")\n    axes[0, i].grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n\n# Plot each race's data (excluding Cook County with linear scale) on the second row\nfor i, race in enumerate(races_to_plot):\n    race_data = merged_data_excluding_cook[merged_data_excluding_cook['race'] == race]\n\n    # Ensure data is not empty\n    if race_data.empty:\n        print(f\"No data available for the selected race: {race}\")\n        continue\n\n    # Plot the gradient map for the current race\n    race_data.plot(\n        column='total_arrests',  # Use linear scale for excluding Cook\n        cmap='Reds',\n        linewidth=0.5,\n        edgecolor='black',\n        ax=axes[1, i],\n        legend=(i == len(races_to_plot) - 1)  # Show legend only on the last subplot\n    )\n    axes[1, i].set_title(f\"Total Arrests (Excluding Cook): {race}\")\n    axes[1, i].set_xlabel(\"Longitude\")\n    axes[1, i].set_ylabel(\"Latitude\")\n    axes[1, i].grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nHypothesis Testing\n\nKruskal-Wallis\nThe Kruskal-Wallis Test was applied to assess whether significant differences exist in total arrests across racial groups—specifically Black, White, and Hispanic populations. This statistical method was chosen due to its suitability for the data and the research question at hand:\n\nAs a non-parametric test, it bypasses the requirement for data to adhere to a normal distribution.\n\nIt is effective for comparing three or more independent groups, making it ideal for this analysis of racial categories.\n\nThe test assumes that the groups being compared are independent of one another.\n\nBy evaluating medians and ranking data, it demonstrates resilience against the influence of outliers and skewed distributions.\n\nNull Hypothesis (H₀): There is no significant difference in total arrests across racial groups.\nAlternative Hypothesis (H₁): There is a significant difference in total arrests across racial groups.\n\n\nCode\nfrom scipy.stats import kruskal\n\n# Group total arrests by race\nblack_arrests = merged_total_arrests[merged_total_arrests['race'] == 'Black']['total_arrests']\nwhite_arrests = merged_total_arrests[merged_total_arrests['race'] == 'White']['total_arrests']\nhispanic_arrests = merged_total_arrests[merged_total_arrests['race'] == 'Hispanic']['total_arrests']\n\n# Kruskal-Wallis Test\nh_stat, p_val = kruskal(black_arrests, white_arrests, hispanic_arrests)\nprint(f\"Kruskal-Wallis Test: H-statistic = {h_stat:.4f}, p-value = {p_val:.4f}\")\n\n\nKruskal-Wallis Test: H-statistic = 161.1528, p-value = 0.0000\n\n\nGiven the p-value of 0.0000, which is well below the conventional threshold of 0.05, the results indicate statistically significant differences in total arrests between the racial groups. In other words, the distribution of total arrests is not equal across the Black, White, and Hispanic categories. This finding underscores the presence of disparities in arrest rates among these racial groups, warranting further investigation to explore underlying causes, systemic factors, or policy implications contributing to these observed differences.\n\n\nANOVA Permutation Test\nTo complement the Kruskal-Wallis test, an ANOVA Permutation Test was performed to assess whether there are significant differences in total arrests across racial groups (e.g., Black, White, and Hispanic). Unlike the Wallis test, which is a non-parametric test, ANOVA compares the means of the groups and assumes normally distributed data with stable variance. The ANOVA Permutation Test is robust and does not rely on strict assumptions of normality. By resampling the data (permutations), it provides a valid test for group differences even when the data has been log-transformed.\nNull Hypothesis (H₀): There is no significant difference in total arrests across racial groups (Black, White, and Hispanic).\nAlternative Hypothesis (H₁): There is a significant difference in total arrests across racial groups (Black, White, and Hispanic).\n\nVisualizing the Skewed Data\n\n\nCode\n# Original and log-transformed data\noriginal_data = merged_total_arrests['total_arrests']\nlog_transformed_data = np.log1p(merged_total_arrests['total_arrests'])\n\n# Create subplots\nfig, ax = plt.subplots(1, 2, figsize=(12, 6))\n\n# Plot Original Data\nax[0].hist(original_data, bins=30, color='blue', alpha=0.7)\nax[0].set_title(\"Original Data\")\nax[0].set_xlabel(\"Total Arrests\")\nax[0].set_ylabel(\"Frequency\")\n\n# Plot Log-Transformed Data\nax[1].hist(log_transformed_data, bins=30, color='orange', alpha=0.7)\nax[1].set_title(\"Log-Transformed Data\")\nax[1].set_xlabel(\"Log(1 + Total Arrests)\")\nax[1].set_ylabel(\"Frequency\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWhy Log Scaling Was Necessary:\n- The original data for total arrests was highly skewed, with extreme values (e.g., Cook County arrests) dominating the distribution.\n- Log transformation reduces this skewness, stabilizes variance, and makes the data closer to a normal distribution.\n- For ANOVA, the assumptions of normality and homogeneity of variance are critical. Log transformation ensures these assumptions are better met, improving the reliability of the test results.\nIn contrast, the Kruskal-Wallis test does not require normality or equal variance since it is non-parametric and works with ranks instead of raw values. This makes log scaling unnecessary for Kruskal-Wallis.\n\n\nCode\nimport numpy as np\nfrom scipy.stats import permutation_test\n\n# Group data\ngroups = [\n    merged_total_arrests[merged_total_arrests['race'] == 'Black']['log_total_arrests'],\n    merged_total_arrests[merged_total_arrests['race'] == 'White']['log_total_arrests'],\n    merged_total_arrests[merged_total_arrests['race'] == 'Hispanic']['log_total_arrests']\n]\n\n# Function to calculate F-statistic\ndef f_statistic(*args):\n    group_means = [np.mean(g) for g in args]\n    grand_mean = np.mean([x for g in args for x in g])\n    ss_between = sum(len(g) * (m - grand_mean)**2 for g, m in zip(args, group_means))\n    ss_within = sum(sum((x - m)**2 for x in g) for g, m in zip(args, group_means))\n    return ss_between / ss_within\n\n# Perform permutation test\nresult = permutation_test(groups, f_statistic, n_resamples=9999, alternative='two-sided')\nprint(f\"Permutation-Based ANOVA: p-value = {result.pvalue:.4f}\")\n\n\nPermutation-Based ANOVA: p-value = 0.0002\n\n\nThe p-value (0.0002) is far below the significance threshold of 0.05. This means the reject the null hypothesis is rejected. Since the p-value is extremely small, it can be concluded that there are statistically significant differences in total arrests between at least two of the racial groups."
  },
  {
    "objectID": "technical-details/eda/main.html#overrepresentation-by-county",
    "href": "technical-details/eda/main.html#overrepresentation-by-county",
    "title": "Exploratory Data Analysis",
    "section": "Overrepresentation by County",
    "text": "Overrepresentation by County\n\nLog-Scaled Overrepresentation Ratios by County\nThe raw values for the overrepresentation ratios across races clearly demonstrate the need for scaling. Given the wide disparity in values, log scaling was implemented to compress large values while preserving their relative differences. To ensure consistency across maps, a global color scale was applied using vmin and vmax. This guarantees that the same color represents the same value across all maps, making comparisons more meaningful:\n\nRaw Values for Overrepresentation Ratios\nThe raw values for the overrepresentation ratios across races clearly demonstrate the need for scaling:\n\n\nCode\ndemographic_df = pd.read_csv('../../data/processed-data/representation_by_county.csv')\n\n# Merge demographic data with Illinois shapefile\nmerged_demographics = illinois_counties.merge(\n    demographic_df, on='county', how='left'\n)\n\n# Define the ratios to plot\nratios_to_plot = [\n    'ratio_of_overrepresentation_of_whites_incarcerated_compared_to_whites_non-incarcerated',\n    'ratio_of_overrepresentation_of_blacks_incarcerated_compared_to_blacks_non-incarcerated',\n    'ratio_of_overrepresentation_of_latinos_incarcerated_compared_to_latinos_non-incarcerated'\n]\n\n# Display min, mean, max, and std for each overrepresentation ratio\nfor ratio in ratios_to_plot:\n    stats = merged_demographics[ratio].agg(['min', 'mean', 'max', 'std'])\n    print(f\"{ratio}:\\n{stats}\\n\")\n\n\nratio_of_overrepresentation_of_whites_incarcerated_compared_to_whites_non-incarcerated:\nmin     0.000000\nmean    0.517474\nmax     1.070000\nstd     0.319997\nName: ratio_of_overrepresentation_of_whites_incarcerated_compared_to_whites_non-incarcerated, dtype: float64\n\nratio_of_overrepresentation_of_blacks_incarcerated_compared_to_blacks_non-incarcerated:\nmin       0.000000\nmean     40.916211\nmax     418.720000\nstd      78.475567\nName: ratio_of_overrepresentation_of_blacks_incarcerated_compared_to_blacks_non-incarcerated, dtype: float64\n\nratio_of_overrepresentation_of_latinos_incarcerated_compared_to_latinos_non-incarcerated:\nmin      0.000000\nmean     6.393895\nmax     96.150000\nstd     12.830208\nName: ratio_of_overrepresentation_of_latinos_incarcerated_compared_to_latinos_non-incarcerated, dtype: float64\n\n\n\nEvidently, the scales are wildly disproportionate:\n\nThe mean for incarcferated Black population (~41) is orders of magnitude higher than for Whites (~0.5) or Latinos (~6).\nThe standard deviation (std) further emphasizes the disparity, particularly for Black incarceration rates.\n\nEven with log scaling, the imbalance remains striking but becomes much easier to visualize and compare.\n\n\nCode\ntitles = [\n    \"Overrepresentation of White Population Incarcerated\",\n    \"Overrepresentation of Black Population Incarcerated\",\n    \"Overrepresentation of Latino Population Incarcerated\"\n]\n\n# Apply log scaling to ratios and store in new columns\nfor ratio in ratios_to_plot:\n    merged_demographics[f'log_{ratio}'] = np.log1p(merged_demographics[ratio])  # log1p avoids log(0)\n\n# Combine all log-scaled ratios to get global color scale\nall_log_ratios = pd.concat([\n    merged_demographics[f'log_{ratios_to_plot[0]}'],\n    merged_demographics[f'log_{ratios_to_plot[1]}'],\n    merged_demographics[f'log_{ratios_to_plot[2]}']\n], axis=0)\n\n# Get global color scale for log values\nvmin, vmax = all_log_ratios.min(), all_log_ratios.max()\n\n# Create subplots\nfig, axes = plt.subplots(1, 3, figsize=(20, 8), constrained_layout=True)\n\n# Plot each log-scaled ratio\nfor i, ratio in enumerate(ratios_to_plot):\n    # BASE LAYER: Plot county boundaries\n    illinois_counties.boundary.plot(\n        ax=axes[i],\n        linewidth=0.5,\n        color=\"black\"\n    )\n\n    # OVERLAY: Plot the log-transformed ratio data\n    merged_demographics.plot(\n        column=f'log_{ratio}',\n        cmap='Purples',\n        linewidth=0.5,\n        edgecolor='black',\n        ax=axes[i],\n        legend=True,\n        vmin=vmin,  # Global log scale min\n        vmax=vmax   # Global log scale max\n    )\n    \n    # Centered titles\n    axes[i].set_title(titles[i], fontsize=12, loc='center', pad=20, x=0.4)\n\n    axes[i].set_axis_off()  # Hide axes for clean appearance\n\nplt.suptitle(\"\\n Log-Scaled Overrepresentation Ratios by County \\n\", fontsize=16)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nThe maps above show log-scaled overrepresentation ratios for incarcerated White, Black, and Latino populations by county in Illinois. When examined alongside the rural-urban classification from Appendix A, clear patterns emerge. For the Black population, overrepresentation is strikingly concentrated in rural counties, particularly in southern and central Illinois. These areas, despite having smaller Black populations, show higher overrepresentation ratios, likely due to systemic disparities magnified by the demographic imbalance. Urban areas, such as Cook County (Chicago), do not show significant overrepresentation, which is surprising. However, this could be explained by Cook County’s large Black population, which may stabilize the incarceration ratio and reduce its visibility on a log-scaled measure. In contrast, the White population shows minimal overrepresentation across both rural and urban counties, with very few areas displaying notable disparities. For the Latino population, overrepresentation appears more concentrated in southern and central rural counties, with some isolated counties in the north and west also showing elevated ratios. This suggests that incarceration disparities for Latinos are more localized compared to the widespread patterns observed for Black individuals. The findings reinforce how rural counties, with smaller minority populations, experience disproportionate incarceration rates, while urban areas like Cook County exhibit less extreme disparities—possibly due to higher baseline minority populations. These trends highlight the need to contextualize overrepresentation ratios within the demographic makeup of each county.\n\n\n\nHypothesis Testing\n\nKruskal-Wallis Test:\nThe Kruskal-Wallis test is appropriate here because the data (log-scaled ratios) does not need to meet normality assumptions, and the test uses ranks rather than raw values, making it robust to skewed data and outliers. The groups being compared—White, Black, and Latino—are independent of each other, which satisfies the test’s assumptions.\nNull Hypothesis (H₀): The medians of the log-scaled overrepresentation ratios are equal across the three racial groups (White, Black, and Latino).\nAlternative Hypothesis (H₁): At least one group has a median log-scaled overrepresentation ratio that is significantly different from the others.\n\n\nCode\n# Extract log-scaled ratios for each group\nwhite_ratios = merged_demographics[f'log_{ratios_to_plot[0]}'].dropna()\nblack_ratios = merged_demographics[f'log_{ratios_to_plot[1]}'].dropna()\nlatino_ratios = merged_demographics[f'log_{ratios_to_plot[2]}'].dropna()\n\n# Kruskal-Wallis Test to check for overall differences\nh_stat, p_val = kruskal(white_ratios, black_ratios, latino_ratios)\nprint(f\"Kruskal-Wallis Test: H-statistic = {h_stat:.4f}, p-value = {p_val:.4f}\")\n\n\nKruskal-Wallis Test: H-statistic = 62.1876, p-value = 0.0000\n\n\nSince the p-value = 0.0000 (far below the significance threshold of 0.05), the null hypothesis is rejected. The test result indicates that there are significant differences in the medians of the log-scaled overrepresentation ratios between at least one pair of racial groups (White, Black, and Latino).\nTo identify which groups are significantly different from each other, you should perform pairwise comparisons using a post-hoc test such as the Mann-Whitney U Test with a correction for multiple comparisons (e.g., Bonferroni adjustment).\n\n\nPairwise Mann-Whitney U Tests:\nThe Mann-Whitney U Test was used to complement the Kruskal-Wallis test by conducting pairwise comparisons of total arrests between racial groups: White, Black, and Latino. This test was appropriate because it is non-parametric, meaning it does not require the data to meet normality assumptions, by comparing two independent groups at a time, it becomes suitable for pairwise analyses. Instead of using raw values, the Mann-Whitney U Test evaluates rank distributions, which makes it effective for handling skewed data and outliers. It is also well-suited for datasets with smaller sample sizes or unequal group sizes, making it a strong choice for this analysis.\nNull Hypothesis (H₀):\nThere is no significant difference in total arrests between the two racial groups being compared.\nAlternative Hypothesis (H₁):\nThere is a significant difference in total arrests between the two racial groups being compared.\nThe Mann-Whitney U Test was performed for all pairwise comparisons (e.g., White vs. Black, White vs. Latino, Black vs. Latino). Results with p-values &lt; 0.05 indicate significant differences between the groups.\n\n\nCode\nfrom scipy.stats import mannwhitneyu\n\n# Pairwise Mann-Whitney U Tests with Bonferroni Adjustment - Only rAn if Kruskal-Wallis indicates significance\nprint(\"\\nPerforming pairwise Mann-Whitney U Tests with Bonferroni Adjustment:\")\npairs = [('White', white_ratios), ('Black', black_ratios), ('Latino', latino_ratios)]\nnum_comparisons = len(pairs) * (len(pairs) - 1) // 2  # Total number of pairwise comparisons\nalpha_adjusted = 0.05 / num_comparisons  # Adjusted significance level\n\nfor i in range(len(pairs)):\n    for j in range(i + 1, len(pairs)):\n        group1_name, group1 = pairs[i]\n        group2_name, group2 = pairs[j]\n        u_stat, p_val_pair = mannwhitneyu(group1, group2, alternative='two-sided')\n        \n        # Check if p-value is below the adjusted threshold\n        print(f\"{group1_name} vs {group2_name}: U-statistic = {u_stat:.4f}, \"\n                f\"p-value = {p_val_pair:.4f}\")\n\nprint(f\"\\nBonferroni-adjusted significance level: {alpha_adjusted:.4f}\")\n\n\n\nPerforming pairwise Mann-Whitney U Tests with Bonferroni Adjustment:\nWhite vs Black: U-statistic = 1848.0000, p-value = 0.0000\nWhite vs Latino: U-statistic = 2591.5000, p-value = 0.0000\nBlack vs Latino: U-statistic = 6162.5000, p-value = 0.0000\n\nBonferroni-adjusted significance level: 0.0167\n\n\nThe results of the pairwise Mann-Whitney U tests with a Bonferroni-adjusted significance level of 0.0167 are as follows:\n\nWhite vs Black: The p-value is below the adjusted threshold, indicating a significant difference in total arrests between White and Black individuals.\nWhite vs Latino: The p-value is below the adjusted threshold, indicating a significant difference in total arrests between White and Latino individuals.\nBlack vs Latino: The p-value is below the adjusted threshold, indicating a significant difference in total arrests between Black and Latino individuals.\n\nAll pairwise comparisons (White vs Black, White vs Latino, and Black vs Latino) show statistically significant differences in total arrests. The Bonferroni adjustment ensures that these results account for multiple comparisons, reducing the risk of false positives."
  },
  {
    "objectID": "technical-details/eda/main.html#implications-2",
    "href": "technical-details/eda/main.html#implications-2",
    "title": "Exploratory Data Analysis",
    "section": "Implications",
    "text": "Implications\nThe geographic analysis reveals critical disparities in the distribution of wrongful convictions and arrests across Illinois counties, highlighting how these issues are both systemic and spatially concentrated. Urban counties, particularly Cook County, dominate the data, with exonerations and arrests far exceeding those observed in rural areas. This concentration reflects broader systemic patterns where urban regions experience higher policing volumes, larger caseloads, and more pronounced racial disparities. The findings underscore that wrongful convictions are not isolated incidents but are intricately tied to urban policing practices and the disproportionate targeting of Black individuals, who bear the heaviest burden of both arrests and misconduct leading to exonerations. In rural counties, such as Livingston and Macon, exonerations and arrests appear minimal in comparison. While this could reflect smaller populations or reduced legal activity, the trends in rural areas still warrant attention. The overrepresentation ratios observed in rural counties (see Appendix A) reveal significant disparities, particularly for Black individuals. Even in regions with smaller Black populations, incarceration and arrest rates remain disproportionate, magnifying systemic biases against minority groups within less densely populated areas.\nBy balancing logarithmic scaling to visualize statewide trends and linear scaling to highlight regional nuances, this analysis demonstrates that systemic failures in the justice system are pervasive yet vary in scale across urban and rural settings. Urban areas like Cook County exemplify the most severe disparities, driven by population density and over-policing of Black communities. At the same time, rural counties exhibit subtler but still significant patterns of racialized injustice."
  },
  {
    "objectID": "technical-details/eda/main.html#disproportionality-index",
    "href": "technical-details/eda/main.html#disproportionality-index",
    "title": "Exploratory Data Analysis",
    "section": "Disproportionality Index",
    "text": "Disproportionality Index\nThe Corporation for Supportive Housing (CSH) created the Racial Disparities and Disproportionality Index (RDDI) to assess disproportionality across systems, including housing data. Drawing on the same foundational principles, I developed an adapted index specifically for my dataset3.\nThe following version of the index measures whether a group’s presence in a given system—such as arrests, over-policing, or exonerations—is proportional to their overall population share. By centering equity-driven analysis, it highlights patterns of overrepresentation and underrepresentation across racial and ethnic groups, providing a clearer view of systemic disparities within these systems.\nTo interpret the index:\n- A value greater than 1 indicates overrepresentation within the system.\n- A value less than 1 indicates underrepresentation within the system.\n- A value of exactly 1 reflects parity, meaning the group’s representation is proportional to their overall population.\n\nExonerations vs. Incarcerated Population by Race\n\n\nCode\n# Aggregate exoneration counts by race and county\nexonerees_by_race_county = exon_df.groupby(['race', 'county']).size().reset_index(name='exonerees')\n\n# Merge exonerations with demographic data on race and county\nmerged_exonerees_demo = exonerees_by_race_county.merge(\n    merged_demographics, \n    on='county', \n    how='left'\n)\n\n# Filter only valid races present in the demographic data\nvalid_races = ['White', 'Black', 'Hispanic']  # Exclude Asian and Native American\nmerged_exonerees_demo = merged_exonerees_demo[merged_exonerees_demo['race'].isin(valid_races)]\n\n# Aggregate incarcerated population by race\nincarcerated_by_race = merged_exonerees_demo.groupby('race')[\n    ['incarcerated_white_population', 'incarcerated_black_population', 'incarcerated_latino_population']\n].sum()\n\n# Map the columns to their respective race labels\nincarcerated_by_race = incarcerated_by_race.rename(columns={\n    'incarcerated_white_population': 'White',\n    'incarcerated_black_population': 'Black',\n    'incarcerated_latino_population': 'Hispanic'\n})\n\n# Sum incarcerated population for valid races only\nincarcerated_total = incarcerated_by_race.sum(axis=1)\n\n# Calculate total exonerations by race\nexonerations_by_race = merged_exonerees_demo.groupby('race')['exonerees'].sum()\n\n# Calculate percentages\nincarcerated_percent = incarcerated_total / incarcerated_total.sum()\nexoneration_percent = exonerations_by_race / exonerations_by_race.sum()\n\n# Calculate disproportionality index\ndisproportionality_index = (exoneration_percent / incarcerated_percent).fillna(0)\n\n# Convert to DataFrame for plotting\ndisproportionality_df = disproportionality_index.reset_index()\ndisproportionality_df.columns = ['race', 'disproportionality_index']\n\n# Plot disproportionality index\nplt.figure(figsize=(10, 6))\nsns.barplot(data=disproportionality_df, x='race', y='disproportionality_index', palette='plasma')\nplt.title(\"Disproportionality Index: Exonerations vs Incarcerated Population by Race\")\nplt.xlabel(\"Race\")\nplt.ylabel(\"Disproportionality Index\")\nplt.axhline(y=1, color='gray', linestyle='--', label=\"Parity (Exonerations = Incarcerated)\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe graph displays the Disproportionality Index comparing exonerations to the incarcerated population by race. The dashed line at 1.0 represents parity, where exonerations would align proportionally with incarceration rates. For the Black population, the index value significantly exceeds 1.0, indicating that Black individuals are overrepresented in exonerations relative to their incarceration rate. This suggests systemic issues that may lead to wrongful convictions disproportionately impacting Black communities. In contrast, the Hispanic population shows an index value below 1.0 but higher than that of White individuals. While there is some disproportionality, it is less pronounced compared to the Black population. The White population has the lowest index value, well below 1.0, indicating that White individuals are underrepresented in exonerations relative to their incarceration rate. This may suggest fewer wrongful convictions or more favorable outcomes for White individuals within the criminal justice system. Overall, the graph highlights a clear racial disparity, with Black individuals experiencing the greatest disproportionality in exonerations—a trend that underscores deeper systemic inequities within the justice system.\n\n\nExonerations vs. Arrests by Race\nExonerations reflect wrongful convictions that result from arrests, while arrests represent the volume of individuals taken into custody by law enforcement—a proxy for policing practices. The Disproportionality Index: Exonerations vs. Arrests provides a way to analyze whether certain racial groups experience wrongful convictions at rates disproportionate to their arrest volumes.\nIf the ratio of exonerations to arrests is higher for a specific race, it suggests that individuals within that group are being wrongfully arrested and convicted at disproportionately high rates relative to how often they are arrested. This can be interpreted as an indicator of over-policing or bias in policing practices, where arrests occur at a higher rate without sufficient cause, increasing the likelihood of wrongful convictions.\nWhile this measure highlights the connection between arrests and wrongful convictions, it is distinct from analyzing exonerations vs. the incarcerated population. Exonerations compared to incarceration rates highlight broader issues of systemic bias within the criminal justice system, focusing on who remains incarcerated unjustly. Meanwhile, exonerations compared to arrests emphasize the front-end policing practices—who is being taken into custody in the first place and whether those arrests were justified.\nBy using both comparisons—exonerations vs. arrests and exonerations vs. incarceration—it becomes possible to capture the full scope of disparities, from over-policing to wrongful incarceration, and to identify where systemic failures occur across different stages of the justice process.\n\n\nCode\n# Aggregate exoneration counts by race and county\nexonerees_by_race_county = exon_df.groupby(['race', 'county']).size().reset_index(name='exonerees')\n# Merge exonerees and arrests by race and county\nmerged_exoneree_arrest = exonerees_by_race_county.merge(\n    aggregated_data_melted, on=['race', 'county'], how='left'\n)\n\n# Fill NaN values in arrests with 0 (in case no arrests are recorded for a county)\nmerged_exoneree_arrest['total_arrests'] = merged_exoneree_arrest['total_arrests'].fillna(0)\n\n# Filter out races that are not relevant (Asian and Native American)\nvalid_races = ['White', 'Black', 'Hispanic']\nmerged_exoneree_arrest = merged_exoneree_arrest[merged_exoneree_arrest['race'].isin(valid_races)]\n\n# Calculate total arrests and exonerations by race\ntotal_arrests_by_race = merged_exoneree_arrest.groupby('race')['total_arrests'].sum()\ntotal_exonerations_by_race = merged_exoneree_arrest.groupby('race')['exonerees'].sum()\n\n# Calculate percentages\narrest_percent = total_arrests_by_race / total_arrests_by_race.sum() * 100\nexoneration_percent = total_exonerations_by_race / total_exonerations_by_race.sum() * 100\n\n# Calculate disproportionality index\ndisproportionality = (exoneration_percent / arrest_percent).fillna(0)\n\n# Convert to DataFrame for plotting\ndisproportionality_df = disproportionality.reset_index()\ndisproportionality_df.columns = ['race', 'disproportionality_index']\n\n# Plot disproportionality index\nplt.figure(figsize=(10, 6))\nsns.barplot(data=disproportionality_df, x='race', y='disproportionality_index', palette='plasma')\nplt.title(\"Disproportionality Index: Exonerations vs Arrests by Race\")\nplt.xlabel(\"Race\")\nplt.ylabel(\"Disproportionality Index\")\nplt.axhline(1, color='gray', linestyle='--', label=\"Parity (Exonerations = Arrests)\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe Disproportionality Index graph comparing exonerations to arrests initially indicates significant disparities, particularly for Hispanic individuals. At first glance, the Hispanic group exhibits the highest disproportionality index—well above parity—suggesting that exonerations are disproportionately high relative to their arrest volume. However, this observation requires further scrutiny when considering the raw counts of total arrests and exonerations.\n\n\nCode\n# Calculate raw counts for context\nrace_counts = merged_exoneree_arrest.groupby('race')[['exonerees', 'total_arrests']].sum().reset_index()\n\n# Plot raw counts for comparison\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n# Plot total arrests\nsns.barplot(data=race_counts, x='race', y='total_arrests', ax=axes[0], palette='viridis')\naxes[0].set_title(\"Total Arrests by Race\")\naxes[0].set_ylabel(\"Total Arrests\")\naxes[0].set_xlabel(\"Race\")\n\n# Plot total exonerations\nsns.barplot(data=race_counts, x='race', y='exonerees', ax=axes[1], palette='viridis')\naxes[1].set_title(\"Total Exonerations by Race\")\naxes[1].set_ylabel(\"Total Exonerations\")\naxes[1].set_xlabel(\"Race\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIn the second visualization of Total Arrests vs. Total Exonerations by Race, its evident that while the Hispanic population has a high disproportionality index, this result is influenced by the small number of arrests relative to other groups. For example, Black individuals have the highest total arrests (over 2.2 million) and the highest number of exonerations (418), while White individuals follow closely in arrests (1.8 million) but have significantly fewer exonerations (47). In contrast, the Hispanic group has a much smaller total arrest count (1.68 million) and a moderate number of exonerations (81).\n\n\nCode\n# Summarize counts by race\nrace_summary = merged_exoneree_arrest.groupby('race').agg(\n    total_arrests=('total_arrests', 'sum'),\n    total_exonerees=('exonerees', 'sum')\n).reset_index()\n\npd.set_option('display.max_columns', None)  \nrace_summary\n\n\n\n\n\n\n\n\n\nrace\ntotal_arrests\ntotal_exonerees\n\n\n\n\n0\nBlack\n2297196.0\n418\n\n\n1\nHispanic\n168944.0\n81\n\n\n2\nWhite\n1815269.0\n47\n\n\n\n\n\n\n\n\n\nCode\ntotal_arrests_sum = race_summary[\"total_arrests\"].sum()\nprint (\"Total Arrest Volume: \", total_arrests_sum)\n\n\nTotal Arrest Volume:  4281409.0\n\n\nTo address this, I implemented a threshold of 170,000 total arrests—approximately 4% of the total arrest volume—to filter out racial groups with insufficient arrest counts. This ensures that the results are not skewed by small sample sizes. For example, the Hispanic group, with a total of 168,944 arrests, makes up just under 4% of all arrests. By setting this threshold, the analysis excludes groups with arrest counts below this level, preventing inflated disproportionality indices caused by smaller numbers.\n\n\nCode\n# Apply threshold: Only include races with total arrests &gt; 100,000\ntotal_arrests_by_race = total_arrests_by_race[total_arrests_by_race &gt;170000]\nvalid_races_with_threshold = total_arrests_by_race.index  # Get the races that meet the threshold\n\n# Filter exoneration counts to match the races above\ntotal_exonerations_by_race = total_exonerations_by_race[total_exonerations_by_race.index.isin(valid_races_with_threshold)]\n\n# Calculate percentages\narrest_percent = total_arrests_by_race / total_arrests_by_race.sum() * 100\nexoneration_percent = total_exonerations_by_race / total_exonerations_by_race.sum() * 100\n\n# Calculate disproportionality index\ndisproportionality = (exoneration_percent / arrest_percent).fillna(0)\n\n# Convert to DataFrame for plotting\ndisproportionality_df = disproportionality.reset_index()\ndisproportionality_df.columns = ['race', 'disproportionality_index']\n\n# Plot disproportionality index\nplt.figure(figsize=(10, 6))\nsns.barplot(data=disproportionality_df, x='race', y='disproportionality_index', palette='plasma')\nplt.title(\"Disproportionality Index: Exonerations vs Arrests by Race (Threshold: &gt;170,000 Arrests)\")\nplt.xlabel(\"Race\")\nplt.ylabel(\"Disproportionality Index\")\nplt.axhline(1, color='gray', linestyle='--', label=\"Parity (Exonerations = Arrests)\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe updated disproportionality index focuses on racial groups with a substantial share of arrests, providing a clearer and more accurate analysis. By applying a 170,000 threshold, which represents 4% of the total arrest volume, groups with smaller arrest counts—like the Hispanic population—are filtered out. This prevents inflated indices caused by small sample sizes. As a result, the analysis reveals that Black individuals remain disproportionately exonerated relative to their arrest rate, while the previously observed Hispanic disproportionality becomes more contextualized when considering their smaller overall share of arrests.\n\nWhy This Matters\nThis process demonstrates that while initial disproportionality measures can highlight disparities, raw counts and thresholds are essential for providing context. Smaller arrest volumes can distort disproportionality indices, leading to misleading conclusions about systemic disparities. By introducing a threshold, the analysis ensures that overrepresentation is evaluated fairly, accurately, and meaningfully across racial groups with a substantial presence in the data.\n\n\n\nImplications\nThe disproportionality analysis highlights profound racial disparities across multiple stages of the criminal justice system, from arrests to exonerations. Black individuals experience the highest disproportionality in exonerations relative to both arrests and incarceration rates. This suggests that Black communities face systemic failures at multiple levels: they are overpoliced, wrongfully arrested, and disproportionately subjected to wrongful convictions. These findings reinforce how deeply racial bias is embedded within law enforcement and prosecutorial practices, culminating in devastating injustices. The comparison of exonerations to arrests is particularly illuminating, as it exposes front-end policing failures. Over-policing in predominantly Black communities results in high arrest volumes, increasing the likelihood of wrongful convictions. Despite representing a disproportionate share of total arrests, Black individuals are also exonerated at disproportionately high rates, signaling a clear breakdown in the legitimacy of these arrests and the justice process that follows.\nThe analysis initially identified Hispanic individuals with an elevated disproportionality index in exonerations relative to arrests. Applying a threshold of 170,000 arrests—approximately 4% of the total arrest volume—helps contextualize this result by accounting for smaller arrest counts that can inflate the index. However, as the other data shows, Hispanic individuals do have higher arrest and incarceration rates than White individuals, with values that are the second highest after Black individuals. While their overall arrest volume is smaller than that of Black or White populations, the higher disproportionality index underscores that wrongful convictions still impact Hispanic individuals at elevated rates relative to their arrest volume. The updated analysis focuses on racial groups with substantial arrest volumes while reinforcing that these disparities remain significant.\nIn sum, this analysis underscores how racial bias and systemic flaws converge at every stage of the justice process—arrests, convictions, and eventual exonerations—disproportionately impacting Black individuals."
  },
  {
    "objectID": "technical-details/eda/main.html#frequency-of-om-tags-in-exoneration-cases",
    "href": "technical-details/eda/main.html#frequency-of-om-tags-in-exoneration-cases",
    "title": "Exploratory Data Analysis",
    "section": "Frequency of OM Tags in Exoneration Cases",
    "text": "Frequency of OM Tags in Exoneration Cases\nThis visualization provides a clear understanding of which forms of official misconduct are most prevalent in wrongful conviction cases. By identifying trends, the analysis underscores systemic issues in the criminal justice system that contribute to these injustices.\n\n\nCode\n# Count the frequency of each tag\ntag_columns = [\n    'arson', 'bitemark', 'co_defendant_confessed', 'conviction_integrity_unit',\n    'child_sex_abuse_hysteria_case', 'child_victim', 'female_exoneree', \n    'federal_case', 'homicide', 'innocence_organization', 'jailhouse_informant', \n    'juvenile_defendant', 'misdemeanor', 'no_crime_case', 'guilty_plea_case', \n    'posthumous_exoneration', 'sexual_assault', 'shaken_baby_syndrome_case',\n    'prosecutor_misconduct', 'police_officer_misconduct', 'forensic_analyst_misconduct', \n    'child_welfare_worker_misconduct', 'withheld_exculpatory_evidence',\n    'misconduct_that_is_not_withholding_evidence', 'knowingly_permitting_perjury', \n    'witness_tampering_or_misconduct_interrogating_co_defendant',\n    'misconduct_in_interrogation_of_exoneree', 'perjury_by_official', \n    'prosecutor_lied_in_court'\n]\n\n# Count occurrences of each tag\ntag_counts = exon_df[tag_columns].sum().sort_values(ascending=False)\ntag_counts.plot(kind='bar', figsize=(12, 6), color='skyblue')\nplt.title(\"Frequency of OM Tags in Cases\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Tags\")\nplt.xticks(rotation=45, ha='right')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe bar chart displays the frequency of official misconduct tags across all wrongful conviction cases. Each tag represents a specific type of misconduct, and the height of the bars indicates how often each tag appears. At a glance, misconduct types like “misconduct_that_is_not_withholding_evidence,” “police officer misconduct,” and “withheld_exculpatory_evidence” dominate the chart, highlighting the most common failures contributing to wrongful convictions. These categories reveal recurring patterns of negligence or deliberate misconduct within the justice system, particularly by law enforcement and prosecutors. Tags toward the right, such as “child_welfare_worker_misconduct” or “child_sex_abuse_hysteria_case,” occur far less frequently, suggesting that while impactful, they play a smaller role in the Illinois dataset compared to the more systemic issues seen on the left side of the chart. The visual clearly underscores how evidentiary issues, misconduct by officials, and failures during investigations are central drivers of wrongful convictions. These patterns expose the depth of systemic flaws and provide insight into where reforms in accountability and oversight are most urgently needed."
  },
  {
    "objectID": "technical-details/eda/main.html#total-tags-per-case-and-county-level-distribution",
    "href": "technical-details/eda/main.html#total-tags-per-case-and-county-level-distribution",
    "title": "Exploratory Data Analysis",
    "section": "Total Tags Per Case and County-Level Distribution",
    "text": "Total Tags Per Case and County-Level Distribution\nThe distribution of total tags per case is explored through a histogram with a density curve. This visualization shows how many misconduct tags are typically associated with each wrongful conviction case, revealing the overall frequency and variability of misconduct on a case-by-case basis.\nThe analysis is further refined at the county level using a boxplot, showing how the total number of misconduct tags varies across different counties. This highlights geographical disparities, with some counties exhibiting higher concentrations of misconduct tags than others.\n\n\nCode\n# Distribution of tag_sum\nplt.figure(figsize=(8, 4))\nsns.histplot(exon_df['tag_sum'], kde=True, bins=20, color='blue')\nplt.title(\"Distribution of Total Tags Per Exoneree\")\nplt.xlabel(\"Total Tags (tag_sum)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThe histogram visualizes the distribution of total misconduct tags per exoneree, providing insight into how many official misconduct types are typically associated with individual wrongful conviction cases. The inclusion of a density curve smooths the distribution, highlighting the overall shape of the data. The distribution reveals that most cases cluster around a total tag count of 6 to 8, indicating that these cases often involve multiple forms of misconduct. Fewer cases fall on the extremes, with some showing very low or very high tag counts.\nThis visualization emphasizes that wrongful convictions are rarely tied to a single instance of misconduct. Instead, they are often characterized by a combination of failures, such as withheld evidence, perjury, or misconduct by officials, compounding the harm inflicted on exonerees. By analyzing this distribution, the data underscores the systemic and multifaceted nature of official misconduct in wrongful conviction cases.\n\n\nCode\n# Tag sum by county\nplt.figure(figsize=(12, 6))\nsns.boxplot(data=df, x='county', y='tag_sum', palette='coolwarm')\nplt.title(\"Distribution of Total Tags by County\")\nplt.xlabel(\"County\")\nplt.ylabel(\"Total Tags\")\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe boxplot reveals significant disparities in the total number of misconduct tags across Illinois counties, with a clear distinction between urban and rural areas. Counties like Lake, Kane, and St. Clair, which appear to have higher tag counts and greater variability, are classified as urban counties (Appendix A). The combination of higher population densities and heavier caseloads in urban areas may contribute to the increased prevalence of official misconduct in wrongful conviction cases.\nOn the other hand, counties such as Lawrence and Macon, which display consistently lower tag counts, are classified as rural counties (Appendix A). This may reflect fewer documented wrongful conviction cases or less reporting of official misconduct in these areas. Rural counties tend to have smaller populations and fewer legal resources, which could limit the exposure or identification of misconduct.\nThese findings highlight a broader systemic pattern where urban counties, with their larger legal infrastructures and caseloads, see higher instances of misconduct tied to wrongful convictions. Meanwhile, rural counties may experience underreporting or lack the capacity to fully document similar issues. The urban-rural divide, as outlined in Appendix A, provides critical context for understanding these regional disparities."
  },
  {
    "objectID": "technical-details/eda/main.html#misconduct-tags-by-race",
    "href": "technical-details/eda/main.html#misconduct-tags-by-race",
    "title": "Exploratory Data Analysis",
    "section": "Misconduct Tags by Race",
    "text": "Misconduct Tags by Race\nThis visualization examines the total number of misconduct tags across racial groups to highlight racial disparities in wrongful convictions. Misconduct tags are aggregated for each race, providing a clear view of which groups are disproportionately impacted by official misconduct.\n\n\nCode\n# Group misconduct-related tags\nmisconduct_tags = [\n    'prosecutor_misconduct', 'police_officer_misconduct', \n    'forensic_analyst_misconduct', 'child_welfare_worker_misconduct',\n    'withheld_exculpatory_evidence', 'misconduct_that_is_not_withholding_evidence',\n    'knowingly_permitting_perjury', 'witness_tampering_or_misconduct_interrogating_co_defendant',\n    'misconduct_in_interrogation_of_exoneree', 'perjury_by_official', \n    'prosecutor_lied_in_court'\n]\n\n# Create a column for combined misconduct\nexon_df['total_misconduct'] = exon_df[misconduct_tags].sum(axis=1)\n\n# Aggregate total misconduct by race (using sum for total counts)\nrace_misconduct = exon_df.groupby('race')['total_misconduct'].sum().reset_index()\n\n# Bar plot of total misconduct by race\nplt.figure(figsize=(10, 6))\nsns.barplot(data=race_misconduct, x='race', y='total_misconduct', palette='viridis')\nplt.title(\"Total Misconduct Tags by Race\")\nplt.xlabel(\"Race\")\nplt.ylabel(\"Total Number of Misconduct Tags\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe chart makes clear the staggering racial disparities in official misconduct tied to wrongful convictions. Black exonerees face an overwhelming burden, with the total number of misconduct tags far exceeding that of any other racial group. This disparity underscores how deeply systemic bias and official misconduct are embedded within the criminal legal system, disproportionately impacting Black individuals at every level. Hispanic exonerees follow but with significantly fewer misconduct tags, while White and Native American exonerees show even lower counts. Asian exonerees, by contrast, have virtually no recorded instances of misconduct tags, which may reflect reporting gaps or smaller representation in the dataset rather than an absence of misconduct. The magnitude of misconduct affecting Black exonerees highlights a critical failure in accountability and exposes racialized patterns of injustice within wrongful convictions.\n\nHypothesis Testing\n\nShapiro-Wilk Test For Normality\nThe Shapiro-Wilk test is used to determine if the total misconduct tags for each racial group are normally distributed. It is appropriate here because it is sensitive to deviations from normality and works well for small to moderately sized datasets, ensuring that the assumptions of parametric tests like ANOVA are met.\nNull Hypothesis (H₀): The OM misconduct tags are normally distributed for each racial group.\nAlternative Hypothesis (H₁): The OM misconduct tags are not normally distributed for each racial group.\n\n\nCode\nfrom scipy.stats import f_oneway, shapiro, levene\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Check counts of total_misconduct for each race\nrace_counts = exon_df.groupby('race')['total_misconduct'].count()\nprint(\"Counts of total_misconduct by race:\")\nprint(race_counts)\n\n# Filter out groups with fewer than 3 data points\nvalid_races = race_counts[race_counts &gt;= 3].index\nfiltered_df = exon_df[exon_df['race'].isin(valid_races)]\n\n# Re-run Shapiro-Wilk Test for Normality\nfrom scipy.stats import shapiro\n\nprint(\"\\nShapiro-Wilk Test for Normality (p &lt; 0.05 indicates non-normality):\")\nfor race in valid_races:\n    stat, p = shapiro(filtered_df['total_misconduct'][filtered_df['race'] == race])\n    print(f\"{race}: p-value = {p:.4f}\")\n\n\nCounts of total_misconduct by race:\nrace\nAsian                1\nBlack              418\nHispanic            81\nNative American      1\nWhite               47\nName: total_misconduct, dtype: int64\n\nShapiro-Wilk Test for Normality (p &lt; 0.05 indicates non-normality):\nBlack: p-value = 0.0000\nHispanic: p-value = 0.0000\nWhite: p-value = 0.0003\n\n\nThe results of the Shapiro-Wilk test show that the p-values for all racial groups—Black, Hispanic, and White—are below the significance threshold of 0.05, indicating that the total misconduct tags for all groups do not follow a normal distribution. The p-values for Black and Hispanic groups are exactly 0.0000, providing extremely strong evidence against the null hypothesis. For the White group, the p-value is 0.0003, which, while slightly higher, still represents strong evidence that the data is not normally distributed.\n\n\nOne-Way ANOVA\nThe one-way ANOVA test is used here to assess whether there are statistically significant differences in the mean number of total misconduct tags across racial groups. This test is appropriate because it compares the means of more than two independent groups—in this case, racial groups—to determine if the observed differences reflect a real effect or are simply due to random chance.\nANOVA operates under two key assumptions: that the data within each group is approximately normally distributed and that variances across groups are roughly equal (homogeneity of variances). While the Shapiro-Wilk test showed a violation of the normality assumption, ANOVA is known to be robust to deviations from normality, especially when group sizes are sufficiently large, as with the Black and Hispanic groups in this dataset. For smaller groups, like White, the robustness still holds if interpreted carefully, since ANOVA focuses on identifying differences between group means rather than the exact shape of each group’s distribution.\nNull Hypothesis (H₀): There is no significant difference in the mean total misconduct tags between racial groups.\nAlternative Hypothesis (H₁): At least one racial group has a significantly different mean total misconduct tag count.\n\n\nCode\n# Conduct One-Way ANOVA\nf_stat, p_anova = f_oneway(*groups)\nprint(f\"\\nOne-Way ANOVA Results: F-statistic = {f_stat:.4f}, p-value = {p_anova:.4f}\")\n\n\n\nOne-Way ANOVA Results: F-statistic = 164.6119, p-value = 0.0000\n\n\nSince the p-value (0.0000) is less than the significance threshold of 0.05, the null hypothesis of the one-way ANOVA is rejected. This result provides statistically significant evidence that there is a difference in the mean total misconduct tags between at least two of the racial groups. However, the ANOVA test itself does not specify which groups are responsible for this difference—it only confirms that not all group means are equal.\nTo determine which specific pairs of racial groups have significantly different mean misconduct tags, post-hoc testing is performed using Tukey’s HSD. Tukey’s test compares all possible pairs of group means while controlling for multiple comparisons, ensuring that the results remain statistically valid. By analyzing the output of Tukey’s HSD, it becomes possible to identify exactly which racial group comparisons contribute to the overall significance observed in the ANOVA results.\n\n\nTurkey’s HSD Post-hoc Test\nThe Tukey’s Honest Significant Difference (HSD) test is conducted as a post-hoc analysis following a significant one-way ANOVA result. This test identifies which specific pairs of groups have significantly different means while controlling for the family-wise error rate caused by multiple comparisons. Tukey’s HSD is appropriate in this case because the ANOVA test showed a statistically significant difference (p-value &lt; 0.05), indicating that at least one pair of racial groups has a significantly different mean number of total misconduct tags.\nUsing Tukey’s HSD allows for the identification of specific pairs of racial groups with significant differences in their mean misconduct tags. This step is critical for understanding where the disparities occur and which group comparisons contribute to the overall significant ANOVA result. If the output of Tukey’s HSD indicates a “reject” for particular group comparisons, it confirms that the differences in mean misconduct tags between those groups are statistically significant.\n\n\nCode\n# Post-hoc Test (Tukey's HSD) if ANOVA is significant\nprint(\"\\nPerforming Tukey's HSD Post-hoc Test:\")\ntukey = pairwise_tukeyhsd(\n    endog=exon_df['total_misconduct'],\n    groups=exon_df['race'],\n    alpha=0.05\n)\nprint(tukey)\n\n# Visualize Tukey's results\nplt.figure(figsize=(10, 6))\ntukey.plot_simultaneous()\nplt.title(\"Tukey's HSD Test for Total Misconduct Tags by Race\")\nplt.show()\n\n\n\nPerforming Tukey's HSD Post-hoc Test:\n         Multiple Comparison of Means - Tukey HSD, FWER=0.05          \n======================================================================\n     group1          group2     meandiff p-adj   lower   upper  reject\n----------------------------------------------------------------------\n          Asian           Black   3.4593 0.3626 -1.7408  8.6595  False\n          Asian        Hispanic   4.2963 0.1631 -0.9296  9.5222  False\n          Asian Native American      7.0 0.0702 -0.3454 14.3454  False\n          Asian           White   3.0638 0.4996 -2.1851  8.3128  False\n          Black        Hispanic    0.837 0.0028  0.2064  1.4675   True\n          Black Native American   3.5407 0.3384 -1.6595  8.7408  False\n          Black           White  -0.3955  0.657 -1.1946  0.4036  False\n       Hispanic Native American   2.7037 0.6176 -2.5222  7.9296  False\n       Hispanic           White  -1.2325 0.0039 -2.1849 -0.2801   True\nNative American           White  -3.9362 0.2426 -9.1851  1.3128  False\n----------------------------------------------------------------------\n\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nThe results of the Tukey’s HSD test reveal significant differences in the mean total misconduct tags between specific racial group pairs. A statistically significant difference is observed between the Black and Hispanic groups, as indicated by a very small adjusted p-value (p-adj) and a “True” result in the reject column. Similarly, the comparison between the Hispanic and White groups shows a significant difference, with an adjusted p-value of 0.0039 and a “True” rejection of the null hypothesis.\nFor all other pairwise comparisons—such as Asian vs Black, Asian vs Hispanic, Black vs White, and comparisons involving Native American groups—no statistically significant differences are detected. The confidence intervals for these comparisons include zero, meaning the observed differences are not large enough to be considered statistically significant.\nThe accompanying plot provides a visual representation of the 95% confidence intervals for the mean differences between groups. Each horizontal line represents a confidence interval, and when a line crosses the vertical zero line, it indicates a lack of statistical significance. Significant differences appear only for the Black-Hispanic and Hispanic-White comparisons, where the confidence intervals remain entirely on one side of zero.\nIn conclusion, the Tukey’s HSD results demonstrate that disparities in total misconduct tags are particularly evident for the Hispanic group, which differs significantly from both the Black and White groups. For all other group comparisons, no statistically significant differences were identified.\n\n\n\nBreakdown of OM Types by Race\nThis heatmap shows the distribution of official misconduct types across racial groups, giving a clear picture of how different forms of misconduct disproportionately affect each group. Misconduct tags are aggregated by race, and the color gradient reflects their frequency, with darker shades indicating higher counts. The x-axis lists the specific types of misconduct, while the y-axis categorizes racial groups. Annotated values within each cell provide the exact counts, making the patterns easy to interpret both visually and numerically. By breaking down the data this way, the visualization reveals which types of official misconduct are most prevalent for each racial group, underscoring systemic disparities and offering a foundation for deeper analysis of how misconduct manifests across racial lines.\n\n\nCode\n# Aggregate each misconduct tag by race\nmisconduct_by_race = exon_df.groupby('race')[misconduct_tags].sum()\n\n# Heatmap of misconduct types by race with improved layout\nplt.figure(figsize=(14, 10))  # Adjust figure size for better spacing\nsns.heatmap(\n    misconduct_by_race, \n    annot=True, \n    cmap='coolwarm', \n    fmt='d', \n    linewidths=0.5, \n    cbar_kws={'label': 'Counts'},  # Add a color bar label\n    xticklabels=True, \n    yticklabels=True\n)\nplt.title(\"Breakdown of OM Types by Race\", fontsize=16)  # Larger title font size\nplt.xlabel(\"Misconduct Type\", fontsize=12)  # Larger x-axis label font size\nplt.ylabel(\"Race\", fontsize=12)  # Larger y-axis label font size\nplt.xticks(rotation=30, ha='right', fontsize=10)  # Rotate x-axis labels for readability\nplt.yticks(fontsize=10)  # Adjust y-axis label font size\nplt.tight_layout()  # Optimize layout to avoid overlap\nplt.show()\n\n\n\n\n\n\n\n\n\nThe heatmap reveals clear racial disparities in the distribution of police misconduct types. Black exonerees experience the highest counts across nearly every category, particularly in police officer misconduct, withheld exculpatory evidence, and misconduct_that_is_not_withholding_evidence, where counts exceed 300 in multiple instances. These numbers reflect a systemic pattern where Black individuals are overwhelmingly subjected to multiple forms of misconduct during wrongful convictions.\nHispanic exonerees follow but with much lower frequencies, most notably in police officer misconduct and withheld exculpatory evidence. While misconduct is still present, the lower counts compared to Black exonerees highlight the disproportionate burden carried by Black individuals.\nWhite exonerees show smaller counts across all categories, with misconduct numbers rarely exceeding 30 in any type. For Native American and Asian exonerees, instances of police misconduct are virtually nonexistent in the dataset, which may indicate smaller representation or underreporting rather than an absence of misconduct.\nThe heatmap underscores how specific types of misconduct—such as withheld evidence and direct police misconduct—are disproportionately concentrated among Black exonerees, further reflecting systemic racial biases deeply embedded within the criminal legal system."
  },
  {
    "objectID": "technical-details/eda/main.html#misconduct-tags-by-county",
    "href": "technical-details/eda/main.html#misconduct-tags-by-county",
    "title": "Exploratory Data Analysis",
    "section": "Misconduct Tags by County",
    "text": "Misconduct Tags by County\nThe following examines the distribution of misconduct types across counties using a heatmap. Misconduct tags are aggregated by county to identify how various types of official misconduct appear across different regions. Each cell in the heatmap shows the frequency of a specific misconduct type within a particular county, with a color gradient reflecting the counts. Darker shades represent higher frequencies, while lighter shades indicate fewer instances. By visualizing the data this way, the analysis highlights regional differences in misconduct patterns, allowing for a clearer understanding of which counties report higher concentrations of specific misconduct types.\n\n\nCode\n# Aggregate misconduct by county and type\nmisconduct_county = exon_df.groupby('county')[misconduct_tags].sum()\n\n# Heatmap of misconduct types by county\nplt.figure(figsize=(14, 10))\nsns.heatmap(\n    misconduct_county,\n    annot=True,\n    cmap='coolwarm',\n    fmt='.0f',  # Format numbers as integers\n    linewidths=0.5,\n    cbar_kws={'label': 'Counts'}\n)\nplt.title(\"Breakdown of Misconduct Types by County\", fontsize=16)\nplt.xlabel(\"Misconduct Type\", fontsize=12)\nplt.ylabel(\"County\", fontsize=12)\nplt.xticks(rotation=30, ha='right', fontsize=10)\nplt.yticks(fontsize=10)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe heatmap highlights significant regional disparities in the prevalence of official misconduct types across Illinois counties. Cook County stands out with the highest counts across multiple misconduct categories, particularly in police officer misconduct, withheld exculpatory evidence, and misconduct_that_is_not_withholding_evidence, where counts exceed 400. This suggests that Cook County sees a disproportionate concentration of misconduct tied to wrongful convictions.\nOther counties, such as Dekalb, also report notable frequencies in several categories, though on a smaller scale. In contrast, most other counties show far fewer instances of misconduct, with many reporting near-zero counts across most categories.\nThe stark concentration of misconduct in Cook County underscores systemic issues that may be tied to larger caseloads, greater population density, or more thoroughly documented cases compared to rural counties. These findings reflect broader patterns of geographic inequities in how misconduct is identified and recorded within the criminal legal system.\n\nMisconduct Tags by County and Race\n\n\nCode\n# Aggregate misconduct by county and race\nmisconduct_race_county = exon_df.groupby(['county', 'race'])['total_misconduct'].sum().unstack()\n\n# Heatmap of misconduct by county and race\nplt.figure(figsize=(14, 10))\nsns.heatmap(\n    misconduct_race_county,\n    annot=True, \n    cmap='viridis', \n    fmt='.0f',  \n    linewidths=0.5, \n    cbar_kws={'label': 'Total Misconduct'}\n)\nplt.title(\"Misconduct Tags by County and Race\", fontsize=16)\nplt.xlabel(\"Race\", fontsize=12)\nplt.ylabel(\"County\", fontsize=12)\nplt.xticks(rotation=30, ha='right', fontsize=10)\nplt.yticks(fontsize=10)\nplt.tight_layout()  \nplt.show()\n\n\n\n\n\n\n\n\n\nThe heatmap highlights significant racial and geographic disparities in official misconduct across Illinois counties. Cook County dominates the visualization, with Black exonerees experiencing an overwhelming concentration of misconduct tags—1,363 in total—far surpassing all other racial groups and counties. This disparity underscores the disproportionate burden placed on Black individuals in urban regions like Cook County. Hispanic exonerees in Cook County also show elevated misconduct counts, though significantly lower than their Black counterparts. In other counties, misconduct tags are fewer and more evenly distributed. Counties like Lake and Winnebago report notable misconduct counts for Black exonerees, while White exonerees appear more frequently in rural counties like LaSalle and McHenry, though at much smaller scales. The data makes clear that official misconduct disproportionately affects Black exonerees, particularly in high-population urban counties like Cook. These findings highlight both systemic racial inequities and geographic concentration of wrongful convictions tied to misconduct."
  },
  {
    "objectID": "technical-details/eda/main.html#implications-4",
    "href": "technical-details/eda/main.html#implications-4",
    "title": "Exploratory Data Analysis",
    "section": "Implications",
    "text": "Implications\nThe analysis of misconduct tags in exoneration cases, when connected with earlier arrest data, highlights a clear pattern of racialized injustice within the criminal legal system. Black individuals not only face higher rates of arrests—often tied to over-policing in predominantly Black communities—but are also subjected to disproportionate levels of official misconduct that contribute to wrongful convictions. This double burden underscores how systemic racism permeates multiple stages of the justice process, from arrest to conviction to exoneration. Geographically, the concentration of misconduct in urban counties—most notably in Cook County, home to Chicago—reflects how densely populated areas with significant Black populations experience both over-policing and elevated rates of documented misconduct. The sheer scale of misconduct tags for Black exonerees in these regions exposes systemic failures in law enforcement and prosecutorial accountability. While rural areas report fewer cases, this does not diminish their relevance. Wrongful convictions remain a systemic issue across urban and rural regions, but the scale and visibility are amplified in urban areas. The data makes clear that wrongful convictions are not isolated incidents but part of a broader pattern, with Black individuals disproportionately bearing the brunt of these injustices."
  },
  {
    "objectID": "technical-details/eda/main.html#implications-for-modeling",
    "href": "technical-details/eda/main.html#implications-for-modeling",
    "title": "Exploratory Data Analysis",
    "section": "Implications for Modeling",
    "text": "Implications for Modeling\nThe findings from this EDA provide critical insights that will guide the next stages of the project. Key patterns, such as the racial, geographic, and sentencing disparities in wrongful convictions, highlight areas where predictive modeling can be applied to uncover systemic trends and risks. For unsupervised learning, clustering methods will be explored to identify patterns of wrongful incarceration based on variables such as race, location, and misconduct tags. These clusters can provide deeper insights into how wrongful convictions manifest across different demographics and geographic regions. For supervised learning, the focus will be on building models to predict the probability of wrongful incarceration based on demographic and exoneration data. The analysis will estimate average predicted probabilities across key variables, such as race and county, to uncover patterns of systemic disparities, ultimately highlighting the risk factors most strongly associated with wrongful convictions, providing a data-driven foundation for addressing these inequities and informing targeted policy reforms."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html",
    "href": "technical-details/data-cleaning/main.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "This section outlines the steps taken to clean and preprocess the U.S. exoneration dataset, with a specific focus on Illinois cases. The cleaning process organizes the raw data into a structured and usable format for exploratory data analysis (EDA) and subsequent analysis workflows. By the end of this phase, the dataset will be well-structured, free of inconsistencies, and ready for further exploratory data analysis and machine learning workflows.\n\n\n\n\n\nThe dataset is first loaded to inspect its structure and contents. The purpose of this step is to get an initial sense of the data types, potential missing values, and the distribution of key variables which helps inform the cleaning steps required to make the data consistent and analyzable.\n\n# Import necessary Libraries:\nimport pandas as pd  # Used for data management, exploration, and manipulation\nimport numpy as np  # Used for numerical operations and array-based data processing\nimport seaborn as sns  # Used for data visualization, especially for missing values\nimport matplotlib.pyplot as plt  # Used for plotting and visualizing data\nimport re  # Used for handling and processing regular expressions, e.g., date cleaning\n\n# Load exoneration dataset:\ndf = pd.read_csv('../../data/raw-data/US_exoneration_data.csv')\nprint(\"Initial Dataset: \")\npd.set_option('display.max_columns', None) # Enables display of every column\ndf.head()\n\nInitial Dataset: \n\n\n\n\n\n\n\n\n\nLast Name\nFirst Name\nAge\nRace\nSex\nState\nCounty\nTags\nWorst Crime Display\nSentence\nPosting Date\nOM Tags\nF/MFE\nFC\nILD\nP/FA\nDNA\nMWID\nOM\nDate of Exoneration\nDate of 1st Conviction\nDate of Release\n\n\n\n\n0\nAbbitt\nJoseph\n31.0\nBlack\nMale\nNorth Carolina\nForsyth\nCV;#IO;#SA\nChild Sex Abuse\nLife\n9/1/11\nNaN\nNaN\nNaN\nNaN\nNaN\nDNA\nMWID\nNaN\n9/2/09\n6/22/95\n9/2/09\n\n\n1\nAbbott\nCinque\n19.0\nBlack\nMale\nIllinois\nCook\nCIU;#IO;#NC;#P\nDrug Possession or Sale\nProbation\n2/14/22\nOF;#WH;#NW\nNaN\nNaN\nNaN\nP/FA\nNaN\nNaN\nOM\n2/1/22\n3/25/08\n3/25/08\n\n\n2\nAbdal\nWarith Habib\n43.0\nBlack\nMale\nNew York\nErie\nIO;#SA\nSexual Assault\n20 to Life\n8/29/11\nOF;#WH;#NW;#WT\nF/MFE\nNaN\nNaN\nNaN\nDNA\nMWID\nOM\n9/1/99\n6/6/83\n9/1/99\n\n\n3\nAbernathy\nChristopher\n17.0\nWhite\nMale\nIllinois\nCook\nCIU;#CV;#H;#IO;#JV;#SA\nMurder\nLife without parole\n2/13/15\nOF;#WH;#NW;#INT\nNaN\nFC\nNaN\nP/FA\nDNA\nNaN\nOM\n2/11/15\n1/15/87\n2/11/15\n\n\n4\nAbney\nQuentin\n32.0\nBlack\nMale\nNew York\nNew York\nCV\nRobbery\n20 to Life\n5/13/19\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nMWID\nNaN\n1/19/12\n3/20/06\n1/19/12\n\n\n\n\n\n\n\n\n\n\n\nBefore diving into the broader data cleaning process, I decided to narrow the scope of my research question to Illinois. This choice was intentional to focus the analysis on a specific region, ensuring that the findings are both relevant and manageable within the scope of this project. Illinois was selected because of its extensive record of exoneration cases, particularly in Cook Count (Chicago), which provides a valuable dataset for analyzing systemic issues within the criminal justice system. Chicago, in particular, has long been associated with significant racial disparities and deeply entrenched problems in policing and prosecution, making it a critical focal point for this analysis1. By focusing on Illinois, the dataset remains consistent in terms of jurisdictional laws and practices, allowing for a more accurate and concentrated exploration of patterns and trends in over-policing and wrongful convictions. This regional focus highlights the broader systemic failures of the criminal justice system while enabling a detailed examination of one of the most historically inequitable jurisdictions in terms of racial justice.\n\n\nTo isolate Illinois cases, the dataset was filtered by the state column, retaining only rows where the value matched “Illinois.” This step reduced the dataset to 548 rows, making it more manageable for analysis and visualization. Below is a preview of the filtered dataset:\n\n# Filter Data for Illinois: \ndf = df[df['State'] == 'Illinois']\nprint(\"Number of exonerees for Illinois subset: \" , df.shape[0]) \ndf.head()\n\nNumber of exonerees for Illinois subset:  548\n\n\n\n\n\n\n\n\n\nLast Name\nFirst Name\nAge\nRace\nSex\nState\nCounty\nTags\nWorst Crime Display\nSentence\nPosting Date\nOM Tags\nF/MFE\nFC\nILD\nP/FA\nDNA\nMWID\nOM\nDate of Exoneration\nDate of 1st Conviction\nDate of Release\n\n\n\n\n1\nAbbott\nCinque\n19.0\nBlack\nMale\nIllinois\nCook\nCIU;#IO;#NC;#P\nDrug Possession or Sale\nProbation\n2/14/22\nOF;#WH;#NW\nNaN\nNaN\nNaN\nP/FA\nNaN\nNaN\nOM\n2/1/22\n3/25/08\n3/25/08\n\n\n3\nAbernathy\nChristopher\n17.0\nWhite\nMale\nIllinois\nCook\nCIU;#CV;#H;#IO;#JV;#SA\nMurder\nLife without parole\n2/13/15\nOF;#WH;#NW;#INT\nNaN\nFC\nNaN\nP/FA\nDNA\nNaN\nOM\n2/11/15\n1/15/87\n2/11/15\n\n\n5\nAbrego\nEruby\n20.0\nHispanic\nMale\nIllinois\nCook\nCDC;#H;#IO\nMurder\n90 years\n8/25/22\nOF;#WH;#NW;#WT;#INT;#PJ\nNaN\nFC\nNaN\nP/FA\nNaN\nMWID\nOM\n7/21/22\n9/22/04\n7/21/22\n\n\n10\nAdams\nDemetris\n22.0\nBlack\nMale\nIllinois\nCook\nCIU;#IO;#NC;#P\nDrug Possession or Sale\n1 year\n4/13/20\nOF;#WH;#NW\nNaN\nNaN\nNaN\nP/FA\nNaN\nNaN\nOM\n2/11/20\n9/8/04\n12/26/04\n\n\n15\nAdams\nKenneth\n22.0\nBlack\nMale\nIllinois\nCook\nCDC;#H;#IO;#JI;#SA\nMurder\n75 years\n8/29/11\nPR;#OF;#WH;#NW;#KP;#WT\nF/MFE\nNaN\nNaN\nP/FA\nDNA\nMWID\nOM\n7/2/96\n10/20/78\n6/14/96\n\n\n\n\n\n\n\n\n\n\n\n\n\nMissing values are identified using isnull() to determine their extent and distribution across the dataset. The goal is to ensure that no critical data gaps remain unaddressed before proceeding with analysis.\n\n# Managing Missing Data - Identifying which columns have a lot of missing data:\nna_counts = df.isna().sum()\nprint(na_counts)\n\nLast Name                   0\nFirst Name                  0\nAge                         1\nRace                        0\nSex                         0\nState                       0\nCounty                      0\nTags                       15\nWorst Crime Display         0\nSentence                    0\nPosting Date                0\nOM Tags                    70\nF/MFE                     474\nFC                        410\nILD                       440\nP/FA                       67\nDNA                       482\nMWID                      442\nOM                         70\nDate of Exoneration         0\nDate of 1st Conviction      0\nDate of Release             0\ndtype: int64\n\n\n\n\n\n\n\nThe following columns were removed due to excessive missing data:\n\nF/MFE, ILD, P/FA, DNA, MWID, FC: Each of these columns had more than 50% missing values, which made them unreliable for meaningful analysis. Removing them ensures the dataset remains robust and manageable without introducing bias from imputation.\n\nRetaining “OM” and “OM Tags” Columns Initially, I removed the OM (Official Misconduct) and OM Tags columns, assuming their information would be captured in the general Tags column. However, during the exploratory data analysis (EDA), I discovered that these columns contained unique and valuable insights not present in the Tags column; as a result I retained them for further analysis.\n\n# Drop columns with excessive missing values: \ndf_original = df.copy()\ndf.drop(columns = ['F/MFE', 'ILD', 'P/FA', 'DNA', 'MWID', 'FC'], inplace = True)\n\n\n\n\nTo better understand the distribution of missing values, a heatmap is generated. This visualization provides a clear overview of where missing values occur, helping to decide which columns or rows to address in subsequent steps.\n\n\nA heatmap is generated to visualize the extent of missing data before cleaning. Columns with a high proportion of missing values are easily identifiable, providing a clear justification for their removal.\n\n# Heatmap of missing data before cleaning:\nplt.figure(figsize=(12, 6))\nsns.heatmap(df_original.isnull(), cbar=False, cmap='viridis')\nplt.title('Heatmap of Missing Data (Before Cleaning)')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nA second heatmap is generated after cleaning to confirm that all unnecessary columns with excessive missing values have been removed. This ensures the dataset is now complete and ready for further analysis.\n\n# Heatmap of missing data after cleaning:\nplt.figure(figsize=(12, 6))\nsns.heatmap(df.isnull(), cbar=False, cmap='viridis')\nplt.title('Heatmap of Missing Data (After Cleaning)')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo ensure consistency and simplify future operations, all column names were standardized by converting them to lowercase and replacing spaces with underscores (_). This transformation enhances readability, aligns with Python’s naming conventions, and makes column names easier to reference in code. For example, a column originally labeled First Name is now first_name.\n\n# Standardize column names by converting to lowercase and replacing spaces with '_':\ndf.columns = df.columns.str.lower().str.replace(' ', '_')\nprint(df.columns)\n\nIndex(['last_name', 'first_name', 'age', 'race', 'sex', 'state', 'county',\n       'tags', 'worst_crime_display', 'sentence', 'posting_date', 'om_tags',\n       'om', 'date_of_exoneration', 'date_of_1st_conviction',\n       'date_of_release'],\n      dtype='object')\n\n\n\n\nAdditionally, the sex column was converted to lowercase to maintain uniformity across textual data. This step ensures that values like “Male” and “male” are treated equivalently during analysis, reducing potential discrepancies caused by case sensitivity.\n\n# Convert sex values to lowercase: \ndf['sex'] = df['sex'].str.lower()\ndf['sex'].head()\n\n1     male\n3     male\n5     male\n10    male\n15    male\nName: sex, dtype: object\n\n\n\n\n\n\nAccurate data type formatting is essential for effective analysis. This section ensures that all variables are correctly identified as numerical, categorical, or date-time types so that they are ready for further processing.\n\n# Display data types for each column:\nprint(df.dtypes)\n\nlast_name                  object\nfirst_name                 object\nage                       float64\nrace                       object\nsex                        object\nstate                      object\ncounty                     object\ntags                       object\nworst_crime_display        object\nsentence                   object\nposting_date               object\nom_tags                    object\nom                         object\ndate_of_exoneration        object\ndate_of_1st_conviction     object\ndate_of_release            object\ndtype: object\n\n\n\n\nUpon reviewing the data types, it was noted that several columns, such as Last Name, First Name, Race, State, County, and Worst Crime Display, were classified as object. While this is acceptable for textual data, converting these columns to string ensures consistency and prevents potential issues when performing text-specific operations. This transformation also allows for better optimization and clarity in the data processing pipeline. The changes were necessary to standardize the dataset and ensure compatibility with downstream analysis tasks.\n\n# Convert relevant columns to string type\nstring_columns = ['last_name', 'first_name', 'race', 'sex', 'state', 'county', 'worst_crime_display']\ndf[string_columns] = df[string_columns].astype('string')\n\n# Check updated data types\nprint(df.dtypes)\n\nlast_name                 string[python]\nfirst_name                string[python]\nage                              float64\nrace                      string[python]\nsex                       string[python]\nstate                     string[python]\ncounty                    string[python]\ntags                              object\nworst_crime_display       string[python]\nsentence                          object\nposting_date                      object\nom_tags                           object\nom                                object\ndate_of_exoneration               object\ndate_of_1st_conviction            object\ndate_of_release                   object\ndtype: object\n\n\n\n\n\nAll date columns (posting_date, date_of_exoneration, date_of_1st_conviction, date_of_release) are converted to datetime format. This transformation ensures consistency and allows for easier time-based calculations, such as measuring the time between conviction and exoneration.\n\n# Convert date columns into datetime format:\nfor col in ['posting_date', 'date_of_exoneration', 'date_of_1st_conviction', 'date_of_release']:\n    # Convert with explicit format (MM/DD/YY):\n    df[col] = pd.to_datetime(df[col], format='%m/%d/%y', errors='coerce')\n\nprint(df[['posting_date', 'date_of_exoneration', 'date_of_1st_conviction', 'date_of_release']].head())\n\n   posting_date date_of_exoneration date_of_1st_conviction date_of_release\n1    2022-02-14          2022-02-01             2008-03-25      2008-03-25\n3    2015-02-13          2015-02-11             1987-01-15      2015-02-11\n5    2022-08-25          2022-07-21             2004-09-22      2022-07-21\n10   2020-04-13          2020-02-11             2004-09-08      2004-12-26\n15   2011-08-29          1996-07-02             1978-10-20      1996-06-14\n\n\n\n# Verify updated data types:\nprint(df.dtypes)\n\nlast_name                 string[python]\nfirst_name                string[python]\nage                              float64\nrace                      string[python]\nsex                       string[python]\nstate                     string[python]\ncounty                    string[python]\ntags                              object\nworst_crime_display       string[python]\nsentence                          object\nposting_date              datetime64[ns]\nom_tags                           object\nom                                object\ndate_of_exoneration       datetime64[ns]\ndate_of_1st_conviction    datetime64[ns]\ndate_of_release           datetime64[ns]\ndtype: object\n\n\n\n\n\nThe sentence column contains textual descriptions of sentencing outcomes, including terms like “Life without parole,” “Death,” or a specified number of years. To facilitate analysis, this column is transformed into a numerical format (sentence_in_years) by converting life sentences and probation to placeholder values and handling ranges or mixed units (e.g., years and months).\n\n# Print unique sentencing values for a better idea on how to best clean column: \nunique_sentences = df['sentence'].unique()\nprint(unique_sentences)\n\n['Probation' 'Life without parole' '90 years' '1 year' '75 years'\n '30 years' '55 years' '2 years' '3 years' '6 years' '45 years'\n '1 year and 6 months' '50 years' '60 years' 'Life' '80 years' '18 years'\n '4 years' '85 years' '20 years' '35 years' '2 years and 6 months'\n '82 years' '12 years' 'Not sentenced' '22 years' '32 years' 'Death'\n '5 years' '40 years' '25 years' '26 years' '4 years and 6 months'\n '9 years' '48 years' '30 days' '84 years' '3 months and 25 days'\n '2 years and 2 months' '3 months' '44 years' '6 months' '25 to 50 years'\n '29 years' '23 years' '31 years' '11 years' '8 years' '24 years'\n '3 years and four months' '42 years' '3 years and 6 months' '65 years'\n '76 years' '15 years' '50 to Life' '86 years' '70 years' '28 years'\n '13 years' '47 years' '36 years' '18 months' '1 year and 4 months'\n '8 years and 6 months' '6 years and 6 months' '58 years' '95 years'\n '7 years' '34 years' '62 years' '27 years' '69 years' '57 years'\n '50 to 100 years' '4 months' '4 years and 3 months' '37 years' '10 years'\n '67 years' '46 years' '17 years' '10 to 22 years' '6 years and 7 months'\n '5 years and 6 months' '2 Years']\n\n\n\n\nThe sentence column is cleaned to convert textual descriptions into numerical values: 1. Probation and Not Sentenced are set to 0. 2. Life sentences and the death penalty are represented as 100 for placeholder analysis. 3. Ranges (e.g., “25 to 50 years”) are averaged to a single value. 4. Years and months are combined into total years for uniformity.\nThis standardization facilitates meaningful comparisons and quantitative analysis of sentencing patterns.\n\ndef clean_sentence(value):\n    \"\"\" Cleans the 'sentence' column values to numeric years for numerical EDA \n    - Probation is represented as 0.\n    - 'Not sentenced' is converted to np.nan.\n    - 'Life' and 'Death' sentences are represented as 100 (placeholder).\n    - Years and months are converted to a numeric value in years. \"\"\"\n\n    if value == 'Probation':\n        return 0\n    elif value == 'Not sentenced':\n        return np.nan  # NaN for not sentenced\n    elif 'Life' in value or value == 'Death':\n        return 100  # Placeholder for life sentences or death penalty\n    elif 'year' in value or 'month' in value:\n\n        # Handles ranges like '25 to 50 years'\n        if 'to' in value:\n            years = [int(num) for num in re.findall(r'\\d+', value)]\n            return sum(years) / len(years)  # Average the range \n        \n        # Handle \"X years and Y months\"\n        elif 'and' in value:\n            numbers = [float(num) for num in re.findall(r'\\d+', value)]\n            if len(numbers) == 2:  # Both years and months are present\n                years, months = numbers\n                return years + (months / 12)  # Convert months to years\n            elif len(numbers) == 1:  # Only one number is present\n                return numbers[0]  # Treat it as years\n            \n        # Handle only months or only years\n        elif 'months' in value:\n            months = int(re.search(r'\\d+', value).group())\n            return months / 12  # Convert months to years\n        else:  # Only years\n            return int(re.search(r'\\d+', value).group())\n    else:\n        return np.nan  # Anything unexpected as None\n    \ndf['sentence_in_years'] = df['sentence'].apply(clean_sentence)\n\n# Check results\ndf[['sentence', 'sentence_in_years']].head(10)\n\n\n\n\n\n\n\n\nsentence\nsentence_in_years\n\n\n\n\n1\nProbation\n0.0\n\n\n3\nLife without parole\n100.0\n\n\n5\n90 years\n90.0\n\n\n10\n1 year\n1.0\n\n\n15\n75 years\n75.0\n\n\n21\nProbation\n0.0\n\n\n22\nProbation\n0.0\n\n\n24\n30 years\n30.0\n\n\n25\n55 years\n55.0\n\n\n45\n1 year\n1.0\n\n\n\n\n\n\n\n\n# Check updated data types\nprint(df.dtypes)\n\nlast_name                 string[python]\nfirst_name                string[python]\nage                              float64\nrace                      string[python]\nsex                       string[python]\nstate                     string[python]\ncounty                    string[python]\ntags                              object\nworst_crime_display       string[python]\nsentence                          object\nposting_date              datetime64[ns]\nom_tags                           object\nom                                object\ndate_of_exoneration       datetime64[ns]\ndate_of_1st_conviction    datetime64[ns]\ndate_of_release           datetime64[ns]\nsentence_in_years                float64\ndtype: object\n\n\nAfter transforming the sentence column into a numerical format, the new column, sentence_in_years is now represented as a float64, which aligns with the desired structure for numerical analysis. This conversion allows for quantitative exploration of the sentencing data, such as aggregations and comparisons, during later stages of analysis. The original sentence column is retained for reference purposes, as it preserves the detailed textual descriptions that might be useful for contextual insights. The tags, OM, and OM_tags columns will be addressed later, so for now the datatype may remain as an object.\n\n\n\n\n\nThe tags and OM-tags columns contain important categorical information about each exoneration case. To make this data more useful for analysis, both columns were transformed into multiple binary columns, where each tag indicates the presence (1) or absence (0) of a specific feature. Additionally, a tag_sum column was created to capture the total number of tags associated with each case, providing a summary metric.\nThe cleaning process involved the following steps:\n\nRemoving Unnecessary Characters:\nUnwanted characters such as # were removed, and delimiters were standardized to ensure consistency in the data.\nSplitting Tags:\nThe tags and OM-tags columns were split into individual values to facilitate binary encoding.\nRenaming Binary Columns:\nEach binary column was renamed using clear and descriptive labels by mapping the original tags to their definitions. This mapping process translated short tag codes into their full meanings, improving interpretability. For reference, the definitions of the tags are based on the descriptions provided by the National Registry of Exonerations2.\nAdding a tag_sum Column:\nA new column was created to calculate the total number of tags for each case, enabling easier analysis of case complexity.\n\nThis transformation ensures the data is well-structured and ready for exploratory analysis, providing detailed insights into the systemic patterns in exoneration cases.\n\n# Clean 'tags' column:\ndf['tags'] = df['tags'].str.replace('#', '', regex=False).str.replace(\";\", \",\")\ndf['OM-tags'] = df['om_tags'].str.replace('#', '', regex=False).str.replace(\";\", \",\")\n\n# Define the mapping for tags:\ntag_mapping = {\n    \"A\": \"arson\",\n    \"BM\": \"bitemark\",\n    \"CDC\": \"co_defendant_confessed\",\n    \"CIU\": \"conviction_integrity_unit\",\n    \"CSH\": \"child_sex_abuse_hysteria_case\",\n    \"CV\": \"child_victim\",\n    \"F\": \"female_exoneree\",\n    \"FED\": \"federal_case\",\n    \"H\": \"homicide\",\n    \"IO\": \"innocence_organization\",\n    \"JI\": \"jailhouse_informant\",\n    \"JV\": \"juvenile_defendant\",\n    \"M\": \"misdemeanor\",\n    \"NC\": \"no_crime_case\",\n    \"P\": \"guilty_plea_case\",\n    \"PH\": \"posthumous_exoneration\",\n    \"SA\": \"sexual_assault\",\n    \"SBS\": \"shaken_baby_syndrome_case\",\n    \"PR\": \"prosecutor_misconduct\",\n    \"OF\": \"police_officer_misconduct\",\n    \"FA\": \"forensic_analyst_misconduct\",\n    \"CW\": \"child_welfare_worker_misconduct\",\n    \"WH\": \"withheld_exculpatory_evidence\",\n    \"NW\": \"misconduct_that_is_not_withholding_evidence\",\n    \"KP\": \"knowingly_permitting_perjury\",\n    \"WT\": \"witness_tampering_or_misconduct_interrogating_co_defendant\",\n    \"INT\": \"misconduct_in_interrogation_of_exoneree\",\n    \"PJ\": \"perjury_by_official\",\n    \"PL\": \"prosecutor_lied_in_court\"\n}\n\n# Split 'tags' and 'OM-tags' into lists:\ndf['tags'] = df['tags'].apply(lambda x: x.split(',') if isinstance(x, str) else x)\ndf['OM-tags'] = df['OM-tags'].apply(lambda x: x.split(',') if isinstance(x, str) else x)\n\n# Create binary columns for tags from both 'tags' and 'OM-tags':\nfor tag in tag_mapping.keys():\n    # Check if the tag exists in 'tags' or 'OM-tags':\n    df[tag] = df.apply(\n        lambda row: 1 if (isinstance(row['tags'], list) and tag in row['tags']) or \n                          (isinstance(row['OM-tags'], list) and tag in row['OM-tags']) else 0,\n        axis=1\n    )\n\n# Rename the binary columns using the tag_mapping dictionary:\ndf.rename(columns=tag_mapping, inplace=True)\n\n# Create `tag_sum` column to count the total number of tags for each exoneree:\ndf['tag_sum'] = df[list(tag_mapping.values())].sum(axis=1)\n\n# Drop the original 'tags' and 'OM-tags' columns:\ndf.drop(columns=['tags', 'om_tags', 'OM-tags'], inplace=True)\n\ndf.head()  \n\n\n\n\n\n\n\n\nlast_name\nfirst_name\nage\nrace\nsex\nstate\ncounty\nworst_crime_display\nsentence\nposting_date\nom\ndate_of_exoneration\ndate_of_1st_conviction\ndate_of_release\nsentence_in_years\narson\nbitemark\nco_defendant_confessed\nconviction_integrity_unit\nchild_sex_abuse_hysteria_case\nchild_victim\nfemale_exoneree\nfederal_case\nhomicide\ninnocence_organization\njailhouse_informant\njuvenile_defendant\nmisdemeanor\nno_crime_case\nguilty_plea_case\nposthumous_exoneration\nsexual_assault\nshaken_baby_syndrome_case\nprosecutor_misconduct\npolice_officer_misconduct\nforensic_analyst_misconduct\nchild_welfare_worker_misconduct\nwithheld_exculpatory_evidence\nmisconduct_that_is_not_withholding_evidence\nknowingly_permitting_perjury\nwitness_tampering_or_misconduct_interrogating_co_defendant\nmisconduct_in_interrogation_of_exoneree\nperjury_by_official\nprosecutor_lied_in_court\ntag_sum\n\n\n\n\n1\nAbbott\nCinque\n19.0\nBlack\nmale\nIllinois\nCook\nDrug Possession or Sale\nProbation\n2022-02-14\nOM\n2022-02-01\n2008-03-25\n2008-03-25\n0.0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n7\n\n\n3\nAbernathy\nChristopher\n17.0\nWhite\nmale\nIllinois\nCook\nMurder\nLife without parole\n2015-02-13\nOM\n2015-02-11\n1987-01-15\n2015-02-11\n100.0\n0\n0\n0\n1\n0\n1\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n1\n0\n0\n1\n0\n0\n10\n\n\n5\nAbrego\nEruby\n20.0\nHispanic\nmale\nIllinois\nCook\nMurder\n90 years\n2022-08-25\nOM\n2022-07-21\n2004-09-22\n2022-07-21\n90.0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n1\n1\n1\n0\n9\n\n\n10\nAdams\nDemetris\n22.0\nBlack\nmale\nIllinois\nCook\nDrug Possession or Sale\n1 year\n2020-04-13\nOM\n2020-02-11\n2004-09-08\n2004-12-26\n1.0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n7\n\n\n15\nAdams\nKenneth\n22.0\nBlack\nmale\nIllinois\nCook\nMurder\n75 years\n2011-08-29\nOM\n1996-07-02\n1978-10-20\n1996-06-14\n75.0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n1\n0\n1\n1\n0\n0\n1\n1\n1\n1\n0\n0\n0\n11\n\n\n\n\n\n\n\n\n# Convert 'OM' column to binary (1 if \"OM\" is present, 0 otherwise)\ndf['om'] = df['om'].apply(lambda x: 1 if str(x).strip().upper() == \"OM\" else 0)\n\n# Verify the transformation\nprint(df['om'].value_counts())\n\nom\n1    478\n0     70\nName: count, dtype: int64\n\n\n\n\n\nThe geocoded Illinois counties from Data Collection were merged into the main dataset:\n\nLoad and Standardize Data: Geocoded data was loaded, and column names were standardized to lowercase for consistency.\nFilter Relevant Counties: The geocoded data was filtered to include only counties present in the main dataset.\nMerge Data: Using a left join on county and state, geographic details (geocode_address, latitude, and longitude) were added to the dataset.\n\n\n# Read the geocoded population data from Data Collection\ngeocode_unique = pd.read_csv(\"../../data/raw-data/geocoded_population_counties.csv\")\n\n# Rename columns to lowercase for consistency\ngeocode_unique.rename(columns={\"County\": \"county\", \"State\": \"state\"}, inplace=True)\n\n# Filter geocode_unique to only include counties present in df\ngeocode_unique_filtered = geocode_unique[\n    geocode_unique[['county', 'state']].apply(tuple, axis=1).isin(df[['county', 'state']].apply(tuple, axis=1))\n]\n\n# Merge the filtered geocoding data into df\ndf = df.merge(geocode_unique_filtered, on=['county', 'state'], how='left')\n\n# Display the relevant columns to verify the merge\nprint(df[['state', 'county', 'geocode_address', 'latitude', 'longitude']].head())\n\n      state county                       geocode_address   latitude  longitude\n0  Illinois   Cook  Cook County, Illinois, United States  41.819738 -87.756525\n1  Illinois   Cook  Cook County, Illinois, United States  41.819738 -87.756525\n2  Illinois   Cook  Cook County, Illinois, United States  41.819738 -87.756525\n3  Illinois   Cook  Cook County, Illinois, United States  41.819738 -87.756525\n4  Illinois   Cook  Cook County, Illinois, United States  41.819738 -87.756525\n\n\n\n\n\nTo quantify the years_lost due to wrongful conviction, this step calculates the difference in years between an individual’s date_of_1st_conviction and their date_of_release.\n\n# Calculate \"years lost\" as the difference in years between release and conviction:\ndf['years_lost'] = (df['date_of_release'] - df['date_of_1st_conviction']).dt.days / 365.25 # Dividing by 365.25 accounts for leap years\n\n# Round the years lost to 2 decimal places:\ndf['years_lost'] = df['years_lost'].round(2)\n\n# Updated DataFrame:\nprint(df[['date_of_1st_conviction', 'date_of_release', 'years_lost']])\n\n    date_of_1st_conviction date_of_release  years_lost\n0               2008-03-25      2008-03-25        0.00\n1               1987-01-15      2015-02-11       28.07\n2               2004-09-22      2022-07-21       17.83\n3               2004-09-08      2004-12-26        0.30\n4               1978-10-20      1996-06-14       17.65\n..                     ...             ...         ...\n543             2005-04-13      2005-04-13        0.00\n544             2005-01-11      2006-07-12        1.50\n545             2005-04-11      2006-12-07        1.66\n546             2003-04-21      2005-03-10        1.89\n547             1994-09-20      2005-01-31       10.37\n\n[548 rows x 3 columns]\n\n\n\n\n\nTo improve readability and logical flow, the following changes were made to the column order:\n\nAlign Sentencing Data:\nThe sentence_in_years column was moved to appear immediately after sentence, ensuring that the cleaned numerical representation of sentencing data is logically aligned with its original textual description.\nReorganize Release and Years Lost:\nThe years_lost column was moved to appear immediately after date_of_release, facilitating easier comparison of release dates and the calculated time lost due to wrongful incarceration.\nGroup Geographic Data:\nThe latitude and longitude columns were moved to follow the county column, grouping geographic information together.\n\n\n# Reordering columns:\ncolumns = list(df.columns)  \n\n#Aligning sentencing data:  \ncolumns.insert(columns.index('sentence') + 1, columns.pop(columns.index('sentence_in_years')))  # Move 'sentence_in_years'\n\n#Reorganizing release and years lost\ncolumns.insert(columns.index('date_of_release') +1, columns.pop(columns.index('years_lost'))) #Move 'years_lost' \n\n# Move 'latitude' and 'longitude' after 'county'\ncolumns.insert(columns.index('county') + 1, columns.pop(columns.index('latitude')))\ncolumns.insert(columns.index('county') + 2, columns.pop(columns.index('longitude')))\n\ndf = df[columns]  # Reorder DataFrame\n\ndf.head(10)\n\n\n\n\n\n\n\n\nlast_name\nfirst_name\nage\nrace\nsex\nstate\ncounty\nlatitude\nlongitude\nworst_crime_display\nsentence\nsentence_in_years\nposting_date\nom\ndate_of_exoneration\ndate_of_1st_conviction\ndate_of_release\nyears_lost\narson\nbitemark\nco_defendant_confessed\nconviction_integrity_unit\nchild_sex_abuse_hysteria_case\nchild_victim\nfemale_exoneree\nfederal_case\nhomicide\ninnocence_organization\njailhouse_informant\njuvenile_defendant\nmisdemeanor\nno_crime_case\nguilty_plea_case\nposthumous_exoneration\nsexual_assault\nshaken_baby_syndrome_case\nprosecutor_misconduct\npolice_officer_misconduct\nforensic_analyst_misconduct\nchild_welfare_worker_misconduct\nwithheld_exculpatory_evidence\nmisconduct_that_is_not_withholding_evidence\nknowingly_permitting_perjury\nwitness_tampering_or_misconduct_interrogating_co_defendant\nmisconduct_in_interrogation_of_exoneree\nperjury_by_official\nprosecutor_lied_in_court\ntag_sum\ngeocode_address\n\n\n\n\n0\nAbbott\nCinque\n19.0\nBlack\nmale\nIllinois\nCook\n41.819738\n-87.756525\nDrug Possession or Sale\nProbation\n0.0\n2022-02-14\n1\n2022-02-01\n2008-03-25\n2008-03-25\n0.00\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n7\nCook County, Illinois, United States\n\n\n1\nAbernathy\nChristopher\n17.0\nWhite\nmale\nIllinois\nCook\n41.819738\n-87.756525\nMurder\nLife without parole\n100.0\n2015-02-13\n1\n2015-02-11\n1987-01-15\n2015-02-11\n28.07\n0\n0\n0\n1\n0\n1\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n1\n0\n0\n1\n0\n0\n10\nCook County, Illinois, United States\n\n\n2\nAbrego\nEruby\n20.0\nHispanic\nmale\nIllinois\nCook\n41.819738\n-87.756525\nMurder\n90 years\n90.0\n2022-08-25\n1\n2022-07-21\n2004-09-22\n2022-07-21\n17.83\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n1\n1\n1\n0\n9\nCook County, Illinois, United States\n\n\n3\nAdams\nDemetris\n22.0\nBlack\nmale\nIllinois\nCook\n41.819738\n-87.756525\nDrug Possession or Sale\n1 year\n1.0\n2020-04-13\n1\n2020-02-11\n2004-09-08\n2004-12-26\n0.30\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n7\nCook County, Illinois, United States\n\n\n4\nAdams\nKenneth\n22.0\nBlack\nmale\nIllinois\nCook\n41.819738\n-87.756525\nMurder\n75 years\n75.0\n2011-08-29\n1\n1996-07-02\n1978-10-20\n1996-06-14\n17.65\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n1\n0\n1\n1\n0\n0\n1\n1\n1\n1\n0\n0\n0\n11\nCook County, Illinois, United States\n\n\n5\nAdams\nSeneca\n20.0\nBlack\nmale\nIllinois\nCook\n41.819738\n-87.756525\nAssault\nProbation\n0.0\n2014-12-08\n1\n2006-12-19\n2006-05-18\n2006-05-18\n0.00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n1\n0\n6\nCook County, Illinois, United States\n\n\n6\nAdams\nTari\n18.0\nBlack\nmale\nIllinois\nCook\n41.819738\n-87.756525\nAssault\nProbation\n0.0\n2014-12-08\n1\n2006-12-19\n2006-05-18\n2006-05-18\n0.00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n1\n0\n6\nCook County, Illinois, United States\n\n\n7\nAgnew\nGregory\n28.0\nBlack\nmale\nIllinois\nLake\n42.332738\n-87.993955\nRobbery\n30 years\n30.0\n2018-01-18\n0\n2001-11-07\n1988-06-14\n2001-11-07\n13.40\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\nLake County, Illinois, United States\n\n\n8\nAguirre\nOmar\n28.0\nHispanic\nmale\nIllinois\nCook\n41.819738\n-87.756525\nMurder\n55 years\n55.0\n2011-08-29\n1\n2003-02-18\n1999-03-09\n2002-12-18\n3.78\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n1\n1\n0\n0\n7\nCook County, Illinois, United States\n\n\n9\nAli\nChauncey\n37.0\nBlack\nmale\nIllinois\nCook\n41.819738\n-87.756525\nDrug Possession or Sale\n1 year\n1.0\n2020-04-13\n1\n2020-02-11\n2007-01-17\n2007-06-27\n0.44\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n7\nCook County, Illinois, United States\n\n\n\n\n\n\n\n\n\n\nThe final cleaned dataset is saved as illinois_exoneration_data.csv, ensuring that all preprocessing steps are reproducible, and the dataset can be used consistently across various analysis stages.\n\ndf.to_csv('../../data/processed-data/illinois_exoneration_data.csv', index=False)\nprint(\"Data saved to 'illinois_exoneration_data.csv'\")\n\nData saved to 'illinois_exoneration_data.csv'"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#introduction-and-motivation",
    "href": "technical-details/data-cleaning/main.html#introduction-and-motivation",
    "title": "Data Cleaning",
    "section": "",
    "text": "This section outlines the steps taken to clean and preprocess the U.S. exoneration dataset, with a specific focus on Illinois cases. The cleaning process organizes the raw data into a structured and usable format for exploratory data analysis (EDA) and subsequent analysis workflows. By the end of this phase, the dataset will be well-structured, free of inconsistencies, and ready for further exploratory data analysis and machine learning workflows."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#initial-overview-of-the-dataset",
    "href": "technical-details/data-cleaning/main.html#initial-overview-of-the-dataset",
    "title": "Data Cleaning",
    "section": "",
    "text": "The dataset is first loaded to inspect its structure and contents. The purpose of this step is to get an initial sense of the data types, potential missing values, and the distribution of key variables which helps inform the cleaning steps required to make the data consistent and analyzable.\n\n# Import necessary Libraries:\nimport pandas as pd  # Used for data management, exploration, and manipulation\nimport numpy as np  # Used for numerical operations and array-based data processing\nimport seaborn as sns  # Used for data visualization, especially for missing values\nimport matplotlib.pyplot as plt  # Used for plotting and visualizing data\nimport re  # Used for handling and processing regular expressions, e.g., date cleaning\n\n# Load exoneration dataset:\ndf = pd.read_csv('../../data/raw-data/US_exoneration_data.csv')\nprint(\"Initial Dataset: \")\npd.set_option('display.max_columns', None) # Enables display of every column\ndf.head()\n\nInitial Dataset: \n\n\n\n\n\n\n\n\n\nLast Name\nFirst Name\nAge\nRace\nSex\nState\nCounty\nTags\nWorst Crime Display\nSentence\nPosting Date\nOM Tags\nF/MFE\nFC\nILD\nP/FA\nDNA\nMWID\nOM\nDate of Exoneration\nDate of 1st Conviction\nDate of Release\n\n\n\n\n0\nAbbitt\nJoseph\n31.0\nBlack\nMale\nNorth Carolina\nForsyth\nCV;#IO;#SA\nChild Sex Abuse\nLife\n9/1/11\nNaN\nNaN\nNaN\nNaN\nNaN\nDNA\nMWID\nNaN\n9/2/09\n6/22/95\n9/2/09\n\n\n1\nAbbott\nCinque\n19.0\nBlack\nMale\nIllinois\nCook\nCIU;#IO;#NC;#P\nDrug Possession or Sale\nProbation\n2/14/22\nOF;#WH;#NW\nNaN\nNaN\nNaN\nP/FA\nNaN\nNaN\nOM\n2/1/22\n3/25/08\n3/25/08\n\n\n2\nAbdal\nWarith Habib\n43.0\nBlack\nMale\nNew York\nErie\nIO;#SA\nSexual Assault\n20 to Life\n8/29/11\nOF;#WH;#NW;#WT\nF/MFE\nNaN\nNaN\nNaN\nDNA\nMWID\nOM\n9/1/99\n6/6/83\n9/1/99\n\n\n3\nAbernathy\nChristopher\n17.0\nWhite\nMale\nIllinois\nCook\nCIU;#CV;#H;#IO;#JV;#SA\nMurder\nLife without parole\n2/13/15\nOF;#WH;#NW;#INT\nNaN\nFC\nNaN\nP/FA\nDNA\nNaN\nOM\n2/11/15\n1/15/87\n2/11/15\n\n\n4\nAbney\nQuentin\n32.0\nBlack\nMale\nNew York\nNew York\nCV\nRobbery\n20 to Life\n5/13/19\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nMWID\nNaN\n1/19/12\n3/20/06\n1/19/12"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#subsetting-the-data-illinois",
    "href": "technical-details/data-cleaning/main.html#subsetting-the-data-illinois",
    "title": "Data Cleaning",
    "section": "",
    "text": "Before diving into the broader data cleaning process, I decided to narrow the scope of my research question to Illinois. This choice was intentional to focus the analysis on a specific region, ensuring that the findings are both relevant and manageable within the scope of this project. Illinois was selected because of its extensive record of exoneration cases, particularly in Cook Count (Chicago), which provides a valuable dataset for analyzing systemic issues within the criminal justice system. Chicago, in particular, has long been associated with significant racial disparities and deeply entrenched problems in policing and prosecution, making it a critical focal point for this analysis1. By focusing on Illinois, the dataset remains consistent in terms of jurisdictional laws and practices, allowing for a more accurate and concentrated exploration of patterns and trends in over-policing and wrongful convictions. This regional focus highlights the broader systemic failures of the criminal justice system while enabling a detailed examination of one of the most historically inequitable jurisdictions in terms of racial justice.\n\n\nTo isolate Illinois cases, the dataset was filtered by the state column, retaining only rows where the value matched “Illinois.” This step reduced the dataset to 548 rows, making it more manageable for analysis and visualization. Below is a preview of the filtered dataset:\n\n# Filter Data for Illinois: \ndf = df[df['State'] == 'Illinois']\nprint(\"Number of exonerees for Illinois subset: \" , df.shape[0]) \ndf.head()\n\nNumber of exonerees for Illinois subset:  548\n\n\n\n\n\n\n\n\n\nLast Name\nFirst Name\nAge\nRace\nSex\nState\nCounty\nTags\nWorst Crime Display\nSentence\nPosting Date\nOM Tags\nF/MFE\nFC\nILD\nP/FA\nDNA\nMWID\nOM\nDate of Exoneration\nDate of 1st Conviction\nDate of Release\n\n\n\n\n1\nAbbott\nCinque\n19.0\nBlack\nMale\nIllinois\nCook\nCIU;#IO;#NC;#P\nDrug Possession or Sale\nProbation\n2/14/22\nOF;#WH;#NW\nNaN\nNaN\nNaN\nP/FA\nNaN\nNaN\nOM\n2/1/22\n3/25/08\n3/25/08\n\n\n3\nAbernathy\nChristopher\n17.0\nWhite\nMale\nIllinois\nCook\nCIU;#CV;#H;#IO;#JV;#SA\nMurder\nLife without parole\n2/13/15\nOF;#WH;#NW;#INT\nNaN\nFC\nNaN\nP/FA\nDNA\nNaN\nOM\n2/11/15\n1/15/87\n2/11/15\n\n\n5\nAbrego\nEruby\n20.0\nHispanic\nMale\nIllinois\nCook\nCDC;#H;#IO\nMurder\n90 years\n8/25/22\nOF;#WH;#NW;#WT;#INT;#PJ\nNaN\nFC\nNaN\nP/FA\nNaN\nMWID\nOM\n7/21/22\n9/22/04\n7/21/22\n\n\n10\nAdams\nDemetris\n22.0\nBlack\nMale\nIllinois\nCook\nCIU;#IO;#NC;#P\nDrug Possession or Sale\n1 year\n4/13/20\nOF;#WH;#NW\nNaN\nNaN\nNaN\nP/FA\nNaN\nNaN\nOM\n2/11/20\n9/8/04\n12/26/04\n\n\n15\nAdams\nKenneth\n22.0\nBlack\nMale\nIllinois\nCook\nCDC;#H;#IO;#JI;#SA\nMurder\n75 years\n8/29/11\nPR;#OF;#WH;#NW;#KP;#WT\nF/MFE\nNaN\nNaN\nP/FA\nDNA\nMWID\nOM\n7/2/96\n10/20/78\n6/14/96"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#handling-missing-data",
    "href": "technical-details/data-cleaning/main.html#handling-missing-data",
    "title": "Data Cleaning",
    "section": "",
    "text": "Missing values are identified using isnull() to determine their extent and distribution across the dataset. The goal is to ensure that no critical data gaps remain unaddressed before proceeding with analysis.\n\n# Managing Missing Data - Identifying which columns have a lot of missing data:\nna_counts = df.isna().sum()\nprint(na_counts)\n\nLast Name                   0\nFirst Name                  0\nAge                         1\nRace                        0\nSex                         0\nState                       0\nCounty                      0\nTags                       15\nWorst Crime Display         0\nSentence                    0\nPosting Date                0\nOM Tags                    70\nF/MFE                     474\nFC                        410\nILD                       440\nP/FA                       67\nDNA                       482\nMWID                      442\nOM                         70\nDate of Exoneration         0\nDate of 1st Conviction      0\nDate of Release             0\ndtype: int64\n\n\n\n\n\n\n\nThe following columns were removed due to excessive missing data:\n\nF/MFE, ILD, P/FA, DNA, MWID, FC: Each of these columns had more than 50% missing values, which made them unreliable for meaningful analysis. Removing them ensures the dataset remains robust and manageable without introducing bias from imputation.\n\nRetaining “OM” and “OM Tags” Columns Initially, I removed the OM (Official Misconduct) and OM Tags columns, assuming their information would be captured in the general Tags column. However, during the exploratory data analysis (EDA), I discovered that these columns contained unique and valuable insights not present in the Tags column; as a result I retained them for further analysis.\n\n# Drop columns with excessive missing values: \ndf_original = df.copy()\ndf.drop(columns = ['F/MFE', 'ILD', 'P/FA', 'DNA', 'MWID', 'FC'], inplace = True)\n\n\n\n\nTo better understand the distribution of missing values, a heatmap is generated. This visualization provides a clear overview of where missing values occur, helping to decide which columns or rows to address in subsequent steps.\n\n\nA heatmap is generated to visualize the extent of missing data before cleaning. Columns with a high proportion of missing values are easily identifiable, providing a clear justification for their removal.\n\n# Heatmap of missing data before cleaning:\nplt.figure(figsize=(12, 6))\nsns.heatmap(df_original.isnull(), cbar=False, cmap='viridis')\nplt.title('Heatmap of Missing Data (Before Cleaning)')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nA second heatmap is generated after cleaning to confirm that all unnecessary columns with excessive missing values have been removed. This ensures the dataset is now complete and ready for further analysis.\n\n# Heatmap of missing data after cleaning:\nplt.figure(figsize=(12, 6))\nsns.heatmap(df.isnull(), cbar=False, cmap='viridis')\nplt.title('Heatmap of Missing Data (After Cleaning)')\nplt.show()"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#column-standardization",
    "href": "technical-details/data-cleaning/main.html#column-standardization",
    "title": "Data Cleaning",
    "section": "",
    "text": "To ensure consistency and simplify future operations, all column names were standardized by converting them to lowercase and replacing spaces with underscores (_). This transformation enhances readability, aligns with Python’s naming conventions, and makes column names easier to reference in code. For example, a column originally labeled First Name is now first_name.\n\n# Standardize column names by converting to lowercase and replacing spaces with '_':\ndf.columns = df.columns.str.lower().str.replace(' ', '_')\nprint(df.columns)\n\nIndex(['last_name', 'first_name', 'age', 'race', 'sex', 'state', 'county',\n       'tags', 'worst_crime_display', 'sentence', 'posting_date', 'om_tags',\n       'om', 'date_of_exoneration', 'date_of_1st_conviction',\n       'date_of_release'],\n      dtype='object')\n\n\n\n\nAdditionally, the sex column was converted to lowercase to maintain uniformity across textual data. This step ensures that values like “Male” and “male” are treated equivalently during analysis, reducing potential discrepancies caused by case sensitivity.\n\n# Convert sex values to lowercase: \ndf['sex'] = df['sex'].str.lower()\ndf['sex'].head()\n\n1     male\n3     male\n5     male\n10    male\n15    male\nName: sex, dtype: object"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#data-type-correction-and-formatting",
    "href": "technical-details/data-cleaning/main.html#data-type-correction-and-formatting",
    "title": "Data Cleaning",
    "section": "",
    "text": "Accurate data type formatting is essential for effective analysis. This section ensures that all variables are correctly identified as numerical, categorical, or date-time types so that they are ready for further processing.\n\n# Display data types for each column:\nprint(df.dtypes)\n\nlast_name                  object\nfirst_name                 object\nage                       float64\nrace                       object\nsex                        object\nstate                      object\ncounty                     object\ntags                       object\nworst_crime_display        object\nsentence                   object\nposting_date               object\nom_tags                    object\nom                         object\ndate_of_exoneration        object\ndate_of_1st_conviction     object\ndate_of_release            object\ndtype: object\n\n\n\n\nUpon reviewing the data types, it was noted that several columns, such as Last Name, First Name, Race, State, County, and Worst Crime Display, were classified as object. While this is acceptable for textual data, converting these columns to string ensures consistency and prevents potential issues when performing text-specific operations. This transformation also allows for better optimization and clarity in the data processing pipeline. The changes were necessary to standardize the dataset and ensure compatibility with downstream analysis tasks.\n\n# Convert relevant columns to string type\nstring_columns = ['last_name', 'first_name', 'race', 'sex', 'state', 'county', 'worst_crime_display']\ndf[string_columns] = df[string_columns].astype('string')\n\n# Check updated data types\nprint(df.dtypes)\n\nlast_name                 string[python]\nfirst_name                string[python]\nage                              float64\nrace                      string[python]\nsex                       string[python]\nstate                     string[python]\ncounty                    string[python]\ntags                              object\nworst_crime_display       string[python]\nsentence                          object\nposting_date                      object\nom_tags                           object\nom                                object\ndate_of_exoneration               object\ndate_of_1st_conviction            object\ndate_of_release                   object\ndtype: object\n\n\n\n\n\nAll date columns (posting_date, date_of_exoneration, date_of_1st_conviction, date_of_release) are converted to datetime format. This transformation ensures consistency and allows for easier time-based calculations, such as measuring the time between conviction and exoneration.\n\n# Convert date columns into datetime format:\nfor col in ['posting_date', 'date_of_exoneration', 'date_of_1st_conviction', 'date_of_release']:\n    # Convert with explicit format (MM/DD/YY):\n    df[col] = pd.to_datetime(df[col], format='%m/%d/%y', errors='coerce')\n\nprint(df[['posting_date', 'date_of_exoneration', 'date_of_1st_conviction', 'date_of_release']].head())\n\n   posting_date date_of_exoneration date_of_1st_conviction date_of_release\n1    2022-02-14          2022-02-01             2008-03-25      2008-03-25\n3    2015-02-13          2015-02-11             1987-01-15      2015-02-11\n5    2022-08-25          2022-07-21             2004-09-22      2022-07-21\n10   2020-04-13          2020-02-11             2004-09-08      2004-12-26\n15   2011-08-29          1996-07-02             1978-10-20      1996-06-14\n\n\n\n# Verify updated data types:\nprint(df.dtypes)\n\nlast_name                 string[python]\nfirst_name                string[python]\nage                              float64\nrace                      string[python]\nsex                       string[python]\nstate                     string[python]\ncounty                    string[python]\ntags                              object\nworst_crime_display       string[python]\nsentence                          object\nposting_date              datetime64[ns]\nom_tags                           object\nom                                object\ndate_of_exoneration       datetime64[ns]\ndate_of_1st_conviction    datetime64[ns]\ndate_of_release           datetime64[ns]\ndtype: object\n\n\n\n\n\nThe sentence column contains textual descriptions of sentencing outcomes, including terms like “Life without parole,” “Death,” or a specified number of years. To facilitate analysis, this column is transformed into a numerical format (sentence_in_years) by converting life sentences and probation to placeholder values and handling ranges or mixed units (e.g., years and months).\n\n# Print unique sentencing values for a better idea on how to best clean column: \nunique_sentences = df['sentence'].unique()\nprint(unique_sentences)\n\n['Probation' 'Life without parole' '90 years' '1 year' '75 years'\n '30 years' '55 years' '2 years' '3 years' '6 years' '45 years'\n '1 year and 6 months' '50 years' '60 years' 'Life' '80 years' '18 years'\n '4 years' '85 years' '20 years' '35 years' '2 years and 6 months'\n '82 years' '12 years' 'Not sentenced' '22 years' '32 years' 'Death'\n '5 years' '40 years' '25 years' '26 years' '4 years and 6 months'\n '9 years' '48 years' '30 days' '84 years' '3 months and 25 days'\n '2 years and 2 months' '3 months' '44 years' '6 months' '25 to 50 years'\n '29 years' '23 years' '31 years' '11 years' '8 years' '24 years'\n '3 years and four months' '42 years' '3 years and 6 months' '65 years'\n '76 years' '15 years' '50 to Life' '86 years' '70 years' '28 years'\n '13 years' '47 years' '36 years' '18 months' '1 year and 4 months'\n '8 years and 6 months' '6 years and 6 months' '58 years' '95 years'\n '7 years' '34 years' '62 years' '27 years' '69 years' '57 years'\n '50 to 100 years' '4 months' '4 years and 3 months' '37 years' '10 years'\n '67 years' '46 years' '17 years' '10 to 22 years' '6 years and 7 months'\n '5 years and 6 months' '2 Years']\n\n\n\n\nThe sentence column is cleaned to convert textual descriptions into numerical values: 1. Probation and Not Sentenced are set to 0. 2. Life sentences and the death penalty are represented as 100 for placeholder analysis. 3. Ranges (e.g., “25 to 50 years”) are averaged to a single value. 4. Years and months are combined into total years for uniformity.\nThis standardization facilitates meaningful comparisons and quantitative analysis of sentencing patterns.\n\ndef clean_sentence(value):\n    \"\"\" Cleans the 'sentence' column values to numeric years for numerical EDA \n    - Probation is represented as 0.\n    - 'Not sentenced' is converted to np.nan.\n    - 'Life' and 'Death' sentences are represented as 100 (placeholder).\n    - Years and months are converted to a numeric value in years. \"\"\"\n\n    if value == 'Probation':\n        return 0\n    elif value == 'Not sentenced':\n        return np.nan  # NaN for not sentenced\n    elif 'Life' in value or value == 'Death':\n        return 100  # Placeholder for life sentences or death penalty\n    elif 'year' in value or 'month' in value:\n\n        # Handles ranges like '25 to 50 years'\n        if 'to' in value:\n            years = [int(num) for num in re.findall(r'\\d+', value)]\n            return sum(years) / len(years)  # Average the range \n        \n        # Handle \"X years and Y months\"\n        elif 'and' in value:\n            numbers = [float(num) for num in re.findall(r'\\d+', value)]\n            if len(numbers) == 2:  # Both years and months are present\n                years, months = numbers\n                return years + (months / 12)  # Convert months to years\n            elif len(numbers) == 1:  # Only one number is present\n                return numbers[0]  # Treat it as years\n            \n        # Handle only months or only years\n        elif 'months' in value:\n            months = int(re.search(r'\\d+', value).group())\n            return months / 12  # Convert months to years\n        else:  # Only years\n            return int(re.search(r'\\d+', value).group())\n    else:\n        return np.nan  # Anything unexpected as None\n    \ndf['sentence_in_years'] = df['sentence'].apply(clean_sentence)\n\n# Check results\ndf[['sentence', 'sentence_in_years']].head(10)\n\n\n\n\n\n\n\n\nsentence\nsentence_in_years\n\n\n\n\n1\nProbation\n0.0\n\n\n3\nLife without parole\n100.0\n\n\n5\n90 years\n90.0\n\n\n10\n1 year\n1.0\n\n\n15\n75 years\n75.0\n\n\n21\nProbation\n0.0\n\n\n22\nProbation\n0.0\n\n\n24\n30 years\n30.0\n\n\n25\n55 years\n55.0\n\n\n45\n1 year\n1.0\n\n\n\n\n\n\n\n\n# Check updated data types\nprint(df.dtypes)\n\nlast_name                 string[python]\nfirst_name                string[python]\nage                              float64\nrace                      string[python]\nsex                       string[python]\nstate                     string[python]\ncounty                    string[python]\ntags                              object\nworst_crime_display       string[python]\nsentence                          object\nposting_date              datetime64[ns]\nom_tags                           object\nom                                object\ndate_of_exoneration       datetime64[ns]\ndate_of_1st_conviction    datetime64[ns]\ndate_of_release           datetime64[ns]\nsentence_in_years                float64\ndtype: object\n\n\nAfter transforming the sentence column into a numerical format, the new column, sentence_in_years is now represented as a float64, which aligns with the desired structure for numerical analysis. This conversion allows for quantitative exploration of the sentencing data, such as aggregations and comparisons, during later stages of analysis. The original sentence column is retained for reference purposes, as it preserves the detailed textual descriptions that might be useful for contextual insights. The tags, OM, and OM_tags columns will be addressed later, so for now the datatype may remain as an object."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#cleaning-the-tags-and-om-tags-columns",
    "href": "technical-details/data-cleaning/main.html#cleaning-the-tags-and-om-tags-columns",
    "title": "Data Cleaning",
    "section": "",
    "text": "The tags and OM-tags columns contain important categorical information about each exoneration case. To make this data more useful for analysis, both columns were transformed into multiple binary columns, where each tag indicates the presence (1) or absence (0) of a specific feature. Additionally, a tag_sum column was created to capture the total number of tags associated with each case, providing a summary metric.\nThe cleaning process involved the following steps:\n\nRemoving Unnecessary Characters:\nUnwanted characters such as # were removed, and delimiters were standardized to ensure consistency in the data.\nSplitting Tags:\nThe tags and OM-tags columns were split into individual values to facilitate binary encoding.\nRenaming Binary Columns:\nEach binary column was renamed using clear and descriptive labels by mapping the original tags to their definitions. This mapping process translated short tag codes into their full meanings, improving interpretability. For reference, the definitions of the tags are based on the descriptions provided by the National Registry of Exonerations2.\nAdding a tag_sum Column:\nA new column was created to calculate the total number of tags for each case, enabling easier analysis of case complexity.\n\nThis transformation ensures the data is well-structured and ready for exploratory analysis, providing detailed insights into the systemic patterns in exoneration cases.\n\n# Clean 'tags' column:\ndf['tags'] = df['tags'].str.replace('#', '', regex=False).str.replace(\";\", \",\")\ndf['OM-tags'] = df['om_tags'].str.replace('#', '', regex=False).str.replace(\";\", \",\")\n\n# Define the mapping for tags:\ntag_mapping = {\n    \"A\": \"arson\",\n    \"BM\": \"bitemark\",\n    \"CDC\": \"co_defendant_confessed\",\n    \"CIU\": \"conviction_integrity_unit\",\n    \"CSH\": \"child_sex_abuse_hysteria_case\",\n    \"CV\": \"child_victim\",\n    \"F\": \"female_exoneree\",\n    \"FED\": \"federal_case\",\n    \"H\": \"homicide\",\n    \"IO\": \"innocence_organization\",\n    \"JI\": \"jailhouse_informant\",\n    \"JV\": \"juvenile_defendant\",\n    \"M\": \"misdemeanor\",\n    \"NC\": \"no_crime_case\",\n    \"P\": \"guilty_plea_case\",\n    \"PH\": \"posthumous_exoneration\",\n    \"SA\": \"sexual_assault\",\n    \"SBS\": \"shaken_baby_syndrome_case\",\n    \"PR\": \"prosecutor_misconduct\",\n    \"OF\": \"police_officer_misconduct\",\n    \"FA\": \"forensic_analyst_misconduct\",\n    \"CW\": \"child_welfare_worker_misconduct\",\n    \"WH\": \"withheld_exculpatory_evidence\",\n    \"NW\": \"misconduct_that_is_not_withholding_evidence\",\n    \"KP\": \"knowingly_permitting_perjury\",\n    \"WT\": \"witness_tampering_or_misconduct_interrogating_co_defendant\",\n    \"INT\": \"misconduct_in_interrogation_of_exoneree\",\n    \"PJ\": \"perjury_by_official\",\n    \"PL\": \"prosecutor_lied_in_court\"\n}\n\n# Split 'tags' and 'OM-tags' into lists:\ndf['tags'] = df['tags'].apply(lambda x: x.split(',') if isinstance(x, str) else x)\ndf['OM-tags'] = df['OM-tags'].apply(lambda x: x.split(',') if isinstance(x, str) else x)\n\n# Create binary columns for tags from both 'tags' and 'OM-tags':\nfor tag in tag_mapping.keys():\n    # Check if the tag exists in 'tags' or 'OM-tags':\n    df[tag] = df.apply(\n        lambda row: 1 if (isinstance(row['tags'], list) and tag in row['tags']) or \n                          (isinstance(row['OM-tags'], list) and tag in row['OM-tags']) else 0,\n        axis=1\n    )\n\n# Rename the binary columns using the tag_mapping dictionary:\ndf.rename(columns=tag_mapping, inplace=True)\n\n# Create `tag_sum` column to count the total number of tags for each exoneree:\ndf['tag_sum'] = df[list(tag_mapping.values())].sum(axis=1)\n\n# Drop the original 'tags' and 'OM-tags' columns:\ndf.drop(columns=['tags', 'om_tags', 'OM-tags'], inplace=True)\n\ndf.head()  \n\n\n\n\n\n\n\n\nlast_name\nfirst_name\nage\nrace\nsex\nstate\ncounty\nworst_crime_display\nsentence\nposting_date\nom\ndate_of_exoneration\ndate_of_1st_conviction\ndate_of_release\nsentence_in_years\narson\nbitemark\nco_defendant_confessed\nconviction_integrity_unit\nchild_sex_abuse_hysteria_case\nchild_victim\nfemale_exoneree\nfederal_case\nhomicide\ninnocence_organization\njailhouse_informant\njuvenile_defendant\nmisdemeanor\nno_crime_case\nguilty_plea_case\nposthumous_exoneration\nsexual_assault\nshaken_baby_syndrome_case\nprosecutor_misconduct\npolice_officer_misconduct\nforensic_analyst_misconduct\nchild_welfare_worker_misconduct\nwithheld_exculpatory_evidence\nmisconduct_that_is_not_withholding_evidence\nknowingly_permitting_perjury\nwitness_tampering_or_misconduct_interrogating_co_defendant\nmisconduct_in_interrogation_of_exoneree\nperjury_by_official\nprosecutor_lied_in_court\ntag_sum\n\n\n\n\n1\nAbbott\nCinque\n19.0\nBlack\nmale\nIllinois\nCook\nDrug Possession or Sale\nProbation\n2022-02-14\nOM\n2022-02-01\n2008-03-25\n2008-03-25\n0.0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n7\n\n\n3\nAbernathy\nChristopher\n17.0\nWhite\nmale\nIllinois\nCook\nMurder\nLife without parole\n2015-02-13\nOM\n2015-02-11\n1987-01-15\n2015-02-11\n100.0\n0\n0\n0\n1\n0\n1\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n1\n0\n0\n1\n0\n0\n10\n\n\n5\nAbrego\nEruby\n20.0\nHispanic\nmale\nIllinois\nCook\nMurder\n90 years\n2022-08-25\nOM\n2022-07-21\n2004-09-22\n2022-07-21\n90.0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n1\n1\n1\n0\n9\n\n\n10\nAdams\nDemetris\n22.0\nBlack\nmale\nIllinois\nCook\nDrug Possession or Sale\n1 year\n2020-04-13\nOM\n2020-02-11\n2004-09-08\n2004-12-26\n1.0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n7\n\n\n15\nAdams\nKenneth\n22.0\nBlack\nmale\nIllinois\nCook\nMurder\n75 years\n2011-08-29\nOM\n1996-07-02\n1978-10-20\n1996-06-14\n75.0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n1\n0\n1\n1\n0\n0\n1\n1\n1\n1\n0\n0\n0\n11\n\n\n\n\n\n\n\n\n# Convert 'OM' column to binary (1 if \"OM\" is present, 0 otherwise)\ndf['om'] = df['om'].apply(lambda x: 1 if str(x).strip().upper() == \"OM\" else 0)\n\n# Verify the transformation\nprint(df['om'].value_counts())\n\nom\n1    478\n0     70\nName: count, dtype: int64"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#merge-with-geocoded-counties",
    "href": "technical-details/data-cleaning/main.html#merge-with-geocoded-counties",
    "title": "Data Cleaning",
    "section": "",
    "text": "The geocoded Illinois counties from Data Collection were merged into the main dataset:\n\nLoad and Standardize Data: Geocoded data was loaded, and column names were standardized to lowercase for consistency.\nFilter Relevant Counties: The geocoded data was filtered to include only counties present in the main dataset.\nMerge Data: Using a left join on county and state, geographic details (geocode_address, latitude, and longitude) were added to the dataset.\n\n\n# Read the geocoded population data from Data Collection\ngeocode_unique = pd.read_csv(\"../../data/raw-data/geocoded_population_counties.csv\")\n\n# Rename columns to lowercase for consistency\ngeocode_unique.rename(columns={\"County\": \"county\", \"State\": \"state\"}, inplace=True)\n\n# Filter geocode_unique to only include counties present in df\ngeocode_unique_filtered = geocode_unique[\n    geocode_unique[['county', 'state']].apply(tuple, axis=1).isin(df[['county', 'state']].apply(tuple, axis=1))\n]\n\n# Merge the filtered geocoding data into df\ndf = df.merge(geocode_unique_filtered, on=['county', 'state'], how='left')\n\n# Display the relevant columns to verify the merge\nprint(df[['state', 'county', 'geocode_address', 'latitude', 'longitude']].head())\n\n      state county                       geocode_address   latitude  longitude\n0  Illinois   Cook  Cook County, Illinois, United States  41.819738 -87.756525\n1  Illinois   Cook  Cook County, Illinois, United States  41.819738 -87.756525\n2  Illinois   Cook  Cook County, Illinois, United States  41.819738 -87.756525\n3  Illinois   Cook  Cook County, Illinois, United States  41.819738 -87.756525\n4  Illinois   Cook  Cook County, Illinois, United States  41.819738 -87.756525"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#calculating-years-lost",
    "href": "technical-details/data-cleaning/main.html#calculating-years-lost",
    "title": "Data Cleaning",
    "section": "",
    "text": "To quantify the years_lost due to wrongful conviction, this step calculates the difference in years between an individual’s date_of_1st_conviction and their date_of_release.\n\n# Calculate \"years lost\" as the difference in years between release and conviction:\ndf['years_lost'] = (df['date_of_release'] - df['date_of_1st_conviction']).dt.days / 365.25 # Dividing by 365.25 accounts for leap years\n\n# Round the years lost to 2 decimal places:\ndf['years_lost'] = df['years_lost'].round(2)\n\n# Updated DataFrame:\nprint(df[['date_of_1st_conviction', 'date_of_release', 'years_lost']])\n\n    date_of_1st_conviction date_of_release  years_lost\n0               2008-03-25      2008-03-25        0.00\n1               1987-01-15      2015-02-11       28.07\n2               2004-09-22      2022-07-21       17.83\n3               2004-09-08      2004-12-26        0.30\n4               1978-10-20      1996-06-14       17.65\n..                     ...             ...         ...\n543             2005-04-13      2005-04-13        0.00\n544             2005-01-11      2006-07-12        1.50\n545             2005-04-11      2006-12-07        1.66\n546             2003-04-21      2005-03-10        1.89\n547             1994-09-20      2005-01-31       10.37\n\n[548 rows x 3 columns]"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#column-reorganization",
    "href": "technical-details/data-cleaning/main.html#column-reorganization",
    "title": "Data Cleaning",
    "section": "",
    "text": "To improve readability and logical flow, the following changes were made to the column order:\n\nAlign Sentencing Data:\nThe sentence_in_years column was moved to appear immediately after sentence, ensuring that the cleaned numerical representation of sentencing data is logically aligned with its original textual description.\nReorganize Release and Years Lost:\nThe years_lost column was moved to appear immediately after date_of_release, facilitating easier comparison of release dates and the calculated time lost due to wrongful incarceration.\nGroup Geographic Data:\nThe latitude and longitude columns were moved to follow the county column, grouping geographic information together.\n\n\n# Reordering columns:\ncolumns = list(df.columns)  \n\n#Aligning sentencing data:  \ncolumns.insert(columns.index('sentence') + 1, columns.pop(columns.index('sentence_in_years')))  # Move 'sentence_in_years'\n\n#Reorganizing release and years lost\ncolumns.insert(columns.index('date_of_release') +1, columns.pop(columns.index('years_lost'))) #Move 'years_lost' \n\n# Move 'latitude' and 'longitude' after 'county'\ncolumns.insert(columns.index('county') + 1, columns.pop(columns.index('latitude')))\ncolumns.insert(columns.index('county') + 2, columns.pop(columns.index('longitude')))\n\ndf = df[columns]  # Reorder DataFrame\n\ndf.head(10)\n\n\n\n\n\n\n\n\nlast_name\nfirst_name\nage\nrace\nsex\nstate\ncounty\nlatitude\nlongitude\nworst_crime_display\nsentence\nsentence_in_years\nposting_date\nom\ndate_of_exoneration\ndate_of_1st_conviction\ndate_of_release\nyears_lost\narson\nbitemark\nco_defendant_confessed\nconviction_integrity_unit\nchild_sex_abuse_hysteria_case\nchild_victim\nfemale_exoneree\nfederal_case\nhomicide\ninnocence_organization\njailhouse_informant\njuvenile_defendant\nmisdemeanor\nno_crime_case\nguilty_plea_case\nposthumous_exoneration\nsexual_assault\nshaken_baby_syndrome_case\nprosecutor_misconduct\npolice_officer_misconduct\nforensic_analyst_misconduct\nchild_welfare_worker_misconduct\nwithheld_exculpatory_evidence\nmisconduct_that_is_not_withholding_evidence\nknowingly_permitting_perjury\nwitness_tampering_or_misconduct_interrogating_co_defendant\nmisconduct_in_interrogation_of_exoneree\nperjury_by_official\nprosecutor_lied_in_court\ntag_sum\ngeocode_address\n\n\n\n\n0\nAbbott\nCinque\n19.0\nBlack\nmale\nIllinois\nCook\n41.819738\n-87.756525\nDrug Possession or Sale\nProbation\n0.0\n2022-02-14\n1\n2022-02-01\n2008-03-25\n2008-03-25\n0.00\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n7\nCook County, Illinois, United States\n\n\n1\nAbernathy\nChristopher\n17.0\nWhite\nmale\nIllinois\nCook\n41.819738\n-87.756525\nMurder\nLife without parole\n100.0\n2015-02-13\n1\n2015-02-11\n1987-01-15\n2015-02-11\n28.07\n0\n0\n0\n1\n0\n1\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n1\n0\n0\n1\n0\n0\n10\nCook County, Illinois, United States\n\n\n2\nAbrego\nEruby\n20.0\nHispanic\nmale\nIllinois\nCook\n41.819738\n-87.756525\nMurder\n90 years\n90.0\n2022-08-25\n1\n2022-07-21\n2004-09-22\n2022-07-21\n17.83\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n1\n1\n1\n0\n9\nCook County, Illinois, United States\n\n\n3\nAdams\nDemetris\n22.0\nBlack\nmale\nIllinois\nCook\n41.819738\n-87.756525\nDrug Possession or Sale\n1 year\n1.0\n2020-04-13\n1\n2020-02-11\n2004-09-08\n2004-12-26\n0.30\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n7\nCook County, Illinois, United States\n\n\n4\nAdams\nKenneth\n22.0\nBlack\nmale\nIllinois\nCook\n41.819738\n-87.756525\nMurder\n75 years\n75.0\n2011-08-29\n1\n1996-07-02\n1978-10-20\n1996-06-14\n17.65\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n1\n0\n1\n1\n0\n0\n1\n1\n1\n1\n0\n0\n0\n11\nCook County, Illinois, United States\n\n\n5\nAdams\nSeneca\n20.0\nBlack\nmale\nIllinois\nCook\n41.819738\n-87.756525\nAssault\nProbation\n0.0\n2014-12-08\n1\n2006-12-19\n2006-05-18\n2006-05-18\n0.00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n1\n0\n6\nCook County, Illinois, United States\n\n\n6\nAdams\nTari\n18.0\nBlack\nmale\nIllinois\nCook\n41.819738\n-87.756525\nAssault\nProbation\n0.0\n2014-12-08\n1\n2006-12-19\n2006-05-18\n2006-05-18\n0.00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n1\n0\n6\nCook County, Illinois, United States\n\n\n7\nAgnew\nGregory\n28.0\nBlack\nmale\nIllinois\nLake\n42.332738\n-87.993955\nRobbery\n30 years\n30.0\n2018-01-18\n0\n2001-11-07\n1988-06-14\n2001-11-07\n13.40\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\nLake County, Illinois, United States\n\n\n8\nAguirre\nOmar\n28.0\nHispanic\nmale\nIllinois\nCook\n41.819738\n-87.756525\nMurder\n55 years\n55.0\n2011-08-29\n1\n2003-02-18\n1999-03-09\n2002-12-18\n3.78\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n1\n1\n0\n0\n7\nCook County, Illinois, United States\n\n\n9\nAli\nChauncey\n37.0\nBlack\nmale\nIllinois\nCook\n41.819738\n-87.756525\nDrug Possession or Sale\n1 year\n1.0\n2020-04-13\n1\n2020-02-11\n2007-01-17\n2007-06-27\n0.44\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n7\nCook County, Illinois, United States"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#exporting-the-cleaned-dataset",
    "href": "technical-details/data-cleaning/main.html#exporting-the-cleaned-dataset",
    "title": "Data Cleaning",
    "section": "",
    "text": "The final cleaned dataset is saved as illinois_exoneration_data.csv, ensuring that all preprocessing steps are reproducible, and the dataset can be used consistently across various analysis stages.\n\ndf.to_csv('../../data/processed-data/illinois_exoneration_data.csv', index=False)\nprint(\"Data saved to 'illinois_exoneration_data.csv'\")\n\nData saved to 'illinois_exoneration_data.csv'"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#introduction-and-motivation-1",
    "href": "technical-details/data-cleaning/main.html#introduction-and-motivation-1",
    "title": "Data Cleaning",
    "section": "Introduction and Motivation",
    "text": "Introduction and Motivation\nThis section documents the steps taken to clean and preprocess the Illinois arrest dataset. Similarly to the U.S. exoneration dataset, the goal is to transform the raw data into a structured and reliable format for analysis. However, this dataset required fewer cleaning steps due to its uniform structure and consistent formatting.\nBy the end of this phase, the Illinois arrest dataset will be well-prepared for exploratory data analysis (EDA) and integration into broader investigative workflows."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#initial-overview-of-the-dataset-1",
    "href": "technical-details/data-cleaning/main.html#initial-overview-of-the-dataset-1",
    "title": "Data Cleaning",
    "section": "Initial Overview of the Dataset",
    "text": "Initial Overview of the Dataset\nThe dataset is first loaded to inspect its structure and contents.\n\n# Load exoneration dataset:\narrest_data = pd.read_csv('../../data/raw-data/illinois_arrest_explorer_data.csv')\nprint(\"Initial Dataset: \")\npd.set_option('display.max_columns', None) # Enables display of every column\narrest_data.head()\n\nInitial Dataset: \n\n\n\n\n\n\n\n\n\nYear\nrace\ncounty_Adams\ncounty_Alexander\ncounty_Bond\ncounty_Boone\ncounty_Brown\ncounty_Bureau\ncounty_Calhoun\ncounty_Carroll\ncounty_Cass\ncounty_Champaign\ncounty_Christian\ncounty_Clark\ncounty_Clay\ncounty_Clinton\ncounty_Coles\ncounty_Cook Chicago\ncounty_Cook County Suburbs\ncounty_Crawford\ncounty_Cumberland\ncounty_Dekalb\ncounty_Dewitt\ncounty_Douglas\ncounty_Dupage\ncounty_Edgar\ncounty_Edwards\ncounty_Effingham\ncounty_Fayette\ncounty_Ford\ncounty_Franklin\ncounty_Fulton\ncounty_Gallatin\ncounty_Greene\ncounty_Grundy\ncounty_Hamilton\ncounty_Hancock\ncounty_Hardin\ncounty_Henderson\ncounty_Henry\ncounty_Iroquois\ncounty_Jackson\ncounty_Jasper\ncounty_Jefferson\ncounty_Jersey\ncounty_Jo Daviess\ncounty_Johnson\ncounty_Kane\ncounty_Kankakee\ncounty_Kendall\ncounty_Knox\ncounty_Lake\ncounty_Lasalle\ncounty_Lawrence\ncounty_Lee\ncounty_Livingston\ncounty_Logan\ncounty_Macon\ncounty_Macoupin\ncounty_Madison\ncounty_Marion\ncounty_Marshall\ncounty_Mason\ncounty_Massac\ncounty_Mcdonough\ncounty_Mchenry\ncounty_Mclean\ncounty_Menard\ncounty_Mercer\ncounty_Monroe\ncounty_Montgomery\ncounty_Morgan\ncounty_Moultrie\ncounty_Non County Agencies\ncounty_Ogle\ncounty_Peoria\ncounty_Perry\ncounty_Piatt\ncounty_Pike\ncounty_Pope\ncounty_Pulaski\ncounty_Putnam\ncounty_Randolph\ncounty_Richland\ncounty_Rock Island\ncounty_Saline\ncounty_Sangamon\ncounty_Schuyler\ncounty_Scott\ncounty_Shelby\ncounty_St. Clair\ncounty_Stark\ncounty_Stephenson\ncounty_Tazewell\ncounty_Union\ncounty_Vermilion\ncounty_Wabash\ncounty_Warren\ncounty_Washington\ncounty_Wayne\ncounty_White\ncounty_Whiteside\ncounty_Will\ncounty_Williamson\ncounty_Winnebago\ncounty_Woodford\n\n\n\n\n0\n2001\nAfrican American\n226\n147\n25\n18\n18\n48\n6\n12\n1\n2059\n16\n6\n1\n22\n130\n86781\n26301\n6\n1\n342\n11\n18\n2265\n1\n1\n108\n6\n1\n6\n25\n1\n1\n52\n1\n1\n1\n6\n92\n250\n580\n1\n246\n6\n16\n16\n2804\n1669\n122\n314\n5712\n184\n6\n86\n180\n71\n1104\n34\n1567\n146\n6\n1\n91\n105\n213\n905\n1\n6\n20\n38\n287\n6\n1\n57\n3403\n60\n1\n13\n1\n153\n1\n81\n6\n1215\n104\n2200\n6\n1\n1\n2462\n1\n355\n168\n17\n876\n6\n46\n25\n6\n16\n128\n3000\n75\n2509\n42\n\n\n1\n2001\nAsian\n1\n1\n1\n1\n1\n1\n1\n1\n1\n49\n1\n1\n1\n1\n1\n722\n460\n1\n1\n15\n1\n1\n139\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n97\n1\n1\n1\n135\n1\n1\n1\n1\n1\n1\n1\n12\n1\n1\n1\n1\n6\n6\n16\n1\n1\n1\n1\n1\n1\n1\n1\n12\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n17\n1\n1\n1\n6\n1\n1\n6\n1\n1\n1\n1\n1\n1\n1\n1\n14\n1\n28\n1\n\n\n2\n2001\nHispanic\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n3\n2001\nNative American\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n146\n30\n1\n1\n1\n1\n1\n17\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n16\n1\n1\n1\n10\n1\n1\n1\n1\n1\n1\n1\n6\n1\n1\n1\n1\n1\n1\n6\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n6\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n6\n1\n\n\n4\n2001\nWhite\n939\n52\n209\n674\n107\n509\n139\n298\n88\n2334\n335\n264\n117\n229\n1635\n37099\n36690\n470\n164\n1740\n539\n181\n13019\n301\n41\n986\n382\n217\n724\n926\n19\n57\n1014\n78\n206\n75\n134\n569\n585\n857\n87\n607\n575\n532\n198\n9634\n1607\n1465\n1160\n15136\n2689\n412\n845\n1146\n586\n1388\n523\n4359\n571\n263\n274\n455\n1007\n3879\n2285\n92\n410\n572\n749\n1053\n149\n1\n1009\n3120\n430\n211\n368\n45\n248\n151\n460\n323\n3117\n711\n3635\n274\n22\n486\n1601\n59\n644\n1804\n327\n1743\n324\n444\n265\n245\n668\n1304\n4144\n631\n4253\n462"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#column-renaming-and-consolidation",
    "href": "technical-details/data-cleaning/main.html#column-renaming-and-consolidation",
    "title": "Data Cleaning",
    "section": "Column Renaming and Consolidation",
    "text": "Column Renaming and Consolidation\nTo streamline the Illinois arrest dataset and improve clarity, the following transformations were applied:\n\nRemoving Prefixes for Simplicity:\nColumns with the county_ prefix were renamed by removing the prefix and capitalizing the remaining column names. This makes the column headers cleaner and more intuitive for analysis.\nStandardizing Race Terminology:\nThe term “African American” in the race column was replaced with “Black” to ensure consistency with the exoneration dataset.\nConsolidating Cook County Data:\nThe columns Cook Chicago and Cook County Suburbs were combined into a single Cook column. This consolidation simplifies the data and groups all arrest information related to Cook County into a unified metric.\n\n\n# Rename columns by removing 'county_' prefix:\narrest_data.columns = [col.replace('county_', '').capitalize() if col.startswith('county_') else col for col in arrest_data.columns]\n\n# Rename \"African American\" to \"Black\" in the column names:\narrest_data['race'] = arrest_data['race'].replace('African American', 'Black')\n\n# Combine \"Cook Chicago\" and \"Cook County Suburbs\" into a single \"Cook\" column:\narrest_data['Cook'] = arrest_data['Cook chicago'] + arrest_data['Cook county suburbs']\n\n# Drop the old columns:\narrest_data.drop(columns=['Cook chicago', 'Cook county suburbs'], inplace=True)\n\narrest_data.head(1)\n\n\n\n\n\n\n\n\nYear\nrace\nAdams\nAlexander\nBond\nBoone\nBrown\nBureau\nCalhoun\nCarroll\nCass\nChampaign\nChristian\nClark\nClay\nClinton\nColes\nCrawford\nCumberland\nDekalb\nDewitt\nDouglas\nDupage\nEdgar\nEdwards\nEffingham\nFayette\nFord\nFranklin\nFulton\nGallatin\nGreene\nGrundy\nHamilton\nHancock\nHardin\nHenderson\nHenry\nIroquois\nJackson\nJasper\nJefferson\nJersey\nJo daviess\nJohnson\nKane\nKankakee\nKendall\nKnox\nLake\nLasalle\nLawrence\nLee\nLivingston\nLogan\nMacon\nMacoupin\nMadison\nMarion\nMarshall\nMason\nMassac\nMcdonough\nMchenry\nMclean\nMenard\nMercer\nMonroe\nMontgomery\nMorgan\nMoultrie\nNon county agencies\nOgle\nPeoria\nPerry\nPiatt\nPike\nPope\nPulaski\nPutnam\nRandolph\nRichland\nRock island\nSaline\nSangamon\nSchuyler\nScott\nShelby\nSt. clair\nStark\nStephenson\nTazewell\nUnion\nVermilion\nWabash\nWarren\nWashington\nWayne\nWhite\nWhiteside\nWill\nWilliamson\nWinnebago\nWoodford\nCook\n\n\n\n\n0\n2001\nBlack\n226\n147\n25\n18\n18\n48\n6\n12\n1\n2059\n16\n6\n1\n22\n130\n6\n1\n342\n11\n18\n2265\n1\n1\n108\n6\n1\n6\n25\n1\n1\n52\n1\n1\n1\n6\n92\n250\n580\n1\n246\n6\n16\n16\n2804\n1669\n122\n314\n5712\n184\n6\n86\n180\n71\n1104\n34\n1567\n146\n6\n1\n91\n105\n213\n905\n1\n6\n20\n38\n287\n6\n1\n57\n3403\n60\n1\n13\n1\n153\n1\n81\n6\n1215\n104\n2200\n6\n1\n1\n2462\n1\n355\n168\n17\n876\n6\n46\n25\n6\n16\n128\n3000\n75\n2509\n42\n113082"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#handling-placeholder-values",
    "href": "technical-details/data-cleaning/main.html#handling-placeholder-values",
    "title": "Data Cleaning",
    "section": "Handling Placeholder Values",
    "text": "Handling Placeholder Values\nDuring preprocessing, it was observed that the dataset contains 1s in certain fields. To address this all 1s in the dataset were replaced with 0s to ensure the integrity of the analysis. This decision ensures that the data is not skewed by suspicious or placeholder values, which could misrepresent trends or introduce bias into the results.\n\nNote: The reasoning behind the presence of these placeholder values is discussed further in the Data Collection section.\n\n\n# Replace 1s with 0s: \narrest_data.replace(1, 0, inplace=True)\n\n# Display the updated dataset to confirm changes:\narrest_data.head()\n\narrest_data.head()\n\n\n\n\n\n\n\n\nYear\nrace\nAdams\nAlexander\nBond\nBoone\nBrown\nBureau\nCalhoun\nCarroll\nCass\nChampaign\nChristian\nClark\nClay\nClinton\nColes\nCrawford\nCumberland\nDekalb\nDewitt\nDouglas\nDupage\nEdgar\nEdwards\nEffingham\nFayette\nFord\nFranklin\nFulton\nGallatin\nGreene\nGrundy\nHamilton\nHancock\nHardin\nHenderson\nHenry\nIroquois\nJackson\nJasper\nJefferson\nJersey\nJo daviess\nJohnson\nKane\nKankakee\nKendall\nKnox\nLake\nLasalle\nLawrence\nLee\nLivingston\nLogan\nMacon\nMacoupin\nMadison\nMarion\nMarshall\nMason\nMassac\nMcdonough\nMchenry\nMclean\nMenard\nMercer\nMonroe\nMontgomery\nMorgan\nMoultrie\nNon county agencies\nOgle\nPeoria\nPerry\nPiatt\nPike\nPope\nPulaski\nPutnam\nRandolph\nRichland\nRock island\nSaline\nSangamon\nSchuyler\nScott\nShelby\nSt. clair\nStark\nStephenson\nTazewell\nUnion\nVermilion\nWabash\nWarren\nWashington\nWayne\nWhite\nWhiteside\nWill\nWilliamson\nWinnebago\nWoodford\nCook\n\n\n\n\n0\n2001\nBlack\n226\n147\n25\n18\n18\n48\n6\n12\n0\n2059\n16\n6\n0\n22\n130\n6\n0\n342\n11\n18\n2265\n0\n0\n108\n6\n0\n6\n25\n0\n0\n52\n0\n0\n0\n6\n92\n250\n580\n0\n246\n6\n16\n16\n2804\n1669\n122\n314\n5712\n184\n6\n86\n180\n71\n1104\n34\n1567\n146\n6\n0\n91\n105\n213\n905\n0\n6\n20\n38\n287\n6\n0\n57\n3403\n60\n0\n13\n0\n153\n0\n81\n6\n1215\n104\n2200\n6\n0\n0\n2462\n0\n355\n168\n17\n876\n6\n46\n25\n6\n16\n128\n3000\n75\n2509\n42\n113082\n\n\n1\n2001\nAsian\n0\n0\n0\n0\n0\n0\n0\n0\n0\n49\n0\n0\n0\n0\n0\n0\n0\n15\n0\n0\n139\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n97\n0\n0\n0\n135\n0\n0\n0\n0\n0\n0\n0\n12\n0\n0\n0\n0\n6\n6\n16\n0\n0\n0\n0\n0\n0\n0\n0\n12\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n17\n0\n0\n0\n6\n0\n0\n6\n0\n0\n0\n0\n0\n0\n0\n0\n14\n0\n28\n0\n1182\n\n\n2\n2001\nHispanic\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n\n\n3\n2001\nNative American\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n17\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n16\n0\n0\n0\n10\n0\n0\n0\n0\n0\n0\n0\n6\n0\n0\n0\n0\n0\n0\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n6\n0\n176\n\n\n4\n2001\nWhite\n939\n52\n209\n674\n107\n509\n139\n298\n88\n2334\n335\n264\n117\n229\n1635\n470\n164\n1740\n539\n181\n13019\n301\n41\n986\n382\n217\n724\n926\n19\n57\n1014\n78\n206\n75\n134\n569\n585\n857\n87\n607\n575\n532\n198\n9634\n1607\n1465\n1160\n15136\n2689\n412\n845\n1146\n586\n1388\n523\n4359\n571\n263\n274\n455\n1007\n3879\n2285\n92\n410\n572\n749\n1053\n149\n0\n1009\n3120\n430\n211\n368\n45\n248\n151\n460\n323\n3117\n711\n3635\n274\n22\n486\n1601\n59\n644\n1804\n327\n1743\n324\n444\n265\n245\n668\n1304\n4144\n631\n4253\n462\n73789"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#exporting-cleaned-and-aggregated-datasets",
    "href": "technical-details/data-cleaning/main.html#exporting-cleaned-and-aggregated-datasets",
    "title": "Data Cleaning",
    "section": "Exporting Cleaned and Aggregated Datasets",
    "text": "Exporting Cleaned and Aggregated Datasets\n\n1. Exporting the Cleaned Dataset\nAfter cleaning the Illinois arrest dataset, the cleaned version was saved as a CSV file. This dataset retains the original structure, including arrest counts broken down by race, county, and year. Maintaining the year-specific information allows for detailed time-series analysis and year-over-year comparisons.\n\narrest_data.to_csv('../../data/processed-data/arrest_data_by_year.csv', index=False)\nprint(\"Data saved to 'arrest_data_by_year.csv'\")\n\nData saved to 'arrest_data_by_year.csv'\n\n\n\n\n2. Aggregating Arrest Data by Race and County\nThe arrest data was aggregated to simplify the analysis by summing totals across all years for each race and county. This step removes the Year column and groups arrests solely by race and county.\nSteps:\n- Grouped the dataset by the race column.\n- Summed arrest counts for each county across all years.\n- Reset the index for a clean, tabular structure.\n\n# Aggregate totals across years for each race:\naggregated_data = arrest_data.groupby('race').sum(numeric_only=True)\naggregated_data = aggregated_data.drop(columns=['Year'])\naggregated_data = aggregated_data.reset_index()\n\n# Preview the aggregated data:\naggregated_data.head()\n\n\n\n\n\n\n\n\nrace\nAdams\nAlexander\nBond\nBoone\nBrown\nBureau\nCalhoun\nCarroll\nCass\nChampaign\nChristian\nClark\nClay\nClinton\nColes\nCrawford\nCumberland\nDekalb\nDewitt\nDouglas\nDupage\nEdgar\nEdwards\nEffingham\nFayette\nFord\nFranklin\nFulton\nGallatin\nGreene\nGrundy\nHamilton\nHancock\nHardin\nHenderson\nHenry\nIroquois\nJackson\nJasper\nJefferson\nJersey\nJo daviess\nJohnson\nKane\nKankakee\nKendall\nKnox\nLake\nLasalle\nLawrence\nLee\nLivingston\nLogan\nMacon\nMacoupin\nMadison\nMarion\nMarshall\nMason\nMassac\nMcdonough\nMchenry\nMclean\nMenard\nMercer\nMonroe\nMontgomery\nMorgan\nMoultrie\nNon county agencies\nOgle\nPeoria\nPerry\nPiatt\nPike\nPope\nPulaski\nPutnam\nRandolph\nRichland\nRock island\nSaline\nSangamon\nSchuyler\nScott\nShelby\nSt. clair\nStark\nStephenson\nTazewell\nUnion\nVermilion\nWabash\nWarren\nWashington\nWayne\nWhite\nWhiteside\nWill\nWilliamson\nWinnebago\nWoodford\nCook\n\n\n\n\n0\nAsian\n12\n0\n0\n22\n0\n0\n0\n0\n0\n1186\n0\n0\n0\n6\n0\n0\n0\n301\n0\n0\n4509\n6\n0\n51\n6\n0\n12\n0\n0\n0\n12\n0\n0\n0\n0\n12\n0\n166\n0\n12\n0\n0\n0\n2239\n30\n66\n0\n2562\n164\n0\n0\n81\n18\n76\n0\n550\n0\n0\n0\n45\n66\n498\n564\n0\n0\n0\n18\n12\n0\n0\n6\n381\n6\n0\n0\n0\n12\n0\n0\n0\n250\n0\n500\n0\n0\n0\n382\n0\n12\n126\n0\n0\n0\n42\n0\n0\n0\n30\n1443\n36\n767\n46\n26764\n\n\n1\nBlack\n4022\n2722\n533\n2410\n92\n780\n18\n292\n235\n45547\n459\n187\n75\n877\n4029\n280\n117\n16146\n594\n499\n59262\n119\n6\n3293\n588\n275\n520\n627\n6\n110\n2249\n0\n122\n0\n207\n2131\n3283\n13120\n45\n5960\n526\n943\n281\n59806\n27437\n4308\n7545\n78841\n7344\n337\n1654\n4521\n2353\n27094\n818\n51485\n4542\n123\n218\n1731\n4214\n6581\n27867\n189\n268\n885\n1275\n7157\n165\n693\n1685\n79960\n1901\n358\n519\n18\n3384\n42\n1731\n213\n26048\n1795\n63703\n81\n0\n110\n60744\n24\n11866\n8363\n399\n17641\n153\n1024\n489\n91\n464\n2893\n58968\n4809\n55837\n2841\n1772659\n\n\n2\nHispanic\n6\n0\n6\n516\n0\n114\n0\n34\n68\n983\n0\n6\n0\n91\n120\n18\n18\n1783\n84\n147\n13828\n0\n0\n51\n35\n67\n22\n0\n0\n30\n470\n0\n6\n0\n11\n18\n104\n186\n0\n160\n24\n101\n0\n14151\n634\n264\n323\n10219\n762\n0\n136\n417\n49\n113\n0\n333\n25\n12\n10\n0\n138\n4126\n1439\n0\n0\n16\n12\n124\n0\n0\n350\n0\n43\n0\n18\n0\n104\n0\n79\n18\n1135\n0\n29\n12\n0\n0\n449\n0\n39\n192\n18\n0\n0\n142\n12\n0\n6\n436\n3822\n217\n3522\n63\n144574\n\n\n3\nNative American\n0\n0\n0\n0\n0\n0\n0\n0\n0\n297\n0\n0\n0\n0\n0\n0\n0\n30\n0\n0\n703\n0\n0\n0\n0\n0\n0\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n149\n0\n12\n0\n220\n6\n0\n0\n0\n0\n6\n0\n65\n0\n0\n0\n0\n0\n18\n24\n0\n0\n0\n0\n0\n0\n0\n0\n42\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n29\n0\n0\n6\n42\n0\n6\n6\n0\n0\n0\n0\n0\n0\n0\n0\n108\n0\n147\n0\n2351\n\n\n4\nWhite\n16743\n1340\n3964\n16580\n1812\n9593\n1844\n5530\n2826\n41996\n10983\n6714\n4818\n8858\n23779\n8663\n4228\n35622\n7121\n5676\n215397\n7031\n1019\n23978\n9115\n4630\n22754\n14545\n838\n5147\n18813\n1218\n5203\n1766\n2653\n11472\n11041\n16698\n3124\n14526\n14337\n7841\n3325\n170618\n31714\n27222\n20553\n188224\n54179\n8499\n13856\n20172\n14366\n30283\n12071\n130786\n19686\n4240\n6970\n8491\n18618\n101548\n48554\n3974\n7257\n10734\n16631\n23129\n2920\n751\n19240\n64589\n12123\n4323\n7890\n795\n4457\n1683\n12061\n6823\n53728\n12729\n82714\n2136\n585\n8671\n49390\n1442\n13615\n61546\n6165\n30772\n6491\n7459\n5220\n5146\n11318\n22690\n97863\n26871\n85292\n12676\n1174775\n\n\n\n\n\n\n\n\naggregated_data.to_csv('../../data/processed-data/aggregated_arrests_2001_to_2021.csv', index=False)\nprint(\"Data saved to 'aggregated_arrests_2001_to_2021.csv'\")\n\nData saved to 'aggregated_arrests_2001_to_2021.csv'"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#introduction-and-motivation-2",
    "href": "technical-details/data-cleaning/main.html#introduction-and-motivation-2",
    "title": "Data Cleaning",
    "section": "Introduction and Motivation",
    "text": "Introduction and Motivation\nThis section documents the minimal cleaning performed on the racial geography dataset which provides population and incarceration statistics by race, making it an essential input for balancing counterfactual data for supervised learning.\nUnlike other datasets, this required minimal preprocessing due to its clean structure and consistent formatting. The primary cleaning steps involved standardizing column names for uniformity.\nBy the end of this step, the dataset is ready for use in data balancing and then machine learning."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#initial-overview-of-the-dataset-2",
    "href": "technical-details/data-cleaning/main.html#initial-overview-of-the-dataset-2",
    "title": "Data Cleaning",
    "section": "Initial Overview of the Dataset",
    "text": "Initial Overview of the Dataset\nThe dataset is first loaded to inspect its structure and contents.\n\nrepresentation_df = pd.read_csv('../../data/raw-data/representation_by_county_raw.csv')\nprint(\"Initial Dataset: \")\npd.set_option('display.max_columns', None) \nrepresentation_df.head()\n\nInitial Dataset: \n\n\n\n\n\n\n\n\n\nCounty\nState\nTotal Population\nTotal White Population\nTotal Black Population\nTotal Latino Population\nIncarcerated Population\nIncarcerated White Population\nIncarcerated Black Population\nIncarcerated Latino Population\nNon-incarcerated Population\nNon-incarcerated White Population\nNon-Incarcerated Black Population\nNon-Incarcerated Latino Population\nRatio of Overrepresentation of Whites Incarcerated Compared to Whites Non-Incarcerated\nRatio of Overrepresentation of Blacks Incarcerated Compared to Blacks Non-Incarcerated\nRatio of Overrepresentation of Latinos Incarcerated Compared to Latinos Non-Incarcerated\n\n\n\n\n0\nAdams County\nIllinois\n67103\n62414\n2331\n776\n110\n73\n36\n0\n66993\n62341\n2295\n776\n0.71\n9.54\n0.00\n\n\n1\nAlexander County\nIllinois\n8238\n4983\n2915\n155\n411\n89\n242\n79\n7827\n4894\n2673\n76\n0.35\n1.72\n19.82\n\n\n2\nBond County\nIllinois\n17768\n15797\n1080\n547\n1542\n500\n657\n304\n16226\n15297\n423\n243\n0.34\n16.32\n13.14\n\n\n3\nBoone County\nIllinois\n54165\n40757\n1064\n10967\n71\n38\n12\n21\n54094\n40719\n1052\n10946\n0.71\n8.71\n1.46\n\n\n4\nBrown County\nIllinois\n6937\n5191\n1280\n402\n2059\n419\n1267\n367\n4878\n4772\n13\n35\n0.21\n227.91\n24.76"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#standardizing-column-names",
    "href": "technical-details/data-cleaning/main.html#standardizing-column-names",
    "title": "Data Cleaning",
    "section": "Standardizing Column Names",
    "text": "Standardizing Column Names\nTo improve readability and simplifyfurther processing, the column names in the dataset were converted to lowercase and spaces were replaced with underscores.\n\nrepresentation_df.columns = representation_df.columns.str.lower().str.replace(' ', '_')\nrepresentation_df.head()\n\n# Remove county from county name \nrepresentation_df['county'] = representation_df['county'].str.replace(' County', '', regex=False)\nrepresentation_df.head()\n\n\n\n\n\n\n\n\ncounty\nstate\ntotal_population\ntotal_white_population\ntotal_black_population\ntotal_latino_population\nincarcerated_population\nincarcerated_white_population\nincarcerated_black_population\nincarcerated_latino_population\nnon-incarcerated_population\nnon-incarcerated_white_population\nnon-incarcerated_black_population\nnon-incarcerated_latino_population\nratio_of_overrepresentation_of_whites_incarcerated_compared_to_whites_non-incarcerated\nratio_of_overrepresentation_of_blacks_incarcerated_compared_to_blacks_non-incarcerated\nratio_of_overrepresentation_of_latinos_incarcerated_compared_to_latinos_non-incarcerated\n\n\n\n\n0\nAdams\nIllinois\n67103\n62414\n2331\n776\n110\n73\n36\n0\n66993\n62341\n2295\n776\n0.71\n9.54\n0.00\n\n\n1\nAlexander\nIllinois\n8238\n4983\n2915\n155\n411\n89\n242\n79\n7827\n4894\n2673\n76\n0.35\n1.72\n19.82\n\n\n2\nBond\nIllinois\n17768\n15797\n1080\n547\n1542\n500\n657\n304\n16226\n15297\n423\n243\n0.34\n16.32\n13.14\n\n\n3\nBoone\nIllinois\n54165\n40757\n1064\n10967\n71\n38\n12\n21\n54094\n40719\n1052\n10946\n0.71\n8.71\n1.46\n\n\n4\nBrown\nIllinois\n6937\n5191\n1280\n402\n2059\n419\n1267\n367\n4878\n4772\n13\n35\n0.21\n227.91\n24.76"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#exporting-the-cleaned-dataset-2",
    "href": "technical-details/data-cleaning/main.html#exporting-the-cleaned-dataset-2",
    "title": "Data Cleaning",
    "section": "Exporting the Cleaned Dataset",
    "text": "Exporting the Cleaned Dataset\nThe final cleaned dataset is saved as representation_by_county.csv.\n\nrepresentation_df.to_csv('../../data/processed-data/representation_by_county.csv', index=False)\nprint(\"Data saved to 'representation_by_county_raw.csv'\")\n\nData saved to 'representation_by_county_raw.csv'"
  },
  {
    "objectID": "technical-details/data-balancing/main.html",
    "href": "technical-details/data-balancing/main.html",
    "title": "Counterfactual Data Balancing",
    "section": "",
    "text": "In this project, I set out to create a balanced dataset that would support supervised learning models for predicting the factors linked to exonerations. At the heart of this process is counterfactual balancing: building a dataset that includes exonerated individuals alongside a comparable group of non-exonerated individuals, drawn to reflect the broader incarcerated population in Illinois. This balance is critical—it allows the model to make fair and meaningful comparisons when identifying patterns and predictors of exoneration outcomes.\n\n\nCounterfactual data is a necessity when access to complete prison population records is unavailable. Since I don’t have access to a full dataset of all incarcerated individuals in Illinois and their exoneration statuses (e.g., ‘exonerated’, ‘not exonerate’), I relied on counterfactuals to bridge the gap and construct a balanced dataset.\nThe idea behind counterfactuals is simple: they allow us to ask “what if?” questions. For example: What if an exonerated person had not been exonerated? Would their characteristics look similar to non-exonerated individuals? Counterfactual data helps isolate these comparisons by holding everything else constant except the hypothetical condition—in this case, exoneration.\nAs explained in this primer on counterfactuals, a counterfactual statement operates on an unrealized “if” condition. The “if” portion, also known as the antecedent, frames the comparison: exonerated individuals versus those who weren’t. This approach is powerful because it reduces bias and ensures that the model is trained on data that is reliable, balanced, and representative.1\n\n\nThe implementation of this counterfactual data balancing relied heavily on expert guidance and code contributions from Professor Jeff Jacobs. His insights and support were invaluable in refining the methodology and making this process possible."
  },
  {
    "objectID": "technical-details/data-balancing/main.html#why-use-counterfactual-data",
    "href": "technical-details/data-balancing/main.html#why-use-counterfactual-data",
    "title": "Counterfactual Data Balancing",
    "section": "",
    "text": "Counterfactual data is a necessity when access to complete prison population records is unavailable. Since I don’t have access to a full dataset of all incarcerated individuals in Illinois and their exoneration statuses (e.g., ‘exonerated’, ‘not exonerate’), I relied on counterfactuals to bridge the gap and construct a balanced dataset.\nThe idea behind counterfactuals is simple: they allow us to ask “what if?” questions. For example: What if an exonerated person had not been exonerated? Would their characteristics look similar to non-exonerated individuals? Counterfactual data helps isolate these comparisons by holding everything else constant except the hypothetical condition—in this case, exoneration.\nAs explained in this primer on counterfactuals, a counterfactual statement operates on an unrealized “if” condition. The “if” portion, also known as the antecedent, frames the comparison: exonerated individuals versus those who weren’t. This approach is powerful because it reduces bias and ensures that the model is trained on data that is reliable, balanced, and representative.1\n\n\nThe implementation of this counterfactual data balancing relied heavily on expert guidance and code contributions from Professor Jeff Jacobs. His insights and support were invaluable in refining the methodology and making this process possible."
  },
  {
    "objectID": "technical-details/data-balancing/main.html#draw-representative-samples",
    "href": "technical-details/data-balancing/main.html#draw-representative-samples",
    "title": "Counterfactual Data Balancing",
    "section": "Draw Representative Samples",
    "text": "Draw Representative Samples\nThe first step in the simulation is to draw a representative sample of 548 “people” from the Illinois prison population. To achieve this, a weighted random sample with replacement was performed from the il_df dataset. Sampling weights were determined based on each county’s total incarcerated population, ensuring that counties with larger populations contributed proportionally more to the sample.\nA random seed (random_state=5000) was set to ensure the results are replicable. This step produces a valid population-weighted sample where the only known characteristic of each “person” is their county.\n\nil_sample_df = il_df.sample(\n    num_il,\n    replace = True,\n    weights = il_df['Total'],\n    random_state = 5000,\n).copy()\nil_sample_df.head()\n\n\n\n\n\n\n\n\ncounty\nstate\nTotal\nWhite\nBlack\nLatino\nOther\n\n\n\n\n15\nCook\nIllinois\n11649\n1769\n8369\n1468\n43\n\n\n36\nHenry\nIllinois\n301\n172\n108\n21\n0\n\n\n72\nPerry\nIllinois\n2323\n561\n1398\n352\n12\n\n\n15\nCook\nIllinois\n11649\n1769\n8369\n1468\n43\n\n\n53\nLogan\nIllinois\n3060\n963\n1705\n389\n3\n\n\n\n\n\n\n\n\nil_sample_df['county'].value_counts(normalize=True).head()\n\ncounty\nCook        0.142336\nWill        0.060219\nRandolph    0.056569\nPerry       0.040146\nLogan       0.040146\nName: proportion, dtype: float64"
  },
  {
    "objectID": "technical-details/data-balancing/main.html#simulating-racial-distribution",
    "href": "technical-details/data-balancing/main.html#simulating-racial-distribution",
    "title": "Counterfactual Data Balancing",
    "section": "Simulating Racial Distribution",
    "text": "Simulating Racial Distribution\nTo replicate the racial makeup of the incarcerated population, racial counts for each county were used to create a probability distribution for race. For each row in il_sample_df (which represents a sampled county), a distribution was formed based on the race-specific counts, and a single “person” was drawn from that distribution.\nThis process was done row-by-row using NumPy’s random.choice() function. A random seed (RNG) was also set to ensure the results remain consistent and replicable across runs.\n\nrng = np.random.default_rng(seed = 5000)\ndef draw_race_sample(row):\n  race_counts = [row[cur_val] for cur_val in race_category_names]\n  total_count = sum(race_counts)\n  race_probs = [cur_count / total_count for cur_count in race_counts]\n  # And now we have a probability distribution! We can use rng.choice() to sample from it\n  sampled_vals = rng.choice(race_category_names, size=1, p=race_probs)\n  # We only sampled 1 value here, so we use [0] to extract it\n  sampled_val = list(sampled_vals)[0]\n  return sampled_val\n\nBefore sampling, the function was tested by drawing multiple samples for a specific county—Cook County, in this case. To verify its accuracy, the expected proportions for sampling N inmates from Cook were first computed.\n\ncook_row = il_df[il_df['county'] == \"Cook\"].iloc[0]\nfor cname in race_category_names:\n  cook_row[f'{cname}_prop'] = cook_row[cname] / cook_row['Total']\ncook_row\n\ncounty             Cook\nstate          Illinois\nTotal             11649\nWhite              1769\nBlack              8369\nLatino             1468\nOther                43\nWhite_prop     0.151859\nBlack_prop     0.718431\nLatino_prop    0.126019\nOther_prop     0.003691\nName: 15, dtype: object\n\n\nThis means that if the draw_race_sample() function is working correctly, it should generate “White” 15.2% of the time, “Black” 71.8% of the time, and so on. To confirm this, a sample of size N=5000 was generated from Cook County to check whether the proportions align with the expected values.\n\nN = 5000\ncook_samples = [draw_race_sample(cook_row) for _ in range(N)]\ncook_sample_df = pd.DataFrame(cook_samples, columns = ['Race'])\ncook_sample_df['Race'].value_counts(normalize=True)\n\nRace\nBlack     0.7186\nWhite     0.1518\nLatino    0.1260\nOther     0.0036\nName: proportion, dtype: float64\n\n\nThe results look good and are very close to the expected proportions, which confirms that the draw_race_sample() function is working as intended. With this validation, the function can now be used to sample a race value for each row in il_sample_df.\nThis step also introduces the tqdm library, which is useful for tracking progress when running simulations like this. It helps monitor how long the code takes per row, ensuring the simulation remains efficient.\n\nil_sample_df['Race'] = il_sample_df.progress_apply(draw_race_sample, axis=1)\n\n100%|██████████| 548/548 [00:00&lt;00:00, 8289.38it/s]\n\n\n\nsample_cols_to_keep = [\n    'county',\n    'state',\n    'Race'\n]\nil_sample_df = il_sample_df[sample_cols_to_keep].copy()\nil_sample_df\n\n\n\n\n\n\n\n\ncounty\nstate\nRace\n\n\n\n\n15\nCook\nIllinois\nBlack\n\n\n36\nHenry\nIllinois\nWhite\n\n\n72\nPerry\nIllinois\nBlack\n\n\n15\nCook\nIllinois\nBlack\n\n\n53\nLogan\nIllinois\nBlack\n\n\n...\n...\n...\n...\n\n\n51\nLee\nIllinois\nWhite\n\n\n10\nChristian\nIllinois\nBlack\n\n\n25\nFayette\nIllinois\nBlack\n\n\n44\nKane\nIllinois\nWhite\n\n\n52\nLivingston\nIllinois\nBlack\n\n\n\n\n548 rows × 3 columns\n\n\n\nLet’s take a look at the racial distribution of the Cook County subset from our sample to see how it turned out:\n\ncook_sample_df = il_sample_df[il_sample_df['county'] == \"Cook\"].copy()\ncook_sample_df['Race'].value_counts(normalize=True)\n\nRace\nBlack     0.743590\nLatino    0.166667\nWhite     0.089744\nName: proportion, dtype: float64\n\n\nThe results show a slight oversample of Latinos compared to the population expectation and an undersample of Whites. While this might seem odd, it’s actually a feature of this sampling process. The goal here is to simulate the simplified model of the Exoneration Registry, where the sample of exonerees represents a subset of 548 inmates from Cook County. This allows for a direct comparison with another size-548 subset of those still incarcerated in Cook.\nWith this step completed, the 548 rows from il_sample_df can now be combined with the 548 rows in exon_il_df, creating a balanced DataFrame with a total of 1,096 rows. Half of these rows represent exonerated individuals from Illinois, and the other half represent non-exonerated individuals, sampled to be statistically representative of Illinois’ incarcerated population as a whole."
  },
  {
    "objectID": "technical-details/data-balancing/main.html#constructing-the-final-balanced-dataset",
    "href": "technical-details/data-balancing/main.html#constructing-the-final-balanced-dataset",
    "title": "Counterfactual Data Balancing",
    "section": "Constructing the Final Balanced Dataset",
    "text": "Constructing the Final Balanced Dataset\nTo prepare the final balanced dataset, a new label column was added to distinguish between exonerated and non-exonerated individuals. Specifically:\n- The Label column in exon_il_df was set to “Exonerated”.\n- The Label column in il_sample_df was set to “Non-Exonerated”.\nTo avoid confusion when combining datasets, the county column in il_sample_df was renamed to County. With the labels in place and columns aligned, both datasets were combined into a single DataFrame using pd.concat().\nNext, a race mapping was applied to standardize the race categories across datasets:\n- “Asian” and “Native American” were combined into the “Other” category.\n- “Black,” “White,” and “Hispanic” categories were kept as-is.\nTo clean up, the race and Race columns were combined, prioritizing non-NaN values to ensure no data was lost. The original race column was then dropped. Similarly, the county and County columns were merged, and the original county column was removed to streamline the final DataFrame.\nFinally, the resulting Race and County columns were checked to confirm the expected values, and the first few rows of the balanced dataset were displayed to verify everything was in place.\n\n# Construct our new label: exonerated vs. non-exonerated\nexon_il_df['Label'] = \"Exonerated\"\nil_sample_df['Label'] = \"Non-Exonerated\"\nil_sample_df = il_sample_df.rename(columns={'county' : 'County'}) # Rename to distinguish when combining datasets\n\n# And combine!\nbalanced_df = pd.concat([exon_il_df, il_sample_df], axis=0)\n# Define the mapping for 'race'\nrace_mapping = {\n    'Asian': 'Other',\n    'Native American': 'Other',\n    'Black': 'Black',\n    'White': 'White',\n    'Hispanic': 'Hispanic'\n}\n\n\n# Map the 'race' column\nbalanced_df['race'] = balanced_df['race'].map(race_mapping)\n\n# Combine 'race' and 'Race' columns, prioritizing non-NaN values\nbalanced_df['Race'] = balanced_df['race'].combine_first(balanced_df['Race'])\n\n# Drop the old 'race' column\nbalanced_df.drop(columns=['race'], inplace=True)\n\n# Combine 'county' and 'County' columns, prioritizing non-NaN values\nbalanced_df['County'] = balanced_df['county'].combine_first(balanced_df['County'])\n\n# Drop the old 'county' column\nbalanced_df.drop(columns=['county'], inplace=True)\n\n# Verify the final Race column\nprint(balanced_df['Race'].value_counts())\nprint(balanced_df['County'].value_counts())\nbalanced_df.head()\n\nRace\nBlack     709\nWhite     207\nLatino     92\nOther       5\nName: count, dtype: int64\nCounty\nCook           552\nWill            37\nRandolph        31\nJefferson       23\nLogan           22\nPerry           22\nLivingston      22\nFulton          21\nJohnson         21\nTazewell        19\nLawrence        18\nMontgomery      17\nBond            17\nVermilion       16\nDuPage          15\nWinnebago       15\nLake            14\nSt. Clair       14\nClinton         14\nLa Salle        14\nFayette         13\nLee             13\nBrown           12\nKane            12\nKnox            11\nPeoria          10\nMorgan          10\nRock Island     10\nMacon            9\nCrawford         9\nMcHenry          7\nChristian        6\nWilliamson       6\nChampaign        5\nMcLean           4\nSangamon         4\nHenry            3\nKankakee         3\nStephenson       2\nWoodford         2\nEdgar            2\nEffingham        2\nIroquois         2\nAdams            2\nRichland         1\nMenard           1\nPope             1\nMadison          1\nBoone            1\nJackson          1\nCumberland       1\nWashington       1\nDupage           1\nDekalb           1\nMoultrie         1\nLaSalle          1\nDe Witt          1\nName: count, dtype: int64\n\n\n\n\n\n\n\n\n\nlast_name\nfirst_name\nage\nsex\nstate\nlatitude\nlongitude\nworst_crime_display\nsentence\nsentence_in_years\n...\nwitness_tampering_or_misconduct_interrogating_co_defendant\nmisconduct_in_interrogation_of_exoneree\nperjury_by_official\nprosecutor_lied_in_court\ntag_sum\ngeocode_address\nRace_orig\nLabel\nCounty\nRace\n\n\n\n\n0\nAbbott\nCinque\n19.0\nmale\nIllinois\n41.819738\n-87.756525\nDrug Possession or Sale\nProbation\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n7.0\nCook County, Illinois, United States\nBlack\nExonerated\nCook\nBlack\n\n\n1\nAbernathy\nChristopher\n17.0\nmale\nIllinois\n41.819738\n-87.756525\nMurder\nLife without parole\n100.0\n...\n0.0\n1.0\n0.0\n0.0\n10.0\nCook County, Illinois, United States\nWhite\nExonerated\nCook\nWhite\n\n\n2\nAbrego\nEruby\n20.0\nmale\nIllinois\n41.819738\n-87.756525\nMurder\n90 years\n90.0\n...\n1.0\n1.0\n1.0\n0.0\n9.0\nCook County, Illinois, United States\nHispanic\nExonerated\nCook\nNaN\n\n\n3\nAdams\nDemetris\n22.0\nmale\nIllinois\n41.819738\n-87.756525\nDrug Possession or Sale\n1 year\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n7.0\nCook County, Illinois, United States\nBlack\nExonerated\nCook\nBlack\n\n\n4\nAdams\nKenneth\n22.0\nmale\nIllinois\n41.819738\n-87.756525\nMurder\n75 years\n75.0\n...\n1.0\n0.0\n0.0\n0.0\n11.0\nCook County, Illinois, United States\nBlack\nExonerated\nCook\nBlack\n\n\n\n\n5 rows × 51 columns\n\n\n\n\nbalanced_df.to_csv(\"../../data/processed-data/exonerees_balanced.csv\", index=False)"
  },
  {
    "objectID": "technical-details/data-balancing/main.html#next-steps",
    "href": "technical-details/data-balancing/main.html#next-steps",
    "title": "Counterfactual Data Balancing",
    "section": "Next Steps",
    "text": "Next Steps\nThis balanced dataset can now be used for supervised learning tasks, such as:\n- Predicting Exoneration Factors: Training machine learning models to identify the characteristics most associated with exoneration outcomes.\n- Comparative Analysis: Exploring differences in demographics, geographic distribution, or other variables between exonerated and non-exonerated individuals.\n- Visualization and Insights: Mapping trends or disparities across counties and racial groups to better understand systemic patterns in wrongful convictions.\nWith this dataset, models and analyses can provide deeper insights into the factors driving exonerations while ensuring fairness and balance in comparisons."
  },
  {
    "objectID": "technical-details/data-collection/main.html",
    "href": "technical-details/data-collection/main.html",
    "title": "Data Collection",
    "section": "",
    "text": "For the data collection process, I gathered multiple datasets, including arrest records from the ICJIA Arrest Explorer, exoneration data from the National Registry of Exonerations, incarceration demographics from the Prison Policy Initiative, and geospatial information. This diverse collection enabled an in-depth examination of systemic racial inequities that drive the over-policing of marginalized communities while also facilitating a critical analysis of nuanced racial patterns in exoneration data, exploring their connection to broader societal structures. Together, these datasets provide a framework for understanding the interconnected dynamics of racial injustice in the criminal justice system.\n\n\nMy goal in the data collection process is to gather the datasets needed to examine systemic racial disparities in policing and exonerations. I’m focused on sourcing reliable arrest records, exoneration data, incarceration demographics, and geospatial information. Ultimately, I aim to compile data that can stand alone or work together to analyze over-policing trends, uncover racial patterns in exonerations, and explore the geographic dynamics connected to these issues. I’m particularly driven by the need to ground these systemic inequities in hard numbers and statistical analysis. Too often, people refuse to acknowledge the reality of racism without concrete evidence. By putting real data behind these patterns, I hope to demonstrate the scope and impact of these injustices in a way that cannot be ignored.\n\n\n\n\nIdentify and collect datasets needed to analyze racial disparities in over-policing and wrongful convictions in Illinois, including arrest records, exoneration data, incarceration demographics, and geospatial information.\n\nEnsure datasets are accurate, up-to-date, and aligned with the project’s goals by prioritizing reliable sources like official registries and well-documented repositories.\n\nStructure the data to support analysis of racial patterns in policing and exonerations, allowing for both independent and interconnected evaluations.\n\nDocument all steps in the data collection process, including sourcing methods and preprocessing workflows, to ensure transparency and reproducibility.\n\nCollect and preprocess geocoded data and Illinois county shapefiles to enable geographic exploratory data analysis and visualizations."
  },
  {
    "objectID": "technical-details/data-collection/main.html#goals-motivation",
    "href": "technical-details/data-collection/main.html#goals-motivation",
    "title": "Data Collection",
    "section": "",
    "text": "My goal in the data collection process is to gather the datasets needed to examine systemic racial disparities in policing and exonerations. I’m focused on sourcing reliable arrest records, exoneration data, incarceration demographics, and geospatial information. Ultimately, I aim to compile data that can stand alone or work together to analyze over-policing trends, uncover racial patterns in exonerations, and explore the geographic dynamics connected to these issues. I’m particularly driven by the need to ground these systemic inequities in hard numbers and statistical analysis. Too often, people refuse to acknowledge the reality of racism without concrete evidence. By putting real data behind these patterns, I hope to demonstrate the scope and impact of these injustices in a way that cannot be ignored."
  },
  {
    "objectID": "technical-details/data-collection/main.html#objectives",
    "href": "technical-details/data-collection/main.html#objectives",
    "title": "Data Collection",
    "section": "",
    "text": "Identify and collect datasets needed to analyze racial disparities in over-policing and wrongful convictions in Illinois, including arrest records, exoneration data, incarceration demographics, and geospatial information.\n\nEnsure datasets are accurate, up-to-date, and aligned with the project’s goals by prioritizing reliable sources like official registries and well-documented repositories.\n\nStructure the data to support analysis of racial patterns in policing and exonerations, allowing for both independent and interconnected evaluations.\n\nDocument all steps in the data collection process, including sourcing methods and preprocessing workflows, to ensure transparency and reproducibility.\n\nCollect and preprocess geocoded data and Illinois county shapefiles to enable geographic exploratory data analysis and visualizations."
  },
  {
    "objectID": "technical-details/data-collection/main.html#foundation-exoneration-and-arrest-datasets",
    "href": "technical-details/data-collection/main.html#foundation-exoneration-and-arrest-datasets",
    "title": "Data Collection",
    "section": "Foundation: Exoneration and Arrest Datasets",
    "text": "Foundation: Exoneration and Arrest Datasets\nThe exoneration and arrest datasets formed the backbone of the project, each bringing a unique perspective to the analysis of racial disparities in the criminal justice system.\n\nIllinois Arrest Data\nThe Illinois arrest dataset was sourced from the Illinois Criminal Justice Information Authority’s (ICJIA) Arrest Explorer, a platform providing aggregate arrest data from the Criminal History Record Information (CHRI) system—a statewide resource for demographic and offense-related variables.1\nTo ensure privacy and confidentiality, ICJIA applied the following modifications:\n\nCounts under 10 are approximated (e.g., 1 for counts 0–4, 6 for counts 5–9),\n\nSubtotals, such as arrests by race or county, are accurate within +1/-1, and\n\nStatewide totals align exactly with the CHRI database at the time of retrieval, which occurs twice annually.\n\nFurther, the dataset excludes juvenile arrests, class C misdemeanors, and cases with missing demographic details. For this project, the data was first filtered by race, county, and year, and then downloaded directly to examine patterns relevant to my analysis.2\n\nimport pandas as pd\narrest_df = pd.read_csv('../../data/raw-data/illinois_arrest_explorer_data.csv')\narrest_df.head(3)\n\n\n\n\n\n\n\n\nYear\nrace\ncounty_Adams\ncounty_Alexander\ncounty_Bond\ncounty_Boone\ncounty_Brown\ncounty_Bureau\ncounty_Calhoun\ncounty_Carroll\n...\ncounty_Wabash\ncounty_Warren\ncounty_Washington\ncounty_Wayne\ncounty_White\ncounty_Whiteside\ncounty_Will\ncounty_Williamson\ncounty_Winnebago\ncounty_Woodford\n\n\n\n\n0\n2001\nAfrican American\n226\n147\n25\n18\n18\n48\n6\n12\n...\n6\n46\n25\n6\n16\n128\n3000\n75\n2509\n42\n\n\n1\n2001\nAsian\n1\n1\n1\n1\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n1\n14\n1\n28\n1\n\n\n2\n2001\nHispanic\n1\n1\n1\n1\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n3 rows × 106 columns\n\n\n\n\n\nExoneration Data\nThe exoneration dataset was downloaded directly from the National Registry of Exonerations, a collaborative initiative by the Newkirk Center for Science and Society at the University of California (Irvine), the University of Michigan Law School, and Michigan State University College of Law. Established in 2012 by Rob Warden, then Executive Director of Northwestern University’s Pritzker School of Law’s Center on Wrongful Convictions, and Samuel R. Gross, a Law Professor at the University of Michigan, the Registry collects and publishes comprehensive, searchable statistical data and detailed case records for exonerations of innocent criminal defendants in the United States dating back to 19893.\nThe Registry defines exonerations as cases where a person, following new evidence of innocence, is officially cleared through actions like factual declarations of innocence, pardons, or the dismissal/acquittal of charges.4\nTo access the exoneration dataset, a spreadsheet request form was submitted, and the dataset was provided under conditions ensuring its proper use. These conditions include restrictions on retransmission, a requirement for advance notice of publication, and the obligation to report any identified errors or missing data.\n\nexoneration_df = pd.read_csv('../../data/raw-data/US_exoneration_data.csv')\nexoneration_df.head(3)\n\n\n\n\n\n\n\n\nLast Name\nFirst Name\nAge\nRace\nSex\nState\nCounty\nTags\nWorst Crime Display\nSentence\n...\nF/MFE\nFC\nILD\nP/FA\nDNA\nMWID\nOM\nDate of Exoneration\nDate of 1st Conviction\nDate of Release\n\n\n\n\n0\nAbbitt\nJoseph\n31.0\nBlack\nMale\nNorth Carolina\nForsyth\nCV;#IO;#SA\nChild Sex Abuse\nLife\n...\nNaN\nNaN\nNaN\nNaN\nDNA\nMWID\nNaN\n9/2/09\n6/22/95\n9/2/09\n\n\n1\nAbbott\nCinque\n19.0\nBlack\nMale\nIllinois\nCook\nCIU;#IO;#NC;#P\nDrug Possession or Sale\nProbation\n...\nNaN\nNaN\nNaN\nP/FA\nNaN\nNaN\nOM\n2/1/22\n3/25/08\n3/25/08\n\n\n2\nAbdal\nWarith Habib\n43.0\nBlack\nMale\nNew York\nErie\nIO;#SA\nSexual Assault\n20 to Life\n...\nF/MFE\nNaN\nNaN\nNaN\nDNA\nMWID\nOM\n9/1/99\n6/6/83\n9/1/99\n\n\n\n\n3 rows × 22 columns"
  },
  {
    "objectID": "technical-details/data-collection/main.html#adding-context-web-scraping-and-demographic-data",
    "href": "technical-details/data-collection/main.html#adding-context-web-scraping-and-demographic-data",
    "title": "Data Collection",
    "section": "Adding Context: Web Scraping and Demographic Data",
    "text": "Adding Context: Web Scraping and Demographic Data\nTo contextualize these datasets within broader racial and geographic dynamics, I scraped incarceration demographics for Illinois counties from the Prison Policy Initiative’s. Using Python libraries like requests and BeautifulSoup, I extracted and processed incarceration rates by race for geographic analysis. This provided insight into racial overrepresentation within incarceration systems and illuminated trends at the county level.5\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Retrieve the HTML content of the target webpage\nhtml_url = \"https://www.prisonpolicy.org/racialgeography/counties.html\"\nresult = requests.get(html_url)\n\n# Parse the HTML content with BeautifulSoup\nsoup = BeautifulSoup(result.text)\n\n# Locate the table element containing the data\ntable_elt = soup.find(\"table\")\n\n# Convert the HTML table into a Pandas DataFrame\ntable_sio = StringIO(str(table_elt))\ncounty_df = pd.read_html(table_sio)[0]\n\n# Clean the column names by removing unnecessary characters\ncounty_df.columns = [c.replace(\"\",\"\").replace(\"\",\"\").strip() for c in county_df.columns]\n\n# Filter for Illinois\nil_df = county_df[county_df['State'] == \"Illinois\"].copy()\n\n# Export to CSV\nil_df.to_csv('../../data/raw-data/representation_by_county_raw.csv', index=False)\nprint(\"Data saved to 'representation_by_county_raw.csv'\")\nil_df.head()\n\nData saved to 'representation_by_county_raw.csv'\n\n\n\n\n\n\n\n\n\nCounty\nState\nTotal Population\nTotal White Population\nTotal Black Population\nTotal Latino Population\nIncarcerated Population\nIncarcerated White Population\nIncarcerated Black Population\nIncarcerated Latino Population\nNon-incarcerated Population\nNon-incarcerated White Population\nNon-Incarcerated Black Population\nNon-Incarcerated Latino Population\nRatio of Overrepresentation of Whites Incarcerated Compared to Whites Non-Incarcerated\nRatio of Overrepresentation of Blacks Incarcerated Compared to Blacks Non-Incarcerated\nRatio of Overrepresentation of Latinos Incarcerated Compared to Latinos Non-Incarcerated\n\n\n\n\n595\nAdams County\nIllinois\n67103\n62414\n2331\n776\n110\n73\n36\n0\n66993\n62341\n2295\n776\n0.71\n9.54\n0.00\n\n\n596\nAlexander County\nIllinois\n8238\n4983\n2915\n155\n411\n89\n242\n79\n7827\n4894\n2673\n76\n0.35\n1.72\n19.82\n\n\n597\nBond County\nIllinois\n17768\n15797\n1080\n547\n1542\n500\n657\n304\n16226\n15297\n423\n243\n0.34\n16.32\n13.14\n\n\n598\nBoone County\nIllinois\n54165\n40757\n1064\n10967\n71\n38\n12\n21\n54094\n40719\n1052\n10946\n0.71\n8.71\n1.46\n\n\n599\nBrown County\nIllinois\n6937\n5191\n1280\n402\n2059\n419\n1267\n367\n4878\n4772\n13\n35\n0.21\n227.91\n24.76"
  },
  {
    "objectID": "technical-details/data-collection/main.html#geographical-insights-api-based-geocoding",
    "href": "technical-details/data-collection/main.html#geographical-insights-api-based-geocoding",
    "title": "Data Collection",
    "section": "Geographical Insights: API-Based Geocoding",
    "text": "Geographical Insights: API-Based Geocoding\nTo strengthen the exploratory data analysis (EDA), I utilized geocoded data, adding latitude, longitude, and full addresses for Illinois counties. This addition enabled the visualization of systemic racial disparities across geographic areas, revealing trends and disparities among counties, identifying geographic clusters of exoneration cases, and examining regional variations in systemic factors.\nThe geocoding process was conducted using GeoPy, a Python library that serves as an interface to geocoding APIs, specifically the Nominatim API from OpenStreetMap. GeoPy simplifies the retrieval of geographic details like latitude, longitude, and full address from place names by sending requests to the Nominatim API and processing the responses.\n\n# Import the Nominatim geocoder for converting location names into geographic coordinates:\nfrom geopy.geocoders import Nominatim  \n\n# Initialize the geolocator with a user-defined agent to avoid request limits:\ngeolocator = Nominatim(user_agent=\"illinois_exoneration_geocode\") \n\n# Clean the 'County' column by removing the word \"County\" and any extra spaces:\nil_df['County'] = il_df['County'].str.replace(\"County\", \"\").str.strip()\n\n# Rename the DataFrame to clarify it contains Illinois counties:\nillinois_counties = il_df[['County', 'State']].copy()\n\n# Define a function to geocode counties and return geographic details:\ndef geocode_county(row):\n    \"\"\"\n    Takes a row containing 'County' and 'State' columns.\n    Uses the geolocator to find the full address, latitude, and longitude.\n    Returns a dictionary with the geocoded data or None if geocoding fails.\n    \"\"\"\n    try:\n        # Combine county and state into a query string for geocoding:\n        location = geolocator.geocode(f\"{row['County']}, {row['State']}, USA\")\n        \n        # If a location is found, return the geocoded details:\n        if location:\n            return {\n                'address': location.address,    # The full geocoded address\n                'latitude': location.latitude,  # The latitude coordinate\n                'longitude': location.longitude # The longitude coordinate\n            }\n        else:  # Print a message if no geocoding result is found:\n            print(f\"Failed: No result for {row['County']}, {row['State']}\")\n            return None\n    except Exception as e:  # Handle errors during geocoding and print them:\n        print(f\"Error geocoding {row['County']}, {row['State']}: {e}\")\n        return None\n\n# Apply the geocoding function to each Illinois county:\ngeocoded_results = illinois_counties.apply(geocode_county, axis=1)\n\n# Extract the geocoding results into separate columns for address, latitude, and longitude:\nillinois_counties['geocode_address'] = geocoded_results.apply(lambda x: x['address'] if isinstance(x, dict) and 'address' in x else None)\nillinois_counties['latitude'] = geocoded_results.apply(lambda x: x['latitude'] if isinstance(x, dict) and 'latitude' in x else None)\nillinois_counties['longitude'] = geocoded_results.apply(lambda x: x['longitude'] if isinstance(x, dict) and 'longitude' in x else None)\n\n# Display the first few rows of the DataFrame with geocoded data:\nillinois_counties.head()\n\n\n\n\n\n\n\n\nCounty\nState\ngeocode_address\nlatitude\nlongitude\n\n\n\n\n595\nAdams\nIllinois\nAdams County, Illinois, United States\n39.978779\n-91.211006\n\n\n596\nAlexander\nIllinois\nAlexander County, Illinois, United States\n37.180153\n-89.350283\n\n\n597\nBond\nIllinois\nBond County, Illinois, United States\n38.863033\n-89.439142\n\n\n598\nBoone\nIllinois\nBoone County, Illinois, United States\n42.321246\n-88.823551\n\n\n599\nBrown\nIllinois\nBrown County, Illinois, United States\n39.949821\n-90.748566\n\n\n\n\n\n\n\n\n# Save the geocoded results to a CSV file \nillinois_counties.to_csv(\"../../data/raw-data/geocoded_population_counties.csv\", index=False)\nprint(\"Geocoded county data saved to 'geocoded_population_counties.csv'\")\n\nGeocoded county data saved to 'geocoded_population_counties.csv'"
  },
  {
    "objectID": "technical-details/data-collection/main.html#mapping-illinois-county",
    "href": "technical-details/data-collection/main.html#mapping-illinois-county",
    "title": "Data Collection",
    "section": "Mapping: Illinois County",
    "text": "Mapping: Illinois County\nTo visualize geocoded data and perform geographic exploratory data analysis (EDA), I required a shapefile for Illinois county boundaries. While the Census Bureau provides Illinois shapefiles, these files did not work as expected for my Exploratory Data Analysis (EDA) purposes due to compatibility issues.\nInstead, I sourced a shapefile directly from the Illinois State Geological Survey (ISGS) which included county boundaries in a format compatible with the GIS tools I used. ISGS’s shapefiles provided the geographic foundation for mapping and visualizing trends across Illinois counties for EDA which will allow me to adda crucial layer of context to the datasets and support meaningful EDA.6"
  },
  {
    "objectID": "technical-details/data-collection/main.html#challenges",
    "href": "technical-details/data-collection/main.html#challenges",
    "title": "Data Collection",
    "section": "Challenges",
    "text": "Challenges\nThe foundation for meaningful data analysis is accurate and reliable crime data, yet long-standing challenges in criminal history record systems continue to undermine their precision and dependability7. As outlined in the Use and Management of Criminal History Record Information Report (1993), issues such as incomplete data reporting, delays in recording arrests and case dispositions, and inconsistent fingerprint submissions have plagued the system for decades7 . Criminal History Record Information (CHRI), the backbone for arrest datasets like those from the Illinois Criminal Justice Information Authority (ICJIA), is vulnerable to these shortcomings. Dispositions, or the outcomes of arrests, are often missing or delayed, leaving critical gaps in the data that make it difficult to analyze systemic trends. While advances in technology, such as digital fingerprinting, have improved some processes, data fragmentation and insufficient oversight remain persistent barriers to comprehensive and accurate reporting.\nFast forward thirty years, and many of these challenges remain unresolved—now further complicated by uneven implementation of modern systems. The Marshall Project highlights a striking example: in 2022, the FBI’s shift to the National Incident-Based Reporting System (NIBRS) created a significant data gap in national crime statistics8. Over 6,000 police agencies failed to submit their data, representing nearly one-third of all agencies and leaving vast portions of the U.S. population unaccounted for. This includes major departments like the NYPD and LAPD, alongside countless smaller agencies8. These gaps reflect broader systemic failures—inconsistent adoption of updated systems, lack of adequate funding, and minimal oversight—that echo the same issues identified decades ago.\nIn short, crime data, even when sourced from official systems, remains inherently flawed and incomplete. Whether due to outdated processes, inconsistent reporting practices, privacy-driven modifications, or gaps in modern collection systems, crime data often falls short of providing a fully accurate or comprehensive picture. As a result, any analysis relying on this data must account for these imperfections, recognizing that while the data can uncover critical trends and systemic disparities, it is rarely a perfect representation of reality.\n\nArrest Data Precision\nThe Illinois arrest dataset obtained from the ICJIA Arrest Explorer highlights the trade-offs between privacy and precision, adding to the broader challenges of criminal justice data. To protect confidentiality, counts under 10 are approximated—values between 0 and 4 are replaced with 1, while counts from 5 to 9 are replaced with 6. Though this approach is necessary to safeguard sensitive data, it can introduce distortions, particularly in smaller counties or demographic groups, where even slight approximations can significantly skew trends and reduce the accuracy of analysis. Compounding these issues, the lack of oversight in reporting further undermines data consistency and reliability, reflecting the broader systemic limitations that continue to plague many criminal justice datasets."
  },
  {
    "objectID": "technical-details/data-collection/main.html#benchmarks",
    "href": "technical-details/data-collection/main.html#benchmarks",
    "title": "Data Collection",
    "section": "Benchmarks",
    "text": "Benchmarks\nThe National Registry of Exonerations is a trusted and widely used resource, frequently cited in academic and legal research. Unlike government databases, the Registry is run by a team of researchers and academics, which brings a level of precision and thoroughness often missing in state-managed systems9. Its foundation in meticulous documentation and independent research makes it less vulnerable to political or institutional biases. As a result, the Registry stands out as a more reliable and comprehensive tool for understanding wrongful convictions."
  },
  {
    "objectID": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "title": "Data Collection",
    "section": "Conclusion and Future Steps",
    "text": "Conclusion and Future Steps\nThe data collected from various sources provide a foundation for examining systemic racial disparities in over-policing and wrongful convictions. However, challenges such as the lack of precision in Illinois arrest datasets and broader gaps in national crime reporting underscore the need to approach findings with caution. Even with these limitations, the use of reliable resources like the National Registry of Exonerations adds credibility to the analysis. Moving forward, future research could focus on compiling crime data from a single state across local, state, and federal agencies to better understand inconsistencies and uncover patterns of underreporting. Expanding this work to compare data collection practices across states could also reveal regional differences in crime reporting and highlight systemic disparities in how data is recorded and shared."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#sample-of-balanced-dataset",
    "href": "technical-details/supervised-learning/main.html#sample-of-balanced-dataset",
    "title": "Supervised Learning",
    "section": "Sample of Balanced Dataset",
    "text": "Sample of Balanced Dataset\n\n# Import necessary Libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.metrics import roc_curve, roc_auc_score\nimport matplotlib.pyplot as plt\n\nexonerees_balanced = pd.read_csv('../../data/processed-data/exonerees_balanced.csv')\nexonerees_balanced.head()\n\n\n\n\n\n\n\n\nlast_name\nfirst_name\nage\nsex\nstate\nlatitude\nlongitude\nworst_crime_display\nsentence\nsentence_in_years\n...\nwitness_tampering_or_misconduct_interrogating_co_defendant\nmisconduct_in_interrogation_of_exoneree\nperjury_by_official\nprosecutor_lied_in_court\ntag_sum\ngeocode_address\nRace_orig\nLabel\nCounty\nRace\n\n\n\n\n0\nAbbott\nCinque\n19.0\nmale\nIllinois\n41.819738\n-87.756525\nDrug Possession or Sale\nProbation\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n7.0\nCook County, Illinois, United States\nBlack\nExonerated\nCook\nBlack\n\n\n1\nAbernathy\nChristopher\n17.0\nmale\nIllinois\n41.819738\n-87.756525\nMurder\nLife without parole\n100.0\n...\n0.0\n1.0\n0.0\n0.0\n10.0\nCook County, Illinois, United States\nWhite\nExonerated\nCook\nWhite\n\n\n2\nAbrego\nEruby\n20.0\nmale\nIllinois\n41.819738\n-87.756525\nMurder\n90 years\n90.0\n...\n1.0\n1.0\n1.0\n0.0\n9.0\nCook County, Illinois, United States\nHispanic\nExonerated\nCook\nNaN\n\n\n3\nAdams\nDemetris\n22.0\nmale\nIllinois\n41.819738\n-87.756525\nDrug Possession or Sale\n1 year\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n7.0\nCook County, Illinois, United States\nBlack\nExonerated\nCook\nBlack\n\n\n4\nAdams\nKenneth\n22.0\nmale\nIllinois\n41.819738\n-87.756525\nMurder\n75 years\n75.0\n...\n1.0\n0.0\n0.0\n0.0\n11.0\nCook County, Illinois, United States\nBlack\nExonerated\nCook\nBlack\n\n\n\n\n5 rows × 51 columns"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#race-distribution-by-exoneration-status",
    "href": "technical-details/supervised-learning/main.html#race-distribution-by-exoneration-status",
    "title": "Supervised Learning",
    "section": "Race Distribution by Exoneration Status",
    "text": "Race Distribution by Exoneration Status\nA crosstabulation was conducted to analyze the distribution of race between exonerated and non-exonerated individuals within the balanced dataset. Using the pd.crosstab() function, the proportion of each racial category across the two groups was calculated:\n\nExonerated individuals: those who were wrongfully convicted and later cleared.\n\nNon-Exonerated individuals: a simulated group drawn from the broader incarcerated population.\n\nBy normalizing the values, the crosstab presents the relative frequency of each racial category within both groups. This allows us to pinpoint patterns or disparities in the racial composition of exonerated individuals compared to the simulated non-exonerated group.\n\npd.crosstab(\n    exonerees_balanced['Label'], exonerees_balanced['Race'],\n    margins=True,\n    normalize=True\n)\n\n\n\n\n\n\n\nRace\nBlack\nLatino\nOther\nWhite\nAll\n\n\nLabel\n\n\n\n\n\n\n\n\n\nExonerated\n0.412636\n0.000000\n0.000000\n0.046397\n0.459033\n\n\nNon-Exonerated\n0.287266\n0.090819\n0.004936\n0.157947\n0.540967\n\n\nAll\n0.699901\n0.090819\n0.004936\n0.204344\n1.000000\n\n\n\n\n\n\n\n\nExonerated Group: The majority are Black (41.2%) and White (4.6%), with no representation for Latino or Other races.\n\nNon-Exonerated Group: The racial distribution includes 28.7% Black, 9.1% Latino, 0.5% Other, and 15.8% White.\n\nOverall: The total racial distribution reflects the balanced nature of the dataset, with 69.9% Black, 9.1% Latino, and smaller proportions for Other and White categories.\n\nThis breakdown provides an overview of how race is distributed within the dataset, ensuring transparency in the balancing process and highlighting any disparities between the two groups."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#exoneration-label-mapping",
    "href": "technical-details/supervised-learning/main.html#exoneration-label-mapping",
    "title": "Supervised Learning",
    "section": "Exoneration Label Mapping",
    "text": "Exoneration Label Mapping\nIn this step, the labels identifying whether an individual was exonerated or not were converted into a binary format to prepare the dataset for supervised learning. The Label column was mapped as follows:\n- 'Exonerated' → 1\n- 'Non-Exonerated' → 0\nFor clarity and consistency, the column was renamed “Exonerated”, making it easier to interpret the target variable during modeling. To streamline the dataset for further processing, only the relevant features were retained: Race, County, and the newly mapped Exonerated column.\nA quick preview of the transformed data confirmed the changes, showing the updated binary labels alongside the selected features. This clean, focused dataset will serve as the foundation for building machine learning models in the next steps.\n\n# Map 'Exonerated' to 1 and 'Non-Exonerated' to 0\nexonerees_balanced['Label'] = exonerees_balanced['Label'].map({'Exonerated': 1, 'Non-Exonerated': 0})\n\n# Rename the column to 'exonerated'\nexonerees_balanced.rename(columns={'Label': 'Exonerated'}, inplace=True)\n# Keep relevant features\ndata = exonerees_balanced[['Race', 'County', 'Exonerated']].copy()\n\n# Preview the transformed data\nprint(data.head())\n\n\n    Race County  Exonerated\n0  Black   Cook           1\n1  White   Cook           1\n2    NaN   Cook           1\n3  Black   Cook           1\n4  Black   Cook           1"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#one-hot-encoding",
    "href": "technical-details/supervised-learning/main.html#one-hot-encoding",
    "title": "Supervised Learning",
    "section": "One Hot Encoding",
    "text": "One Hot Encoding\nOne hot encoding was applied to convert the categorical features “Race” and “County” into a numerical format suitable for machine learning models. Since algorithms like logistic regression and other supervised models require numerical inputs, this transformation ensures these features can be effectively incorporated during training.\nEach unique value in the “Race” and “County” columns was transformed into its own binary column, where a value of 1 indicates the presence of a specific category, and 0 indicates its absence. To avoid any loss of information, all categories were retained during the encoding process.\nOnce encoded, the original “Race” and “County” columns were dropped and replaced with their respective binary columns. This transformation resulted in a dataset with 64 columns, including the binary target variable “Exonerated”. The final dataset is now fully numeric, clean, and well-structured for training supervised learning models.\n\n# Perform one-hot encoding without dropping any category\nencoder = OneHotEncoder(sparse_output=False)  # Use sparse_output instead of sparse\nencoded_features = encoder.fit_transform(data[['Race', 'County']])\n\n# Create a DataFrame for the encoded features\nencoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(['Race', 'County']))\n\n# Concatenate encoded features with the original dataset\ndata = pd.concat([data.drop(columns=['Race', 'County']), encoded_df], axis=1)\n\n# Preview the data\ndata.head()\n\n\n\n\n\n\n\n\n\nExonerated\nRace_Black\nRace_Latino\nRace_Other\nRace_White\nRace_nan\nCounty_Adams\nCounty_Bond\nCounty_Boone\nCounty_Brown\n...\nCounty_Sangamon\nCounty_St. Clair\nCounty_Stephenson\nCounty_Tazewell\nCounty_Vermilion\nCounty_Washington\nCounty_Will\nCounty_Williamson\nCounty_Winnebago\nCounty_Woodford\n\n\n\n\n0\n1\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n1\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n1\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n1\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 63 columns"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#logistic-regression",
    "href": "technical-details/supervised-learning/main.html#logistic-regression",
    "title": "Supervised Learning",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic Regression was selected as the baseline model due to its simplicity, efficiency, and strong interpretability. As a linear model, it is particularly well-suited for binary classification tasks, such as predicting whether an individual is exonerated (1) or not (0). Logistic Regression estimates the probability of class membership, offering clear insights into the relationship between the input features and the target variable.\nA key advantage of Logistic Regression is its assumption of a linear relationship between the independent variables and the log-odds of the outcome. This assumption works effectively when the dataset is well-preprocessed and balanced, as is the case here.\nBy serving as the benchmark model, Logistic Regression provides a baseline for performance comparison. This allows for a clear evaluation of whether more complex models, such as Random Forest or KNN, can achieve improved predictive accuracy in exchange for additional computational complexity.\n\nModel Performance Summary\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\nimport matplotlib.pyplot as plt\n\n# Logistic Regression Model\nlog_model = LogisticRegression(random_state=42)\nlog_model.fit(X_train, y_train)\n\n# Predictions and Probabilities\nlog_y_pred = log_model.predict(X_test)\nlog_y_pred_proba = log_model.predict_proba(X_test)[:, 1]\n\n# Evaluation Metrics\nlog_accuracy = accuracy_score(y_test, log_y_pred)\nlog_precision = precision_score(y_test, log_y_pred)\nlog_recall = recall_score(y_test, log_y_pred)\nlog_f1 = f1_score(y_test, log_y_pred)\nlog_roc_auc = roc_auc_score(y_test, log_y_pred_proba)\n\nprint(f\"Logistic Regression - Accuracy: {log_accuracy:.2f}, Precision: {log_precision:.2f}, Recall: {log_recall:.2f}, F1 Score: {log_f1:.2f}, ROC-AUC: {log_roc_auc:.2f}\")\n\nLogistic Regression - Accuracy: 0.89, Precision: 0.90, Recall: 0.88, F1 Score: 0.89, ROC-AUC: 0.95\n\n\nThe Logistic Regression model delivered strong performance across all evaluation metrics. An accuracy of 0.89 indicates that the model correctly predicted exoneration outcomes 89% of the time. The precision score of 0.90 shows that 90% of the individuals predicted as exonerated were truly exonerated, highlighting the model’s ability to minimize false positives. Similarly, the recall of 0.88 demonstrates that the model successfully identified 88% of all actual exonerated cases, proving its effectiveness at capturing true positives.\nThe F1 Score, which balances precision and recall, came in at 0.89, confirming strong overall classification performance. Finally, the ROC-AUC score of 0.95 reflects the model’s exceptional ability to distinguish between exonerated and non-exonerated individuals across different probability thresholds. A score this high indicates that the model is highly effective at separating the two classes with minimal overlap in predictions.\nThese results underscore that Logistic Regression serves as a reliable and high-performing baseline model for predicting exoneration outcomes.\n\n\nROC Curve\n\n# ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, log_y_pred_proba)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f\"Logistic Regression ROC Curve (AUC = {log_roc_auc:.2f})\")\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Logistic Regression ROC Curve\")\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\n\n\n\n\nThe ROC Curve’s steep rise toward the top-left corner indicates strong performance in distinguishing between exonerated and non-exonerated individuals. The AUC (Area Under the Curve) value of 0.95 confirms the model’s exceptional discriminatory power. While a perfect model achieves an AUC of 1.0 and a random classifier yields an AUC of 0.5 (represented by the dashed diagonal line), the high AUC score here demonstrates that the Logistic Regression model is highly effective at separating the two classes with minimal false classifications. This ROC curve, combined with the corresponding AUC score, highlights the model’s efficient performance and reliability in predicting exoneration outcomes.\n\n\nConfusion Matrix\n\n## Confusion Matrix\nConfusionMatrixDisplay.from_estimator(log_model, X_test, y_test)\nplt.title(\"Confusion Matrix - Logistic Regression\")\nplt.show()\n\n\n\n\n\n\n\n\nThe Confusion Matrix for the Logistic Regression model provides a clear breakdown of predictions against actual outcomes, highlighting the following results:\n\nTrue Negatives (Top-Left): 96 cases were correctly classified as non-exonerated.\n\nFalse Positives (Top-Right): 11 cases were incorrectly predicted as exonerated.\n\nFalse Negatives (Bottom-Left): 14 cases were incorrectly classified as non-exonerated.\n\nTrue Positives (Bottom-Right): 99 cases were correctly identified as exonerated.\n\nThe model demonstrates strong predictive accuracy, with a high count of true positives and true negatives. While the false positive and false negative rates are relatively low, they still pinpoint areas that could benefit from further refinement. Specifically, the 14 false negatives reflect cases where the model failed to identify individuals who were exonerated, a critical consideration for future adjustments. On the other hand, the 11 false positives represent instances where exoneration was incorrectly predicted, signaling a slight overreach in classification.\n\n\nCalibration Curve\n\n## Calibration Curve\nprob_true, prob_pred = calibration_curve(y_test, log_y_pred_proba, n_bins=10)\nplt.plot(prob_pred, prob_true, marker='o', label=\"Logistic Regression\")\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray', label=\"Perfect Calibration\")\nplt.xlabel(\"Mean Predicted Probability\")\nplt.ylabel(\"Fraction of Positives\")\nplt.title(\"Calibration Curve - Logistic Regression\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe Calibration Curve reveals that the model’s predictions are generally well-calibrated but show slight deviations, particularly at the extremes:\n\nAt lower probabilities (0.0 to 0.2), the model underestimates the actual fraction of positives, indicating a conservative bias in this range.\n\nFor mid-range probabilities (0.4 to 0.6), the predictions align closely with the observed outcomes, reflecting strong calibration in this critical range.\n\nAt higher probabilities (0.8 to 1.0), the model slightly overestimates, as the fraction of positives reaches 1.0 earlier than the ideal calibration line.\n\nThese results suggest that while the model performs reliably across the mid-to-high probability ranges, minor misalignments occur at the extremes. These deviations highlight areas where additional tuning or recalibration could further improve probability estimates. Despite this, the overall calibration remains strong, confirming the model’s effectiveness in producing meaningful and interpretable probability scores for predicting exoneration outcomes."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#naive-bayes",
    "href": "technical-details/supervised-learning/main.html#naive-bayes",
    "title": "Supervised Learning",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nModel Performance Summary\n\nfrom sklearn.naive_bayes import GaussianNB\n\n# Naive Bayes Model\nnb_model = GaussianNB()\nnb_model.fit(X_train, y_train)\n\n# Predictions and Probabilities\nnb_y_pred = nb_model.predict(X_test)\nnb_y_pred_proba = nb_model.predict_proba(X_test)[:, 1]\n\n# Evaluation Metrics\nnb_accuracy = accuracy_score(y_test, nb_y_pred)\nnb_precision = precision_score(y_test, nb_y_pred)\nnb_recall = recall_score(y_test, nb_y_pred)\nnb_f1 = f1_score(y_test, nb_y_pred)\nnb_roc_auc = roc_auc_score(y_test, nb_y_pred_proba)\n\nprint(f\"Naive Bayes - Accuracy: {nb_accuracy:.2f}, Precision: {nb_precision:.2f}, Recall: {nb_recall:.2f}, F1 Score: {nb_f1:.2f}, ROC-AUC: {nb_roc_auc:.2f}\")\n\nNaive Bayes - Accuracy: 0.83, Precision: 0.75, Recall: 0.99, F1 Score: 0.85, ROC-AUC: 0.86\n\n\nThe Naive Bayes model achieved an accuracy of 0.83, meaning it correctly predicted 83% of the outcomes. A key highlight is the recall score of 0.99, demonstrating the model’s exceptional ability to identify actual positives (exonerated individuals) while minimizing false negatives. This makes Naive Bayes particularly advantageous in scenarios where capturing all exoneration cases is critical. However, the precision score of 0.75 reveals that 25% of the predicted positives were incorrect, pointing to a higher rate of false positives. The F1 Score, which strikes a balance between precision and recall, is 0.85, indicating solid overall performance despite the trade-off in precision. Additionally, the ROC-AUC score of 0.86 confirms that the model effectively differentiates between exonerated and non-exonerated individuals, though it falls slightly short of the performance achieved by Logistic Regression. These results highlight that Naive Bayes prioritizes recall—ensuring most true positives are captured—at the expense of precision. This trade-off makes the model particularly useful in applications where minimizing missed exonerations outweighs the cost of false positives.\n\n\nROC Curve\n\n# ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, nb_y_pred_proba)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f\"Naive Bayes ROC Curve (AUC = {nb_roc_auc:.2f})\")\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Naive Bayes ROC Curve\")\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\n\n\n\n\nThe ROC Curve demonstrates that the Naive Bayes model effectively distinguishes between exonerated and non-exonerated individuals, rising steeply toward the top-left corner. The AUC (Area Under the Curve) value of 0.86 confirms strong overall performance, reflecting the model’s ability to separate the two classes. That said, the slightly lower AUC compared to Logistic Regression suggests that Naive Bayes is less precise when ranking positive predictions. This aligns with its earlier results, where high recall (capturing nearly all true positives) came at the cost of lower precision. The shape of the curve further reveals that while the model achieves excellent sensitivity, it is more prone to false positives at certain thresholds. Ultimately, the Naive Bayes ROC curve highlights the model’s strength in identifying exonerated individuals, reinforcing its reliability for scenarios where capturing true positives is prioritized over reducing false positives.\n\n\nConfusion Matrix\n\n## Confusion Matrix\nConfusionMatrixDisplay.from_estimator(nb_model, X_test, y_test)\nplt.title(\"Confusion Matrix - Logistic Regression\")\nplt.show()\n\n\n\n\n\n\n\n\nThe Confusion Matrix for the Logistic Regression model provides a detailed breakdown of its performance in predicting exoneration outcomes. The matrix shows the following:\n\nTrue Negatives (Top-Left): 70 instances were correctly classified as non-exonerated.\n\nFalse Positives (Top-Right): 37 instances were incorrectly predicted as exonerated.\n\nFalse Negatives (Bottom-Left): Only 1 instance was incorrectly classified as non-exonerated.\n\nTrue Positives (Bottom-Right): 112 instances were correctly classified as exonerated.\n\nThe matrix reveals that the model is highly effective at identifying true positives (exonerated individuals), with only 1 false negative, which aligns with its strong recall score. However, the relatively high number of false positives (37) suggests that the model tends to overpredict exoneration, which slightly impacts precision.\nOverall, while the Logistic Regression model demonstrates strong performance in identifying exonerated individuals, the false positives indicate room for improvement in reducing misclassifications of non-exonerated cases.\n\n\nCalibration Curve\n\n## Calibration Curve\nprob_true, prob_pred = calibration_curve(y_test, nb_y_pred_proba, n_bins=10)\nplt.plot(prob_pred, prob_true, marker='o', label=\"Logistic Regression\")\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray', label=\"Perfect Calibration\")\nplt.xlabel(\"Mean Predicted Probability\")\nplt.ylabel(\"Fraction of Positives\")\nplt.title(\"Calibration Curve - Naive Bayes\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe Calibration Curve reveals a clear pattern of underconfidence, where the predicted probabilities are consistently lower than the actual fraction of positives. Even at higher predicted probabilities (close to 1.0), the model underestimates the true proportion of positive outcomes, failing to align with the perfect calibration line. This behavior is a known limitation of Naive Bayes models, stemming from their strong assumption of feature independence. While this assumption enables efficient performance and strong recall, it often leads to poorly calibrated probability estimates, as seen here where the model identifies positive cases effectively but struggles to assign probabilities that accurately reflect confidence in predictions."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#random-forest",
    "href": "technical-details/supervised-learning/main.html#random-forest",
    "title": "Supervised Learning",
    "section": "Random Forest",
    "text": "Random Forest\n\nModel Performance Summary\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Random Forest Model\nrf_model = RandomForestClassifier(random_state=42)\nrf_model.fit(X_train, y_train)\n\n# Predictions and Probabilities\nrf_y_pred = rf_model.predict(X_test)\nrf_y_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n\n# Evaluation Metrics\nrf_accuracy = accuracy_score(y_test, rf_y_pred)\nrf_precision = precision_score(y_test, rf_y_pred)\nrf_recall = recall_score(y_test, rf_y_pred)\nrf_f1 = f1_score(y_test, rf_y_pred)\nrf_roc_auc = roc_auc_score(y_test, rf_y_pred_proba)\n\nprint(f\"Random Forest - Accuracy: {rf_accuracy:.2f}, Precision: {rf_precision:.2f}, Recall: {rf_recall:.2f}, F1 Score: {rf_f1:.2f}, ROC-AUC: {rf_roc_auc:.2f}\")\n\n\nRandom Forest - Accuracy: 0.89, Precision: 0.89, Recall: 0.89, F1 Score: 0.89, ROC-AUC: 0.95\n\n\nThe Random Forest model delivers outstanding performance across all evaluation metrics, achieving an accuracy of 0.89, meaning 89% of the predictions were correct. Both the precision and recall scores stand at 0.89, reflecting the model’s strong ability to balance minimizing false positives while correctly identifying true positives. This balance underscores its effectiveness in accurately predicting exoneration outcomes. The F1 Score, also 0.89, reinforces the model’s consistency by combining precision and recall into a single, reliable measure. Additionally, the ROC-AUC score of 0.95 demonstrates the model’s exceptional ability to differentiate between exonerated and non-exonerated individuals. This performance aligns closely with that of Logistic Regression, further validating the model’s robustness. Altogether, the Random Forest model provides a well-rounded and reliable classification solution, balancing precision, recall, and accuracy effectively. Its strong performance across all key metrics makes it a highly capable candidate for predicting exoneration outcomes.\n\n\nROC Curve\n\n\n# ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, rf_y_pred_proba)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f\"Random Forest ROC Curve (AUC = {rf_roc_auc:.2f})\")\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Random Forest ROC Curve\")\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\n\n\n\n\nThe visual representation of the ROC Curve reinforces the model’s AUC (Area Under the Curve) value of 0.95, showcasing its excellent discriminatory power. The model consistently achieves a high True Positive Rate while maintaining a low False Positive Rate. This aligns closely with previously reported metrics—accuracy, precision, and recall—further validating the model’s reliability. The curve’s steep rise and near-flat progression highlight Random Forest’s ability to deliver both high sensitivity and specificity, effectively identifying positive cases while minimizing false positives. This strong performance positions Random Forest as a robust and dependable model for predicting exoneration outcomes.\n\n\nFeature Importance\n\n# Feature Importance\nfeature_importances = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False)\nfeature_importances.plot(kind='bar', figsize=(10, 6), title=\"Feature Importance\")\nplt.show()\n\n\n\n\n\n\n\n\nThe Feature Importance plot for the Random Forest model provides insight into which features had the greatest influence on the model’s predictions. Random Forest calculates feature importance by evaluating how much each feature reduces impurity (e.g., Gini index) across all decision trees within the ensemble. This approach makes Random Forest particularly effective for understanding the relative impact of individual predictors. The plot shows that “County_Cook” stands out as the most influential feature by a significant margin, followed closely by race-related variables such as “Race_Black” and “Race_White”. The remaining features, including other counties, contribute far less to the model’s decision-making process, displaying diminishing importance.\nFeature importance was calculated exclusively for the Random Forest model, as its tree-based structure is inherently designed to measure and interpret feature contributions. By contrast, models like Naive Bayes do not provide native feature importance measures due to their reliance on probabilistic assumptions rather than iterative splits within the feature space. The feature importance plot underscores the dominant role of geographic location (e.g., “County_Cook”) and race in predicting exoneration outcomes, offering a clearer understanding of the factors driving the model’s predictions and highlight key areas for further investigation.\n\n\nConfusion Matrix\n\n## Confusion Matrix\nConfusionMatrixDisplay.from_estimator(rf_model, X_test, y_test)\nplt.title(\"Confusion Matrix - Logistic Regression\")\nplt.show()\n\n\n\n\n\n\n\n\nThe Confusion Matrix for the Random Forest model provides a clear evaluation of its classification performance:\n\nTrue Negatives (Top-Left): 95 cases were correctly classified as non-exonerated.\n\nFalse Positives (Top-Right): 12 cases were incorrectly predicted as exonerated.\n\nFalse Negatives (Bottom-Left): 12 cases were incorrectly classified as non-exonerated.\n\nTrue Positives (Bottom-Right): 101 cases were correctly identified as exonerated.\n\nThe results demonstrate that the Random Forest model effectively balances true positives and true negatives, with a relatively low number of misclassifications. That said, the presence of 12 false positives and 12 false negatives highlights areas where the model could benefit from further refinement. These findings align closely with the model’s reported metrics, particularly its high accuracy and recall, reinforcing the model’s overall reliability in predicting exoneration outcomes.\n\n\nCalibration Curve\n\n## Calibration Curve\nprob_true, prob_pred = calibration_curve(y_test, rf_y_pred_proba, n_bins=10)\nplt.plot(prob_pred, prob_true, marker='o', label=\"Logistic Regression\")\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray', label=\"Perfect Calibration\")\nplt.xlabel(\"Mean Predicted Probability\")\nplt.ylabel(\"Fraction of Positives\")\nplt.title(\"Calibration Curve - Logistic Regression\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe Calibration Curve for the Random Forest model reveals slight deviations from perfect calibration, particularly across the lower and mid-range probability values. The curve shows that the model tends to overestimate probabilities in certain bins (e.g., 0.4–0.6) while underestimating in others, creating noticeable oscillations. At higher probability ranges (close to 1.0), the model performs more reliably, aligning closely with the perfect calibration line. This indicates that the model’s confidence in high-probability predictions is well-placed, even if inconsistencies appear at lower thresholds. While the Random Forest model delivers strong classification performance overall, its probability estimates could benefit from additional calibration techniques to improve consistency and reliability across all probability ranges."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#k-nearest-neighbors-knn",
    "href": "technical-details/supervised-learning/main.html#k-nearest-neighbors-knn",
    "title": "Supervised Learning",
    "section": "K-Nearest Neighbors (KNN)",
    "text": "K-Nearest Neighbors (KNN)\n\nOptimal K\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Define the range of 'n_neighbors' to test\nparam_grid = {'n_neighbors': range(1, 31)}  # Try k from 1 to 30\n\n# Initialize the KNN model\nknn = KNeighborsClassifier()\n\n# Use GridSearchCV to find the optimal n_neighbors\ngrid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')  # 5-fold cross-validation\ngrid_search.fit(X_train, y_train)\n\n# Get the best n_neighbors\nbest_n = grid_search.best_params_['n_neighbors']\nbest_score = grid_search.best_score_\n\nprint(f\"Optimal n_neighbors: {best_n}, Cross-Validation Accuracy: {best_score:.2f}\")\n\nOptimal n_neighbors: 6, Cross-Validation Accuracy: 0.88\n\n\nTo optimize the performance of the K-Nearest Neighbors (KNN) model, the optimal value of ‘K’ (number of neighbors) was determined using GridSearchCV with 5-fold cross-validation. A range of K values from 1 to 30 was tested to identify the parameter that maximizes model accuracy. The search identified an optimal K value of 6, achieving a cross-validation accuracy of 0.88. Choosing the appropriate ‘K’ is crucial for managing the trade-off between bias and variance: smaller K values can lead to overfitting, as the model becomes overly sensitive to noise, while larger values risk oversmoothing decision boundaries, causing the model to miss important patterns in the data. With the optimal K=6, the fine-tuned KNN model was finalized and evaluated on the test dataset, ensuring it strikes a balance between capturing patterns effectively and maintaining generalization.\n\n\nModel Performance Summary\n\n# KNN Model\nknn_model = KNeighborsClassifier(n_neighbors=best_n)  # Adjust n_neighbors as needed\nknn_model.fit(X_train, y_train)\n\n# Predictions and Probabilities\nknn_y_pred = knn_model.predict(X_test)\nknn_y_pred_proba = knn_model.predict_proba(X_test)[:, 1]\n\n# Evaluation Metrics\nknn_accuracy = accuracy_score(y_test, knn_y_pred)\nknn_precision = precision_score(y_test, knn_y_pred)\nknn_recall = recall_score(y_test, knn_y_pred)\nknn_f1 = f1_score(y_test, knn_y_pred)\nknn_roc_auc = roc_auc_score(y_test, knn_y_pred_proba)\n\nprint(f\"KNN - Accuracy: {knn_accuracy:.2f}, Precision: {knn_precision:.2f}, Recall: {knn_recall:.2f}, F1 Score: {knn_f1:.2f}, ROC-AUC: {knn_roc_auc:.2f}\")\n\nKNN - Accuracy: 0.92, Precision: 0.91, Recall: 0.95, F1 Score: 0.93, ROC-AUC: 0.94\n\n\nThe KNN model demonstrated excellent performance across all evaluation metrics:\n\nAccuracy: 0.92 — The model correctly predicted 92% of the test set outcomes.\n\nPrecision: 0.91 — 91% of predicted exonerated cases were correct.\n\nRecall: 0.95 — The model successfully identified 95% of actual exonerated individuals, minimizing false negatives.\n\nF1 Score: 0.93 — The harmonic mean of precision and recall reflects a strong balance between the two metrics.\n\nROC-AUC: 0.94 — The model effectively distinguishes between exonerated and non-exonerated classes, with high discriminatory power.\n\nThese results indicate that the KNN model, with an optimal k value of 6, provides a reliable classification of exoneration outcomes, outperforming several other models in overall accuracy and recall.\n\n\nROC Curve\n\n# ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, knn_y_pred_proba)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f\"KNN ROC Curve (AUC = {knn_roc_auc:.2f})\")\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"KNN ROC Curve\")\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\n\n\n\n\nThe ROC Curve rises sharply toward the top-left corner, indicating that the model effectively identifies exonerated cases while keeping false positives to a minimum. The AUC (Area Under the Curve) value of 0.94 further confirms the model’s strong discriminatory power. A high AUC score like this demonstrates that the KNN model reliably separates exonerated and non-exonerated individuals, even as the classification threshold varies. The curve’s steep ascent highlights the model’s ability to achieve high sensitivity early, capturing the majority of true positives while maintaining a low false positive rate. This strong performance underscores the KNN model’s effectiveness as a classifier, particularly with the optimized k = 6.\n\n\nConfusion Matrix\n\n# Confusion Matrix\nConfusionMatrixDisplay.from_estimator(knn_model, X_test, y_test)\nplt.title(\"Confusion Matrix - KNN\")\nplt.show()\n\n\n\n\n\n\n\n\nThe Confusion Matrix for the K-Nearest Neighbors (KNN) model provides a clear breakdown of its performance on the test set:\n\nTrue Negatives (Top-Left): 96 cases were correctly classified as non-exonerated.\n\nFalse Positives (Top-Right): 11 cases were incorrectly predicted as exonerated.\n\nFalse Negatives (Bottom-Left): 6 cases were incorrectly classified as non-exonerated.\n\nTrue Positives (Bottom-Right): 107 cases were correctly identified as exonerated.\n\nThe KNN model delivers strong classification performance, correctly identifying the majority of instances in both classes. The false positive rate remains low, with only 11 misclassifications, while the false negative rate is minimal at just 6 cases. This balance aligns with the model’s high recall (0.95) and precision (0.91), highlighting its effectiveness in capturing true positives while keeping errors to a minimum. The confusion matrix results reinforce the KNN model’s reliability, particularly its strength in accurately identifying exonerated individuals.\n\n\nCalibration Curve\n\n# Calibration Curve\nprob_true, prob_pred = calibration_curve(y_test, knn_y_pred_proba, n_bins=10)\nplt.plot(prob_pred, prob_true, marker='o', label=\"KNN\")\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray', label=\"Perfect Calibration\")\nplt.xlabel(\"Mean Predicted Probability\")\nplt.ylabel(\"Fraction of Positives\")\nplt.title(\"Calibration Curve - KNN\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe Calibration Curve for the K-Nearest Neighbors (KNN) model highlights varying degrees of calibration quality across different probability ranges:\n\nFor lower predicted probabilities (0.0–0.4), the model tends to underestimate the fraction of positives, indicating a conservative bias in these ranges.\n\nAt mid-range probabilities (0.6–0.8), the predictions align closely with the perfect calibration line, suggesting reliable probability estimates in this range.\n\nFor higher predicted probabilities (0.8–1.0), the model slightly overestimates the likelihood of positive outcomes, as seen in the upward deviation from the ideal line.\n\nWhile the KNN model demonstrates strong performance in predicting exoneration outcomes, the calibration curve indicates that probability estimates are less consistent at the extremes. Applying additional calibration techniques could improve reliability and provide more accurate confidence estimates across all probability ranges."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-comparison",
    "href": "technical-details/supervised-learning/main.html#model-comparison",
    "title": "Supervised Learning",
    "section": "Model Comparison",
    "text": "Model Comparison\n\n# Define metrics for each model\ncomparison_data = {\n    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC-AUC\"],\n    \"Logistic Regression\": [log_accuracy, log_precision, log_recall, log_f1, log_roc_auc],\n    \"Naive Bayes\": [nb_accuracy, nb_precision, nb_recall, nb_f1, nb_roc_auc ],\n    \"Random Forest\": [rf_accuracy, rf_precision, rf_recall, rf_f1, rf_roc_auc],\n    \"K-Nearest Neighbors\": [knn_accuracy, knn_precision, knn_recall, knn_f1, knn_roc_auc]\n}\n\n# Create the DataFrame\ncomparison_df = pd.DataFrame(comparison_data)\n# Style the DataFrame for better readability\ncomparison_df.style.set_caption(\"Model Comparison\").highlight_max(axis=1, color=\"lightgreen\")\n\n# Display the table\ncomparison_df.head()\n\n\n\n\n\n\n\n\nMetric\nLogistic Regression\nNaive Bayes\nRandom Forest\nK-Nearest Neighbors\n\n\n\n\n0\nAccuracy\n0.895455\n0.831818\n0.909091\n0.909091\n\n\n1\nPrecision\n0.901786\n0.756757\n0.904348\n0.872000\n\n\n2\nRecall\n0.893805\n0.991150\n0.920354\n0.964602\n\n\n3\nF1 Score\n0.897778\n0.858238\n0.912281\n0.915966\n\n\n4\nROC-AUC\n0.952651\n0.864403\n0.954553\n0.907535\n\n\n\n\n\n\n\nTo identify the best-performing model for predicting exoneration outcomes, four supervised learning algorithms — Logistic Regression, Naive Bayes, Random Forest, and K-Nearest Neighbors (KNN) — were evaluated across key performance metrics, including Accuracy, Precision, Recall, F1 Score, and ROC-AUC.\n\nLogistic Regression delivered strong results with an accuracy of 0.89 and an ROC-AUC of 0.95, showcasing excellent discriminatory power and reliable overall performance.\n\nNaive Bayes excelled in recall, achieving a score of 0.99 by capturing nearly all true positives. However, its lower precision (0.76) resulted in a higher false positive rate, reflecting a trade-off between sensitivity and precision.\n\nK-Nearest Neighbors (KNN) performed exceptionally well, with an accuracy of 0.91, recall of 0.96, and an ROC-AUC of 0.91, solidifying its reliability as a strong classifier despite a slight trade-off in precision.\n\nRandom Forest emerged as the top-performing model, achieving the highest overall accuracy (0.91), precision (0.94), and F1 Score (0.91), alongside a robust ROC-AUC of 0.95.\n\nBased on these results, Random Forest stands out as the most balanced and effective model, delivering superior performance across all critical evaluation metrics.\n\nWhy Random Forest?\nRandom Forest was chosen as the final model due to its superior performance across all evaluation metrics. It delivered the highest accuracy and precision while maintaining strong recall, striking a balance between minimizing false positives and capturing true positives. The ROC-AUC score of 0.95 highlights its exceptional ability to distinguish between exonerated and non-exonerated individuals. Additionally, Random Forest’s feature importance analysis provided valuable interpretability, identifying key predictors such as County_Cook and Race_Black. This combination of high performance, reliability, and interpretability makes Random Forest the most suitable model for predicting exoneration outcomes.\nRandom Forest’s outperformance can be attributed to its unique strengths as an ensemble method, which combines predictions from multiple decision trees to reduce overfitting and deliver consistent results across complex datasets. Specifically:\n\nHandling Non-Linearity: Unlike Logistic Regression, Random Forest does not rely on linear assumptions between features and the target variable, enabling it to capture more complex interactions within the data.\nFeature Importance: By leveraging all input features, Random Forest identifies the most influential predictors, improving both accuracy and interpretability, as demonstrated in the feature importance plot.\nRobustness to Noise and Outliers: Tree-based methods like Random Forest are inherently less sensitive to noisy data and outliers, which can significantly impact simpler models such as KNN or Naive Bayes.\nBalanced Bias-Variance Tradeoff: Averaging predictions across multiple trees allows Random Forest to reduce variance (overfitting) while maintaining low bias, ensuring strong generalization on unseen data.\nHigh Recall Without Sacrificing Precision: While Naive Bayes prioritized recall at the expense of precision, Random Forest effectively balances the two, capturing true positives while minimizing false positives.\n\nThese strengths make Random Forest particularly well-suited for this dataset, where geographic and demographic factors interact in nuanced ways. Its ability to handle complex relationships, combined with its superior predictive power and interpretability, explains why it emerged as the best-performing model."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#average-predicted-probability-of-wrongful-conviction-in-illinois-by-race",
    "href": "technical-details/supervised-learning/main.html#average-predicted-probability-of-wrongful-conviction-in-illinois-by-race",
    "title": "Supervised Learning",
    "section": "Average Predicted Probability of Wrongful Conviction in Illinois by Race",
    "text": "Average Predicted Probability of Wrongful Conviction in Illinois by Race\n\n# Predict probabilities for the test set\nprobabilities = rf_model.predict_proba(X_test)\n\n# The second column contains the probabilities for the positive class (Exonerated)\nexoneration_probabilities = probabilities[:, 1]\n\n# Preview the first few probabilities\nprint(exoneration_probabilities[:10])\n\n[1.         0.         0.86253375 0.         1.         0.86253375\n 0.         0.7507617  0.86253375 0.        ]\n\n\n\n# Ensure X_test retains the one-hot encoded race columns\nX_test_with_probabilities = X_test.copy()  # Create a copy of X_test\nX_test_with_probabilities['exoneration_probability'] = exoneration_probabilities  # Add predicted probabilities\n\n# Decode one-hot encoded race into a single column\nrace_columns = ['Race_Black', 'Race_Latino', 'Race_Other', 'Race_White']\nX_test_with_probabilities['Race'] = X_test_with_probabilities[race_columns].idxmax(axis=1).str.replace('Race_', '')\n\n# Group by race and calculate average probabilities\nrace_probabilities = X_test_with_probabilities.groupby('Race')['exoneration_probability'].mean()\n\nfig, ax = plt.subplots(figsize=(10, 6))\ncolors = plt.cm.coolwarm(np.linspace(0, 1, len(race_probabilities)))\n\nbars = race_probabilities.sort_values().plot(kind='bar', color=colors, ax=ax)\nax.set_title('Average Predicted Probability of Wrongful Conviction by Race')\nax.set_xlabel('Race')\nax.set_ylabel('Average Predicted Probability')\nax.grid(axis='y', linestyle='--', alpha=0.7)  # Add horizontal grid lines for clarity\n\n# Display the plot\nplt.show()\n\n# Display the table of average probabilities\nrace_probabilities_df = race_probabilities.reset_index()\nrace_probabilities_df.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRace\nexoneration_probability\n\n\n\n\n0\nBlack\n0.630754\n\n\n1\nLatino\n0.018190\n\n\n2\nOther\n0.030000\n\n\n3\nWhite\n0.178803\n\n\n\n\n\n\n\nThe visualization reveals a stark disparity in the average predicted probability of wrongful conviction among incarcerated individuals in Illinois when analyzed by race. The model predicts that Black individuals face the highest average probability of being wrongfully convicted, at approximately 63%. In contrast, White individuals show a significantly lower predicted probability of 17.8%, while those categorized as “Other” and Latino have probabilities of 3% and 1.8%, respectively. These results reflect patterns the model identified within the dataset, which captures the exoneration outcomes of Illinois’ incarcerated population. The striking gap in predicted probabilities underscores the disproportionately higher likelihood of wrongful conviction for Black individuals—a concerning indication of racial inequities within the justice system. Meanwhile, the considerably lower probabilities for White, Latino, and Other groups further highlight the extent of this imbalance."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#average-predicted-probability-of-wrongful-conviction-in-illinois-by-county",
    "href": "technical-details/supervised-learning/main.html#average-predicted-probability-of-wrongful-conviction-in-illinois-by-county",
    "title": "Supervised Learning",
    "section": "Average Predicted Probability of Wrongful Conviction in Illinois by County",
    "text": "Average Predicted Probability of Wrongful Conviction in Illinois by County\n\n# Decode one-hot encoded county into a single column\ncounty_columns = [col for col in X_test.columns if col.startswith('County_')]\nfor col in county_columns:\n    county_name = col.replace('County_', '')\n    X_test_with_probabilities.loc[X_test_with_probabilities[col] == 1, 'County'] = county_name\n\n# Group by county and calculate average probabilities\ncounty_probabilities = X_test_with_probabilities.groupby('County')['exoneration_probability'].mean()\n\n# Plot the average probabilities for each county using the coolwarm colormap\nfig, ax = plt.subplots(figsize=(15, 8))\ncoolwarm_colors = plt.cm.coolwarm(np.linspace(0, 1, len(county_probabilities)))\n\nbars = county_probabilities.sort_values().plot(\n    kind='bar',\n    color=coolwarm_colors,\n    ax=ax\n)\nax.set_title('Average Predicted Probability of Wrongful Conviction by County')\nax.set_xlabel('County')\nax.set_ylabel('Average Predicted Probability')\nax.grid(axis='y', linestyle='--', alpha=0.7) \nplt.xticks(rotation=45, ha='right')\n\n# Display the plot\nplt.show()\n\n# Display the table of average probabilities\ncounty_probabilities_df = county_probabilities.reset_index()\ncounty_probabilities_dict = county_probabilities_df.set_index('County')['exoneration_probability'].to_dict()\n\n# Print each key-value pair\nfor county, probability in county_probabilities_dict.items():\n    print(f\"{county}: {probability:.4f}\")\n\n\n\n\n\n\n\n\nBond: 0.0075\nBrown: 0.0000\nChampaign: 0.4724\nChristian: 0.0406\nClinton: 0.0045\nCook: 0.8537\nCumberland: 0.0532\nDuPage: 0.5765\nFayette: 0.0000\nFulton: 0.0000\nHenry: 0.0244\nJefferson: 0.0205\nJohnson: 0.0000\nKankakee: 0.4693\nKnox: 0.0056\nLa Salle: 0.0090\nLawrence: 0.0519\nLee: 0.0071\nLivingston: 0.1500\nLogan: 0.0000\nMacon: 0.3503\nMcHenry: 0.4854\nMontgomery: 0.0000\nMorgan: 0.0037\nMoultrie: 0.0532\nPeoria: 0.2247\nPerry: 0.0000\nRandolph: 0.0000\nRock Island: 0.0000\nSt. Clair: 0.1383\nStephenson: 0.0532\nTazewell: 0.0000\nVermilion: 0.1350\nWill: 0.1541\nWilliamson: 0.2095\nWinnebago: 0.2571\nWoodford: 0.0856\n\n\nCook County emerges with the highest average predicted probability, exceeding 80%. This result is particularly significant, as Cook County, which includes Chicago, represents the largest urban population in Illinois. The data suggests that wrongful convictions are disproportionately concentrated in over-policed, densely populated minority communities (as demonstrated by the EDA). Beyond Cook County, counties such as DuPage, McHenry, and Champaign also exhibit elevated predicted probabilities, reflecting a broader pattern of wrongful convictions in areas with similar over-policing dynamics. The overall findings underscore substantial geographic disparities in the predicted probability of wrongful convictions across Illinois. The sharp divide between counties like Cook and those with near-zero probabilities highlights systemic issues tied to over-policing, population density, and judicial or prosecutorial behavior in minority communities."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#average-predicted-probability-of-wrongful-conviction-in-illinois-by-race-and-ounty",
    "href": "technical-details/supervised-learning/main.html#average-predicted-probability-of-wrongful-conviction-in-illinois-by-race-and-ounty",
    "title": "Supervised Learning",
    "section": "Average Predicted Probability of Wrongful Conviction in Illinois by Race and ounty",
    "text": "Average Predicted Probability of Wrongful Conviction in Illinois by Race and ounty\n\n# Group by race and county, then calculate average probabilities\nrace_county_probabilities = X_test_with_probabilities.groupby(['Race', 'County'])['exoneration_probability'].mean().unstack()\n\n# Plot a heatmap for race vs. county\nimport seaborn as sns\nplt.figure(figsize=(18, 8))  # Increase width for better visibility\n\n# Create the heatmap\nsns.heatmap(\n    race_county_probabilities,  # Assuming this is your DataFrame\n    annot=True,  # Display numerical values\n    fmt=\".2f\",  # Format the numbers to 2 decimal places\n    cmap=\"coolwarm\",  # Use a diverging colormap\n    cbar=True,  # Display color bar\n    linewidths=0.5  # Add spacing between cells\n)\n\n# Customize titles and labels\nplt.title(\"Average Predicted Probability of Wrongful Conviction by Race and County\", fontsize=16)\nplt.xlabel(\"County\", fontsize=14)\nplt.ylabel(\"Race\", fontsize=14)\n\n# Rotate x-axis ticks for better readability\nplt.xticks(rotation=30, ha=\"right\", fontsize=10)\nplt.yticks(fontsize=12)  # Increase font size for y-axis labels\n\n# Display the heatmap\nplt.tight_layout()  # Ensure everything fits within the figure\nplt.show()\n\n\n\n\n\n\n\n\nThe heatmap displays the average predicted probability of wrongful conviction across Illinois counties, broken down by race. The visualization highlights stark racial and geographic disparities, with Black individuals showing the highest predicted probabilities in several counties.\n\nCook County stands out prominently, with Black individuals exhibiting a predicted probability of 0.89, the highest observed value across all groups. Similarly, counties such as Winnebago (0.77), Clinton (0.47), and Logan (0.48) report elevated probabilities for Black individuals, reinforcing patterns of wrongful convictions in over-policed, densely minority-populated areas.\nFor White individuals, the predicted probabilities are generally much lower but still notable in specific counties. Clinton County records a predicted probability of 0.75, while Macon (0.49) and DuPage (0.50) also display higher-than-average values for this group. These exceptions, however, do not alter the overall trend: White individuals consistently show lower probabilities compared to Black individuals across most counties.\nLatino and “Other” racial groups report significantly lower predicted probabilities across the state, with very few values exceeding 0.17. Notably, the highest recorded probability for Latino individuals is in DuPage County (0.17), while values for “Other” groups remain close to zero. These trends may reflect underrepresentation in the dataset rather than an absence of wrongful convictions.\nGeographically, counties such as Cook, Winnebago, Clinton, and Logan emerge as key areas driving the racial disparities observed in the heatmap. The extreme values in counties like Cook underscore its disproportionate contribution to wrongful conviction probabilities, particularly for Black individuals. Meanwhile, counties with near-zero probabilities create a sharp contrast, suggesting regional or systemic factors at play.\nOverall, the heatmap highlights the disproportionate burden of wrongful convictions on Black individuals across specific Illinois counties while showing significantly lower probabilities for White, Latino, and Other groups. These findings point to critical structural and systemic issues that require further investigation to address the racial and geographic disparities evident in the state’s criminal justice system."
  },
  {
    "objectID": "instructions/expectations.html",
    "href": "instructions/expectations.html",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare.\n\n\n\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable.\n\n\n\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields.\n\n\n\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field.\n\n\n\n\n\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important.\n\n\n\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point.\n\n\n\n\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual.\n\n\n\n\n\n\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project .\n\n\n\n\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/expectations.html#get-started-early",
    "href": "instructions/expectations.html#get-started-early",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/expectations.html#graduate-level-work",
    "href": "instructions/expectations.html#graduate-level-work",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "This is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/expectations.html#what-is-impact",
    "href": "instructions/expectations.html#what-is-impact",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Impact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/expectations.html#what-makes-a-good-research-project",
    "href": "instructions/expectations.html#what-makes-a-good-research-project",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Professional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/expectations.html#time-management",
    "href": "instructions/expectations.html#time-management",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/expectations.html#visualization-guidelines",
    "href": "instructions/expectations.html#visualization-guidelines",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Visualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/expectations.html#coding",
    "href": "instructions/expectations.html#coding",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "While not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/expectations.html#debugging",
    "href": "instructions/expectations.html#debugging",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Always remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/llm-usage.html",
    "href": "instructions/llm-usage.html",
    "title": "LLM usage",
    "section": "",
    "text": "We believe that the adoption of LLM tools is inevitable and will be an important skill for success in your future career. Therefore, appropriate and acceptable use of LLM tools is encouraged for this project. Use them to accelerate your workflow and learning, but not as a replacement for critical thinking and understanding. Carefully review and process their output, use them judiciously, and avoid bloating your text with LLM-generated content. Overusing these tools often degrades the quality of your work rather than enhancing it.\nRemember the following guidelines:\n\nUse common sense: If you feel like you’re doing something questionable, you probably are. A good test is to ask yourself, “Would I openly tell the professor or classmates what I’m doing right now?” If the answer is no, you’re probably doing something you shouldn’t.\nCite your LLM use cases: Always cite when and how you’ve used LLM tools. This is a requirement for the project.\nIs your use helping you grow professionally?: If your use of LLM tools is making you a more competent, efficient, and knowledgeable professional, you’re probably using them in an appropriate manner. If you’re using them as a shortcut to avoid work and gain free time, you’re using them incorrectly.\n\n\n\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1\n\n\n\n\n\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code.\n\n\n\n\n\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/llm-usage.html#citation",
    "href": "instructions/llm-usage.html#citation",
    "title": "LLM usage",
    "section": "",
    "text": "ALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/llm-usage.html#acceptable-use-cases",
    "href": "instructions/llm-usage.html#acceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "Note: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/llm-usage.html#unacceptable-use-cases",
    "href": "instructions/llm-usage.html#unacceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "DO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/topic-selection.html",
    "href": "instructions/topic-selection.html",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena\n\n\n\n\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation.\n\n\n\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/topic-selection.html#select-a-broad-topic-area",
    "href": "instructions/topic-selection.html#select-a-broad-topic-area",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/topic-selection.html#narrow-your-focus",
    "href": "instructions/topic-selection.html#narrow-your-focus",
    "title": "Topic selection",
    "section": "",
    "text": "Narrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/topic-selection.html#data-science-questions",
    "href": "instructions/topic-selection.html#data-science-questions",
    "title": "Topic selection",
    "section": "",
    "text": "The data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Apendix",
    "section": "",
    "text": "Appendix A: Illinois Rural/Urban Classification Map\nThe following map provides the rural and urban classification for counties in Illinois. This classification is referenced in the Geographic Analysis section for contextual understanding of urban vs rural dynamics.1\n\n\n\nIllinois Counties by Rural/Urban Classification\n\n\n\n\nAppendix B: Exonerees Balanced Dataset\nThe following dataset, exonerees_balanced.csv, includes balanced exoneration data for analysis. It can be downloaded by clicking the link below:\nDownload Exonerees Balanced Dataset\n\n\n\n\n\nReferences\n\n1. Illinois Primary Health Care Association (IPHCA). (2020). Illinois counties by rural/urban classification. https://dph.illinois.gov/content/dam/soi/en/web/idph/files/rur-urb-2021.pdf"
  },
  {
    "objectID": "technical-details/data-collection/closing.html",
    "href": "technical-details/data-collection/closing.html",
    "title": "Summary",
    "section": "",
    "text": "The foundation for meaningful data analysis is accurate and reliable crime data, yet long-standing challenges in criminal history record systems continue to undermine their precision and dependability1. As outlined in the Use and Management of Criminal History Record Information Report (1993), issues such as incomplete data reporting, delays in recording arrests and case dispositions, and inconsistent fingerprint submissions have plagued the system for decades1 . Criminal History Record Information (CHRI), the backbone for arrest datasets like those from the Illinois Criminal Justice Information Authority (ICJIA), is vulnerable to these shortcomings. Dispositions, or the outcomes of arrests, are often missing or delayed, leaving critical gaps in the data that make it difficult to analyze systemic trends. While advances in technology, such as digital fingerprinting, have improved some processes, data fragmentation and insufficient oversight remain persistent barriers to comprehensive and accurate reporting.\nFast forward thirty years, and many of these challenges remain unresolved—now further complicated by uneven implementation of modern systems. The Marshall Project highlights a striking example: in 2022, the FBI’s shift to the National Incident-Based Reporting System (NIBRS) created a significant data gap in national crime statistics2. Over 6,000 police agencies failed to submit their data, representing nearly one-third of all agencies and leaving vast portions of the U.S. population unaccounted for. This includes major departments like the NYPD and LAPD, alongside countless smaller agencies2. These gaps reflect broader systemic failures—inconsistent adoption of updated systems, lack of adequate funding, and minimal oversight—that echo the same issues identified decades ago.\nIn short, crime data, even when sourced from official systems, remains inherently flawed and incomplete. Whether due to outdated processes, inconsistent reporting practices, privacy-driven modifications, or gaps in modern collection systems, crime data often falls short of providing a fully accurate or comprehensive picture. As a result, any analysis relying on this data must account for these imperfections, recognizing that while the data can uncover critical trends and systemic disparities, it is rarely a perfect representation of reality.\n\n\nThe Illinois arrest dataset obtained from the ICJIA Arrest Explorer highlights the trade-offs between privacy and precision, adding to the broader challenges of criminal justice data. To protect confidentiality, counts under 10 are approximated—values between 0 and 4 are replaced with 1, while counts from 5 to 9 are replaced with 6. Though this approach is necessary to safeguard sensitive data, it can introduce distortions, particularly in smaller counties or demographic groups, where even slight approximations can significantly skew trends and reduce the accuracy of analysis. Compounding these issues, the lack of oversight in reporting further undermines data consistency and reliability, reflecting the broader systemic limitations that continue to plague many criminal justice datasets.\n\n\n\n\nThe National Registry of Exonerations is a trusted and widely used resource, frequently cited in academic and legal research. Unlike government databases, the Registry is run by a team of researchers and academics, which brings a level of precision and thoroughness often missing in state-managed systems3. Its foundation in meticulous documentation and independent research makes it less vulnerable to political or institutional biases. As a result, the Registry stands out as a more reliable and comprehensive tool for understanding wrongful convictions.\n\n\n\nThe data collected from various sources provide a foundation for examining systemic racial disparities in over-policing and wrongful convictions. However, challenges such as the lack of precision in Illinois arrest datasets and broader gaps in national crime reporting underscore the need to approach findings with caution. Even with these limitations, the use of reliable resources like the National Registry of Exonerations adds credibility to the analysis. Moving forward, future research could focus on compiling crime data from a single state across local, state, and federal agencies to better understand inconsistencies and uncover patterns of underreporting. Expanding this work to compare data collection practices across states could also reveal regional differences in crime reporting and highlight systemic disparities in how data is recorded and shared."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#challenges",
    "href": "technical-details/data-collection/closing.html#challenges",
    "title": "Summary",
    "section": "",
    "text": "The foundation for meaningful data analysis is accurate and reliable crime data, yet long-standing challenges in criminal history record systems continue to undermine their precision and dependability1. As outlined in the Use and Management of Criminal History Record Information Report (1993), issues such as incomplete data reporting, delays in recording arrests and case dispositions, and inconsistent fingerprint submissions have plagued the system for decades1 . Criminal History Record Information (CHRI), the backbone for arrest datasets like those from the Illinois Criminal Justice Information Authority (ICJIA), is vulnerable to these shortcomings. Dispositions, or the outcomes of arrests, are often missing or delayed, leaving critical gaps in the data that make it difficult to analyze systemic trends. While advances in technology, such as digital fingerprinting, have improved some processes, data fragmentation and insufficient oversight remain persistent barriers to comprehensive and accurate reporting.\nFast forward thirty years, and many of these challenges remain unresolved—now further complicated by uneven implementation of modern systems. The Marshall Project highlights a striking example: in 2022, the FBI’s shift to the National Incident-Based Reporting System (NIBRS) created a significant data gap in national crime statistics2. Over 6,000 police agencies failed to submit their data, representing nearly one-third of all agencies and leaving vast portions of the U.S. population unaccounted for. This includes major departments like the NYPD and LAPD, alongside countless smaller agencies2. These gaps reflect broader systemic failures—inconsistent adoption of updated systems, lack of adequate funding, and minimal oversight—that echo the same issues identified decades ago.\nIn short, crime data, even when sourced from official systems, remains inherently flawed and incomplete. Whether due to outdated processes, inconsistent reporting practices, privacy-driven modifications, or gaps in modern collection systems, crime data often falls short of providing a fully accurate or comprehensive picture. As a result, any analysis relying on this data must account for these imperfections, recognizing that while the data can uncover critical trends and systemic disparities, it is rarely a perfect representation of reality.\n\n\nThe Illinois arrest dataset obtained from the ICJIA Arrest Explorer highlights the trade-offs between privacy and precision, adding to the broader challenges of criminal justice data. To protect confidentiality, counts under 10 are approximated—values between 0 and 4 are replaced with 1, while counts from 5 to 9 are replaced with 6. Though this approach is necessary to safeguard sensitive data, it can introduce distortions, particularly in smaller counties or demographic groups, where even slight approximations can significantly skew trends and reduce the accuracy of analysis. Compounding these issues, the lack of oversight in reporting further undermines data consistency and reliability, reflecting the broader systemic limitations that continue to plague many criminal justice datasets."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#benchmarks",
    "href": "technical-details/data-collection/closing.html#benchmarks",
    "title": "Summary",
    "section": "",
    "text": "The National Registry of Exonerations is a trusted and widely used resource, frequently cited in academic and legal research. Unlike government databases, the Registry is run by a team of researchers and academics, which brings a level of precision and thoroughness often missing in state-managed systems3. Its foundation in meticulous documentation and independent research makes it less vulnerable to political or institutional biases. As a result, the Registry stands out as a more reliable and comprehensive tool for understanding wrongful convictions."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "title": "Summary",
    "section": "",
    "text": "The data collected from various sources provide a foundation for examining systemic racial disparities in over-policing and wrongful convictions. However, challenges such as the lack of precision in Illinois arrest datasets and broader gaps in national crime reporting underscore the need to approach findings with caution. Even with these limitations, the use of reliable resources like the National Registry of Exonerations adds credibility to the analysis. Moving forward, future research could focus on compiling crime data from a single state across local, state, and federal agencies to better understand inconsistencies and uncover patterns of underreporting. Expanding this work to compare data collection practices across states could also reveal regional differences in crime reporting and highlight systemic disparities in how data is recorded and shared."
  },
  {
    "objectID": "technical-details/data-collection/overview.html",
    "href": "technical-details/data-collection/overview.html",
    "title": "Overview",
    "section": "",
    "text": "For the data collection process, I gathered multiple datasets, including arrest records from the ICJIA Arrest Explorer, exoneration data from the National Registry of Exonerations, incarceration demographics from the Prison Policy Initiative, and geospatial information. This diverse collection enabled an in-depth examination of systemic racial inequities that drive the over-policing of marginalized communities while also facilitating a critical analysis of nuanced racial patterns in exoneration data, exploring their connection to broader societal structures. Together, these datasets provide a framework for understanding the interconnected dynamics of racial injustice in the criminal justice system.\n\n\nMy goal in the data collection process is to gather the datasets needed to examine systemic racial disparities in policing and exonerations. I’m focused on sourcing reliable arrest records, exoneration data, incarceration demographics, and geospatial information. Ultimately, I aim to compile data that can stand alone or work together to analyze over-policing trends, uncover racial patterns in exonerations, and explore the geographic dynamics connected to these issues. I’m particularly driven by the need to ground these systemic inequities in hard numbers and statistical analysis. Too often, people refuse to acknowledge the reality of racism without concrete evidence. By putting real data behind these patterns, I hope to demonstrate the scope and impact of these injustices in a way that cannot be ignored.\n\n\n\n\nIdentify and collect datasets needed to analyze racial disparities in over-policing and wrongful convictions in Illinois, including arrest records, exoneration data, incarceration demographics, and geospatial information.\n\nEnsure datasets are accurate, up-to-date, and aligned with the project’s goals by prioritizing reliable sources like official registries and well-documented repositories.\n\nStructure the data to support analysis of racial patterns in policing and exonerations, allowing for both independent and interconnected evaluations.\n\nDocument all steps in the data collection process, including sourcing methods and preprocessing workflows, to ensure transparency and reproducibility.\n\nCollect and preprocess geocoded data and Illinois county shapefiles to enable geographic exploratory data analysis and visualizations."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#goals-motivation",
    "href": "technical-details/data-collection/overview.html#goals-motivation",
    "title": "Overview",
    "section": "",
    "text": "My goal in the data collection process is to gather the datasets needed to examine systemic racial disparities in policing and exonerations. I’m focused on sourcing reliable arrest records, exoneration data, incarceration demographics, and geospatial information. Ultimately, I aim to compile data that can stand alone or work together to analyze over-policing trends, uncover racial patterns in exonerations, and explore the geographic dynamics connected to these issues. I’m particularly driven by the need to ground these systemic inequities in hard numbers and statistical analysis. Too often, people refuse to acknowledge the reality of racism without concrete evidence. By putting real data behind these patterns, I hope to demonstrate the scope and impact of these injustices in a way that cannot be ignored."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#objectives",
    "href": "technical-details/data-collection/overview.html#objectives",
    "title": "Overview",
    "section": "",
    "text": "Identify and collect datasets needed to analyze racial disparities in over-policing and wrongful convictions in Illinois, including arrest records, exoneration data, incarceration demographics, and geospatial information.\n\nEnsure datasets are accurate, up-to-date, and aligned with the project’s goals by prioritizing reliable sources like official registries and well-documented repositories.\n\nStructure the data to support analysis of racial patterns in policing and exonerations, allowing for both independent and interconnected evaluations.\n\nDocument all steps in the data collection process, including sourcing methods and preprocessing workflows, to ensure transparency and reproducibility.\n\nCollect and preprocess geocoded data and Illinois county shapefiles to enable geographic exploratory data analysis and visualizations."
  },
  {
    "objectID": "technical-details/progress-log.html",
    "href": "technical-details/progress-log.html",
    "title": "Progress log",
    "section": "",
    "text": "Tue: 10-20-2024\n\nDecided on a project topic related to criminal justice reform, narrowing focus to over-policing, race, and poverty in Chicago.\n\nLogged my goal for this project and why the topic is extremely significant in project-planning.qmd.\n\nWed: 10-30-2024\n\nStarted the literature review by summarizing and analyzing key sources, including research on over-policing, wrongful convictions, and socioeconomic factors that contribute to disparities in Chicago’s criminal justice system.\n\nTue: 11-05-2024\n\nContinued review of literature, focusing on research about over-policing, racial disparities, and socioeconomic factors contributing to policing dynamics.\nSummarized and analyzed key sources using LLM, including comprehensive studies on systemic biases, police misconduct, wrongful convictions, and socioeconomic determinants of crime.\nAdded detailed notes from sources to my literature review.qmd file, focusing on findings relevant to Chicago, such as socioeconomic disparities, racial disparities, and historical context.\nCreated a references.bib and added references from literature review to ensure proper citation for each source, integrating new academic and news sources into my project plan.\nDocumented and expanded key insights from sources to inform project direction.\n\nFri: 11-08-2024\n\nMet with Prof. Jacobs to discuss project topic.\n\nMon: 11-11-2024\n\nMet with Prof. Jacobs to go over datasets and ask research questions.\nSelected datasets related to over-policing, race, and exonerations in Illinois.\n\nThurs: 11-21-2024\n\nLoaded in datasets and started cleaning exoneration data.\n\nMon: 11-25-2024\n\nOffice Hours with Prof. Jacobs to go over possibilities for supervised and unsupervised learning.\n\nThurs: 11-28-2024\n\nFinished Cleaning exoneration data.\nCreated about me page for multiclass-portfolio website.\n\nSun: 12-1-2024\n\nCompleted multi-class-portfolio website rough draft and pushed to GU domains.\nAdded in-line prose to data-cleaning exoneration data to explain data cleaning process.\nStarted EDA, beginning with a GIS to analyze the locations of arrests and exonerees.\n\nTues: 12-3-2024\n\nMet with professor Jacobs to answer questions about my EDA and normalization/scaling + sharing misconduct results + help with supervised learning.\nCompleted EDA (still have to add in-text prose to explain).\n\nSun: 12-8-2024\n\nCreation of Exoneration Counterfactuals.\nCompleted Supervised Learning (still have to add in-text prose to explain).\n\nMon: 12-9-2024\n\nRestructured EDA to improve narrative flow and added missing visualizations.\nAdded detailed explanation prose for EDA insights.\nFinalized analysis of geospatial data, ensuring clear findings on wrongful convictions and over-policing trends.\n\nWed: 12-11-2024\n\nStarted unsupervised learning models to explore clustering patterns in wrongful conviction data.\nCompleted first draft of unsupervised learning results.\nAdded explanation prose for K-means clustering and PCA visualizations.\n\nFri: 12-13-2024\n\nEdited and refined supervised learning section, adding explanation prose for predictive models.\nCreated a new landing page for the multi-class portfolio website to improve project presentation.\nIntegrated new visuals and insights into the project report.\n\nSun: 12-15-2024\n\nContinued editing project report for clarity, coherence, and structure.\nUpdated and expanded sections in multi-class portfolio website.\nProofread all project components, ensuring readiness for submission.\n\nMon: 12-16-2024\n\nFinalized project report, including recommendations and call-to-action section.\nMade final edits to multi-class portfolio website and pushed to GU domains.\nReviewed all deliverables for consistency and completeness, including the progress log and usage transparency pages."
  },
  {
    "objectID": "technical-details/progress-log.html#weekly-progress",
    "href": "technical-details/progress-log.html#weekly-progress",
    "title": "Progress log",
    "section": "",
    "text": "Tue: 10-20-2024\n\nDecided on a project topic related to criminal justice reform, narrowing focus to over-policing, race, and poverty in Chicago.\n\nLogged my goal for this project and why the topic is extremely significant in project-planning.qmd.\n\nWed: 10-30-2024\n\nStarted the literature review by summarizing and analyzing key sources, including research on over-policing, wrongful convictions, and socioeconomic factors that contribute to disparities in Chicago’s criminal justice system.\n\nTue: 11-05-2024\n\nContinued review of literature, focusing on research about over-policing, racial disparities, and socioeconomic factors contributing to policing dynamics.\nSummarized and analyzed key sources using LLM, including comprehensive studies on systemic biases, police misconduct, wrongful convictions, and socioeconomic determinants of crime.\nAdded detailed notes from sources to my literature review.qmd file, focusing on findings relevant to Chicago, such as socioeconomic disparities, racial disparities, and historical context.\nCreated a references.bib and added references from literature review to ensure proper citation for each source, integrating new academic and news sources into my project plan.\nDocumented and expanded key insights from sources to inform project direction.\n\nFri: 11-08-2024\n\nMet with Prof. Jacobs to discuss project topic.\n\nMon: 11-11-2024\n\nMet with Prof. Jacobs to go over datasets and ask research questions.\nSelected datasets related to over-policing, race, and exonerations in Illinois.\n\nThurs: 11-21-2024\n\nLoaded in datasets and started cleaning exoneration data.\n\nMon: 11-25-2024\n\nOffice Hours with Prof. Jacobs to go over possibilities for supervised and unsupervised learning.\n\nThurs: 11-28-2024\n\nFinished Cleaning exoneration data.\nCreated about me page for multiclass-portfolio website.\n\nSun: 12-1-2024\n\nCompleted multi-class-portfolio website rough draft and pushed to GU domains.\nAdded in-line prose to data-cleaning exoneration data to explain data cleaning process.\nStarted EDA, beginning with a GIS to analyze the locations of arrests and exonerees.\n\nTues: 12-3-2024\n\nMet with professor Jacobs to answer questions about my EDA and normalization/scaling + sharing misconduct results + help with supervised learning.\nCompleted EDA (still have to add in-text prose to explain).\n\nSun: 12-8-2024\n\nCreation of Exoneration Counterfactuals.\nCompleted Supervised Learning (still have to add in-text prose to explain).\n\nMon: 12-9-2024\n\nRestructured EDA to improve narrative flow and added missing visualizations.\nAdded detailed explanation prose for EDA insights.\nFinalized analysis of geospatial data, ensuring clear findings on wrongful convictions and over-policing trends.\n\nWed: 12-11-2024\n\nStarted unsupervised learning models to explore clustering patterns in wrongful conviction data.\nCompleted first draft of unsupervised learning results.\nAdded explanation prose for K-means clustering and PCA visualizations.\n\nFri: 12-13-2024\n\nEdited and refined supervised learning section, adding explanation prose for predictive models.\nCreated a new landing page for the multi-class portfolio website to improve project presentation.\nIntegrated new visuals and insights into the project report.\n\nSun: 12-15-2024\n\nContinued editing project report for clarity, coherence, and structure.\nUpdated and expanded sections in multi-class portfolio website.\nProofread all project components, ensuring readiness for submission.\n\nMon: 12-16-2024\n\nFinalized project report, including recommendations and call-to-action section.\nMade final edits to multi-class portfolio website and pushed to GU domains.\nReviewed all deliverables for consistency and completeness, including the progress log and usage transparency pages."
  },
  {
    "objectID": "report/report.html",
    "href": "report/report.html",
    "title": "Cell Block Tango:  Steps of Misconduct, Over-Policing, and Cycles of Racial Injustice in Chicago’s Criminal Justice System",
    "section": "",
    "text": "This report investigates systemic failures in Illinois’ criminal justice system, emphasizing the disproportionate impact of wrongful convictions and over-policing on marginalized communities, particularly Black men. Through comprehensive data analysis and machine learning models, key findings highlight racial disparities, patterns of police misconduct, and geographic trends. Actionable recommendations aim to drive policy reforms, improve accountability, and reduce systemic bias."
  },
  {
    "objectID": "report/report.html#geospatial-trends-of-arrests-and-exonerations-by-county",
    "href": "report/report.html#geospatial-trends-of-arrests-and-exonerations-by-county",
    "title": "Cell Block Tango:  Steps of Misconduct, Over-Policing, and Cycles of Racial Injustice in Chicago’s Criminal Justice System",
    "section": "1. Geospatial Trends of Arrests and Exonerations by County",
    "text": "1. Geospatial Trends of Arrests and Exonerations by County\nLog-Scaled Arrests\n\nLog-Scaled Exonerations\n\n\nInsight: Cook County leads Illinois in both arrests and wrongful convictions, showing its outsized role in systemic failures. Smaller rural counties, while having fewer total cases, still display disproportionately high rates of wrongful convictions relative to their population size. This suggests that geographic disparities exist, and rural patterns remain underexplored in justice system analyses."
  },
  {
    "objectID": "report/report.html#distribution-of-years-wrongfully-lost-by-race",
    "href": "report/report.html#distribution-of-years-wrongfully-lost-by-race",
    "title": "Cell Block Tango:  Steps of Misconduct, Over-Policing, and Cycles of Racial Injustice in Chicago’s Criminal Justice System",
    "section": "2. Distribution of Years Wrongfully Lost by Race",
    "text": "2. Distribution of Years Wrongfully Lost by Race\n\n\n\nRace Distribution of Years Lost\n\n\n\nInsight: Black individuals have disproportionately lost years to wrongful imprisonment compared to other racial groups. The distribution skews heavily toward shorter wrongful imprisonments, but significant outliers exist where individuals lost 20+ years. These findings highlight the compounding harm caused by systemic injustice, as even shorter sentences fracture families and communities."
  },
  {
    "objectID": "report/report.html#total-misconduct-tags-by-race",
    "href": "report/report.html#total-misconduct-tags-by-race",
    "title": "Cell Block Tango:  Steps of Misconduct, Over-Policing, and Cycles of Racial Injustice in Chicago’s Criminal Justice System",
    "section": "3. Total Misconduct Tags by Race",
    "text": "3. Total Misconduct Tags by Race\n\n\n\nTotal Misconduct by Race\n\n\n\nInsight: Police and prosecutorial misconduct overwhelmingly affect Black individuals. Misconduct types include perjury, witness tampering, and fabricated evidence. The stark disparity underscores how misconduct disproportionately drives wrongful convictions within marginalized communities and intensifies mistrust in law enforcement."
  },
  {
    "objectID": "report/report.html#misconduct-tags-by-county-and-race",
    "href": "report/report.html#misconduct-tags-by-county-and-race",
    "title": "Cell Block Tango:  Steps of Misconduct, Over-Policing, and Cycles of Racial Injustice in Chicago’s Criminal Justice System",
    "section": "4. Misconduct Tags by County and Race",
    "text": "4. Misconduct Tags by County and Race\n\n\n\nMisconduct Tags by County\n\n\n\nInsight: Cook County exhibits the highest number of misconduct cases, especially targeting Black individuals. Some rural counties (e.g., Winnebago, DuPage) also show patterns of concentrated misconduct despite smaller populations, emphasizing that these issues are not limited to urban areas. This highlights a need for local accountability mechanisms to address misconduct statewide."
  },
  {
    "objectID": "report/report.html#overrepresentation-ratios-by-county",
    "href": "report/report.html#overrepresentation-ratios-by-county",
    "title": "Cell Block Tango:  Steps of Misconduct, Over-Policing, and Cycles of Racial Injustice in Chicago’s Criminal Justice System",
    "section": "5. Overrepresentation Ratios by County",
    "text": "5. Overrepresentation Ratios by County\n\n\n\nOverrepresentation Ratios by County\n\n\n\nInsight: Black and Latino populations are significantly overrepresented in incarceration rates across Illinois counties. The map highlights systemic disparities, particularly in counties with higher urban populations and historical patterns of policing bias. White populations, by contrast, remain largely underrepresented in incarceration rates. These findings emphasize racial inequities in the justice system at a county level."
  },
  {
    "objectID": "report/report.html#disproportionality-index-exonerations-vs.-arrests-by-race",
    "href": "report/report.html#disproportionality-index-exonerations-vs.-arrests-by-race",
    "title": "Cell Block Tango:  Steps of Misconduct, Over-Policing, and Cycles of Racial Injustice in Chicago’s Criminal Justice System",
    "section": "6. Disproportionality Index: Exonerations vs. Arrests by Race",
    "text": "6. Disproportionality Index: Exonerations vs. Arrests by Race\n\n\n\nDisproportionality Index - Arrests\n\n\n\nInsight: Exonerations compared to arrests highlight the front-end failures in the justice system. Black individuals have a disproportionality index exceeding 1.6, meaning they experience wrongful convictions at rates far higher than their arrest frequency would suggest. This points to over-policing and biased arrest practices that disproportionately affect Black communities. White individuals, in contrast, have a disproportionality index well below parity, suggesting fewer wrongful arrests and more favorable outcomes. These trends emphasize systemic bias in policing practices, where wrongful convictions are more likely to result from unjustified or excessive arrests among marginalized populations."
  },
  {
    "objectID": "report/report.html#disproportionality-index-exonerations-vs.-incarcerated-population-by-race",
    "href": "report/report.html#disproportionality-index-exonerations-vs.-incarcerated-population-by-race",
    "title": "Cell Block Tango:  Steps of Misconduct, Over-Policing, and Cycles of Racial Injustice in Chicago’s Criminal Justice System",
    "section": "7. Disproportionality Index: Exonerations vs. Incarcerated Population by Race",
    "text": "7. Disproportionality Index: Exonerations vs. Incarcerated Population by Race\n\n\n\nDisproportionality Index - Incarceration\n\n\n\nInsight: The Disproportionality Index comparing exonerations to the incarcerated population underscores deeper systemic inequities within the justice process. The dashed line at 1.0 represents parity—where exonerations align proportionally with incarceration rates. For Black individuals, the index value significantly exceeds 1.8, indicating extreme overrepresentation in wrongful convictions relative to incarceration rates. This suggests systemic issues, such as biased investigations, sentencing disparities, and racial profiling, disproportionately impacting Black communities.\n\nHispanic individuals show an index value below that of Black individuals but above White populations, indicating some disproportionality but to a lesser extent. White individuals have the lowest index value, well below 1.0, suggesting fewer wrongful convictions or more favorable systemic outcomes. Together, these findings highlight racial disparities across the justice system, where Black individuals face the greatest burden of wrongful incarceration."
  },
  {
    "objectID": "report/report.html#age-distribution-of-exonerees-by-race",
    "href": "report/report.html#age-distribution-of-exonerees-by-race",
    "title": "Cell Block Tango:  Steps of Misconduct, Over-Policing, and Cycles of Racial Injustice in Chicago’s Criminal Justice System",
    "section": "8. Age Distribution of Exonerees by Race",
    "text": "8. Age Distribution of Exonerees by Race\n\n\n\nAge Distribution\n\n\n\nInsight: Wrongful convictions primarily impact Black men aged 20-30, a period critical for family stability, career development, and education. This age-based harm perpetuates cycles of poverty and systemic trauma, further marginalizing affected individuals and communities."
  },
  {
    "objectID": "report/report.html#breakdown-of-official-misconduct-by-race",
    "href": "report/report.html#breakdown-of-official-misconduct-by-race",
    "title": "Cell Block Tango:  Steps of Misconduct, Over-Policing, and Cycles of Racial Injustice in Chicago’s Criminal Justice System",
    "section": "9. Breakdown of Official Misconduct by Race",
    "text": "9. Breakdown of Official Misconduct by Race\n\n\n\nBreakdown of Misconduct\n\n\n\nInsight: Black individuals are overwhelmingly impacted by police misconduct, including perjury, false accusations, and suppression of evidence. This pattern reinforces systemic racial bias in the justice process and raises calls for enhanced accountability and oversight mechanisms within law enforcement."
  },
  {
    "objectID": "report/report.html#predicted-probabilities-of-wrongful-conviction",
    "href": "report/report.html#predicted-probabilities-of-wrongful-conviction",
    "title": "Cell Block Tango:  Steps of Misconduct, Over-Policing, and Cycles of Racial Injustice in Chicago’s Criminal Justice System",
    "section": "10. Predicted Probabilities of Wrongful Conviction",
    "text": "10. Predicted Probabilities of Wrongful Conviction\nBy County\n\nBy Race and County\n\nBy Race\n\n\nInsight: Predictive models identify Cook County and surrounding areas as high-risk for wrongful convictions, particularly for Black individuals. Smaller counties also display concerning probabilities, reinforcing the presence of systemic racial inequities across Illinois’ justice system."
  },
  {
    "objectID": "report/report.html#k-means-clustering-results",
    "href": "report/report.html#k-means-clustering-results",
    "title": "Cell Block Tango:  Steps of Misconduct, Over-Policing, and Cycles of Racial Injustice in Chicago’s Criminal Justice System",
    "section": "11. K-Means Clustering Results",
    "text": "11. K-Means Clustering Results\n\n\n\nK-Means Clustering\n\n\n\nInsight: Cluster analysis reveals clear groupings of cases based on racial, geographic, and misconduct-related factors. Notably, clusters with higher concentrations of Black exonerees correlate with misconduct-prone counties, highlighting systemic racial targeting within these jurisdictions."
  },
  {
    "objectID": "report/report.html#dimensionality-reduction-results",
    "href": "report/report.html#dimensionality-reduction-results",
    "title": "Cell Block Tango:  Steps of Misconduct, Over-Policing, and Cycles of Racial Injustice in Chicago’s Criminal Justice System",
    "section": "12. Dimensionality Reduction Results",
    "text": "12. Dimensionality Reduction Results\n\n\n\nDimensionality Reduction\n\n\n\nInsight: Principal Component Analysis (PCA) and t-SNE visualizations reveal distinct patterns across racial and geographic dimensions. These visualizations demonstrate that race is a key driver in wrongful convictions, particularly for Black and Hispanic populations. Misconduct cases show dense clustering, indicating systemic patterns rather than isolated incidents."
  },
  {
    "objectID": "report/report.html#call-to-action",
    "href": "report/report.html#call-to-action",
    "title": "Cell Block Tango:  Steps of Misconduct, Over-Policing, and Cycles of Racial Injustice in Chicago’s Criminal Justice System",
    "section": "Call to Action",
    "text": "Call to Action\nThe time for incremental change has passed. Systemic injustices require bold, comprehensive solutions. Policymakers, researchers, and advocates must work together to dismantle the cycles of harm perpetuated by wrongful convictions and over-policing.\nThe data is clear: Black men are disproportionately targeted, their lives derailed not by legitimate justice, but by bias, misconduct, and institutional failures. This crisis extends beyond individual cases—it reflects deeply embedded flaws that fracture families, destabilize communities, and erode public trust in the justice system.\nTo move forward, it’s necessary to center impacted voices, address racialized practices in law enforcement and prosecution, and commit to the restoration and healing of harmed communities. Real accountability—both for institutions and individuals—is not optional; it is a moral imperative.\nJustice demands more than apologies or empty promises—it requires action. It is time to transform the system, not as a tool of harm, but as a foundation for equity, dignity, and restoration."
  }
]