[
  {
    "objectID": "technical-details/unsupervised-learning/main.html",
    "href": "technical-details/unsupervised-learning/main.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#suggested-page-structure",
    "href": "technical-details/unsupervised-learning/main.html#suggested-page-structure",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#what-to-address",
    "href": "technical-details/unsupervised-learning/main.html#what-to-address",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/eda/main.html",
    "href": "technical-details/eda/main.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The goal of this EDA is to gain a deeper understanding of the dataset, identify key patterns, and uncover insights that will guide the next stages of the project. This analysis focuses on exploring the geographic and demographic distributions within the data, examining relationships between key variables, and preparing the dataset for modeling. By summarizing and visualizing the data, this section ensures that both the exonerated and non-exonerated groups are well-understood and balanced for supervised learning tasks."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html",
    "href": "technical-details/data-cleaning/main.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "This section outlines the steps taken to clean and preprocess the U.S. exoneration dataset, with a specific focus on Illinois cases. The cleaning process organizes the raw data into a structured and usable format for exploratory data analysis (EDA) and subsequent analysis workflows. By the end of this phase, the dataset will be well-structured, free of inconsistencies, and ready for further exploratory data analysis and machine learning workflows.\n\n\n\n\n\nThe dataset is first loaded to inspect its structure and contents. The purpose of this step is to get an initial sense of the data types, potential missing values, and the distribution of key variables which helps inform the cleaning steps required to make the data consistent and analyzable.\n\n# Import necessary Libraries:\nimport pandas as pd  # Used for data management, exploration, and manipulation\nimport numpy as np  # Used for numerical operations and array-based data processing\nimport seaborn as sns  # Used for data visualization, especially for missing values\nimport matplotlib.pyplot as plt  # Used for plotting and visualizing data\nimport re  # Used for handling and processing regular expressions, e.g., date cleaning\n\n# Load exoneration dataset:\ndf = pd.read_csv('../../data/raw-data/US_exoneration_data.csv')\nprint(\"Initial Dataset: \")\npd.set_option('display.max_columns', None) # Enables display of every column\ndf.head()\n\nInitial Dataset: \n\n\n\n\n\n\n\n\n\nLast Name\nFirst Name\nAge\nRace\nSex\nState\nCounty\nTags\nWorst Crime Display\nSentence\nPosting Date\nOM Tags\nF/MFE\nFC\nILD\nP/FA\nDNA\nMWID\nOM\nDate of Exoneration\nDate of 1st Conviction\nDate of Release\n\n\n\n\n0\nAbbitt\nJoseph\n31.0\nBlack\nMale\nNorth Carolina\nForsyth\nCV;#IO;#SA\nChild Sex Abuse\nLife\n9/1/11\nNaN\nNaN\nNaN\nNaN\nNaN\nDNA\nMWID\nNaN\n9/2/09\n6/22/95\n9/2/09\n\n\n1\nAbbott\nCinque\n19.0\nBlack\nMale\nIllinois\nCook\nCIU;#IO;#NC;#P\nDrug Possession or Sale\nProbation\n2/14/22\nOF;#WH;#NW\nNaN\nNaN\nNaN\nP/FA\nNaN\nNaN\nOM\n2/1/22\n3/25/08\n3/25/08\n\n\n2\nAbdal\nWarith Habib\n43.0\nBlack\nMale\nNew York\nErie\nIO;#SA\nSexual Assault\n20 to Life\n8/29/11\nOF;#WH;#NW;#WT\nF/MFE\nNaN\nNaN\nNaN\nDNA\nMWID\nOM\n9/1/99\n6/6/83\n9/1/99\n\n\n3\nAbernathy\nChristopher\n17.0\nWhite\nMale\nIllinois\nCook\nCIU;#CV;#H;#IO;#JV;#SA\nMurder\nLife without parole\n2/13/15\nOF;#WH;#NW;#INT\nNaN\nFC\nNaN\nP/FA\nDNA\nNaN\nOM\n2/11/15\n1/15/87\n2/11/15\n\n\n4\nAbney\nQuentin\n32.0\nBlack\nMale\nNew York\nNew York\nCV\nRobbery\n20 to Life\n5/13/19\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nMWID\nNaN\n1/19/12\n3/20/06\n1/19/12\n\n\n\n\n\n\n\n\n\n\n\nBefore diving into the broader data cleaning process, I decided to narrow the scope of my research question to Illinois. This choice was intentional to focus the analysis on a specific region, ensuring that the findings are both relevant and manageable within the scope of this project. Illinois was selected because of its extensive record of exoneration cases, particularly in Cook Count (Chicago), which provides a valuable dataset for analyzing systemic issues within the criminal justice system. Chicago, in particular, has long been associated with significant racial disparities and deeply entrenched problems in policing and prosecution, making it a critical focal point for this analysis1. By focusing on Illinois, the dataset remains consistent in terms of jurisdictional laws and practices, allowing for a more accurate and concentrated exploration of patterns and trends in over-policing and wrongful convictions. This regional focus highlights the broader systemic failures of the criminal justice system while enabling a detailed examination of one of the most historically inequitable jurisdictions in terms of racial justice.\n\n\nTo isolate Illinois cases, the dataset was filtered by the state column, retaining only rows where the value matched “Illinois.” This step reduced the dataset to 548 rows, making it more manageable for analysis and visualization. Below is a preview of the filtered dataset:\n\n# Filter Data for Illinois: \ndf = df[df['State'] == 'Illinois']\nprint(\"Number of exonerees for Illinois subset: \" , df.shape[0]) \ndf.head()\n\nNumber of exonerees for Illinois subset:  548\n\n\n\n\n\n\n\n\n\nLast Name\nFirst Name\nAge\nRace\nSex\nState\nCounty\nTags\nWorst Crime Display\nSentence\nPosting Date\nOM Tags\nF/MFE\nFC\nILD\nP/FA\nDNA\nMWID\nOM\nDate of Exoneration\nDate of 1st Conviction\nDate of Release\n\n\n\n\n1\nAbbott\nCinque\n19.0\nBlack\nMale\nIllinois\nCook\nCIU;#IO;#NC;#P\nDrug Possession or Sale\nProbation\n2/14/22\nOF;#WH;#NW\nNaN\nNaN\nNaN\nP/FA\nNaN\nNaN\nOM\n2/1/22\n3/25/08\n3/25/08\n\n\n3\nAbernathy\nChristopher\n17.0\nWhite\nMale\nIllinois\nCook\nCIU;#CV;#H;#IO;#JV;#SA\nMurder\nLife without parole\n2/13/15\nOF;#WH;#NW;#INT\nNaN\nFC\nNaN\nP/FA\nDNA\nNaN\nOM\n2/11/15\n1/15/87\n2/11/15\n\n\n5\nAbrego\nEruby\n20.0\nHispanic\nMale\nIllinois\nCook\nCDC;#H;#IO\nMurder\n90 years\n8/25/22\nOF;#WH;#NW;#WT;#INT;#PJ\nNaN\nFC\nNaN\nP/FA\nNaN\nMWID\nOM\n7/21/22\n9/22/04\n7/21/22\n\n\n10\nAdams\nDemetris\n22.0\nBlack\nMale\nIllinois\nCook\nCIU;#IO;#NC;#P\nDrug Possession or Sale\n1 year\n4/13/20\nOF;#WH;#NW\nNaN\nNaN\nNaN\nP/FA\nNaN\nNaN\nOM\n2/11/20\n9/8/04\n12/26/04\n\n\n15\nAdams\nKenneth\n22.0\nBlack\nMale\nIllinois\nCook\nCDC;#H;#IO;#JI;#SA\nMurder\n75 years\n8/29/11\nPR;#OF;#WH;#NW;#KP;#WT\nF/MFE\nNaN\nNaN\nP/FA\nDNA\nMWID\nOM\n7/2/96\n10/20/78\n6/14/96\n\n\n\n\n\n\n\n\n\n\n\n\n\nMissing values are identified using isnull() to determine their extent and distribution across the dataset. The goal is to ensure that no critical data gaps remain unaddressed before proceeding with analysis.\n\n# Managing Missing Data - Identifying which columns have a lot of missing data:\nna_counts = df.isna().sum()\nprint(na_counts)\n\nLast Name                   0\nFirst Name                  0\nAge                         1\nRace                        0\nSex                         0\nState                       0\nCounty                      0\nTags                       15\nWorst Crime Display         0\nSentence                    0\nPosting Date                0\nOM Tags                    70\nF/MFE                     474\nFC                        410\nILD                       440\nP/FA                       67\nDNA                       482\nMWID                      442\nOM                         70\nDate of Exoneration         0\nDate of 1st Conviction      0\nDate of Release             0\ndtype: int64\n\n\n\n\n\n\n\nThe following columns were removed due to excessive missing data:\n\nF/MFE, ILD, P/FA, DNA, MWID, FC: Each of these columns had more than 50% missing values, which made them unreliable for meaningful analysis. Removing them ensures the dataset remains robust and manageable without introducing bias from imputation.\n\nRetaining “OM” and “OM Tags” Columns Initially, I removed the OM (Official Misconduct) and OM Tags columns, assuming their information would be captured in the general Tags column. However, during the exploratory data analysis (EDA), I discovered that these columns contained unique and valuable insights not present in the Tags column; as a result I retained them for further analysis.\n\n# Drop columns with excessive missing values: \ndf_original = df.copy()\ndf.drop(columns = ['F/MFE', 'ILD', 'P/FA', 'DNA', 'MWID', 'FC'], inplace = True)\n\n\n\n\nTo better understand the distribution of missing values, a heatmap is generated. This visualization provides a clear overview of where missing values occur, helping to decide which columns or rows to address in subsequent steps.\n\n\nA heatmap is generated to visualize the extent of missing data before cleaning. Columns with a high proportion of missing values are easily identifiable, providing a clear justification for their removal.\n\n# Heatmap of missing data before cleaning:\nplt.figure(figsize=(12, 6))\nsns.heatmap(df_original.isnull(), cbar=False, cmap='viridis')\nplt.title('Heatmap of Missing Data (Before Cleaning)')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nA second heatmap is generated after cleaning to confirm that all unnecessary columns with excessive missing values have been removed. This ensures the dataset is now complete and ready for further analysis.\n\n# Heatmap of missing data after cleaning:\nplt.figure(figsize=(12, 6))\nsns.heatmap(df.isnull(), cbar=False, cmap='viridis')\nplt.title('Heatmap of Missing Data (After Cleaning)')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo ensure consistency and simplify future operations, all column names were standardized by converting them to lowercase and replacing spaces with underscores (_). This transformation enhances readability, aligns with Python’s naming conventions, and makes column names easier to reference in code. For example, a column originally labeled First Name is now first_name.\n\n# Standardize column names by converting to lowercase and replacing spaces with '_':\ndf.columns = df.columns.str.lower().str.replace(' ', '_')\nprint(df.columns)\n\nIndex(['last_name', 'first_name', 'age', 'race', 'sex', 'state', 'county',\n       'tags', 'worst_crime_display', 'sentence', 'posting_date', 'om_tags',\n       'om', 'date_of_exoneration', 'date_of_1st_conviction',\n       'date_of_release'],\n      dtype='object')\n\n\n\n\nAdditionally, the sex column was converted to lowercase to maintain uniformity across textual data. This step ensures that values like “Male” and “male” are treated equivalently during analysis, reducing potential discrepancies caused by case sensitivity.\n\n# Convert sex values to lowercase: \ndf['sex'] = df['sex'].str.lower()\ndf['sex'].head()\n\n1     male\n3     male\n5     male\n10    male\n15    male\nName: sex, dtype: object\n\n\n\n\n\n\nAccurate data type formatting is essential for effective analysis. This section ensures that all variables are correctly identified as numerical, categorical, or date-time types so that they are ready for further processing.\n\n# Display data types for each column:\nprint(df.dtypes)\n\nlast_name                  object\nfirst_name                 object\nage                       float64\nrace                       object\nsex                        object\nstate                      object\ncounty                     object\ntags                       object\nworst_crime_display        object\nsentence                   object\nposting_date               object\nom_tags                    object\nom                         object\ndate_of_exoneration        object\ndate_of_1st_conviction     object\ndate_of_release            object\ndtype: object\n\n\n\n\nUpon reviewing the data types, it was noted that several columns, such as Last Name, First Name, Race, State, County, and Worst Crime Display, were classified as object. While this is acceptable for textual data, converting these columns to string ensures consistency and prevents potential issues when performing text-specific operations. This transformation also allows for better optimization and clarity in the data processing pipeline. The changes were necessary to standardize the dataset and ensure compatibility with downstream analysis tasks.\n\n# Convert relevant columns to string type\nstring_columns = ['last_name', 'first_name', 'race', 'sex', 'state', 'county', 'worst_crime_display']\ndf[string_columns] = df[string_columns].astype('string')\n\n# Check updated data types\nprint(df.dtypes)\n\nlast_name                 string[python]\nfirst_name                string[python]\nage                              float64\nrace                      string[python]\nsex                       string[python]\nstate                     string[python]\ncounty                    string[python]\ntags                              object\nworst_crime_display       string[python]\nsentence                          object\nposting_date                      object\nom_tags                           object\nom                                object\ndate_of_exoneration               object\ndate_of_1st_conviction            object\ndate_of_release                   object\ndtype: object\n\n\n\n\n\nAll date columns (posting_date, date_of_exoneration, date_of_1st_conviction, date_of_release) are converted to datetime format. This transformation ensures consistency and allows for easier time-based calculations, such as measuring the time between conviction and exoneration.\n\n# Convert date columns into datetime format:\nfor col in ['posting_date', 'date_of_exoneration', 'date_of_1st_conviction', 'date_of_release']:\n    # Convert with explicit format (MM/DD/YY):\n    df[col] = pd.to_datetime(df[col], format='%m/%d/%y', errors='coerce')\n\nprint(df[['posting_date', 'date_of_exoneration', 'date_of_1st_conviction', 'date_of_release']].head())\n\n   posting_date date_of_exoneration date_of_1st_conviction date_of_release\n1    2022-02-14          2022-02-01             2008-03-25      2008-03-25\n3    2015-02-13          2015-02-11             1987-01-15      2015-02-11\n5    2022-08-25          2022-07-21             2004-09-22      2022-07-21\n10   2020-04-13          2020-02-11             2004-09-08      2004-12-26\n15   2011-08-29          1996-07-02             1978-10-20      1996-06-14\n\n\n\n# Verify updated data types:\nprint(df.dtypes)\n\nlast_name                 string[python]\nfirst_name                string[python]\nage                              float64\nrace                      string[python]\nsex                       string[python]\nstate                     string[python]\ncounty                    string[python]\ntags                              object\nworst_crime_display       string[python]\nsentence                          object\nposting_date              datetime64[ns]\nom_tags                           object\nom                                object\ndate_of_exoneration       datetime64[ns]\ndate_of_1st_conviction    datetime64[ns]\ndate_of_release           datetime64[ns]\ndtype: object\n\n\n\n\n\nThe sentence column contains textual descriptions of sentencing outcomes, including terms like “Life without parole,” “Death,” or a specified number of years. To facilitate analysis, this column is transformed into a numerical format (sentence_in_years) by converting life sentences and probation to placeholder values and handling ranges or mixed units (e.g., years and months).\n\n# Print unique sentencing values for a better idea on how to best clean column: \nunique_sentences = df['sentence'].unique()\nprint(unique_sentences)\n\n['Probation' 'Life without parole' '90 years' '1 year' '75 years'\n '30 years' '55 years' '2 years' '3 years' '6 years' '45 years'\n '1 year and 6 months' '50 years' '60 years' 'Life' '80 years' '18 years'\n '4 years' '85 years' '20 years' '35 years' '2 years and 6 months'\n '82 years' '12 years' 'Not sentenced' '22 years' '32 years' 'Death'\n '5 years' '40 years' '25 years' '26 years' '4 years and 6 months'\n '9 years' '48 years' '30 days' '84 years' '3 months and 25 days'\n '2 years and 2 months' '3 months' '44 years' '6 months' '25 to 50 years'\n '29 years' '23 years' '31 years' '11 years' '8 years' '24 years'\n '3 years and four months' '42 years' '3 years and 6 months' '65 years'\n '76 years' '15 years' '50 to Life' '86 years' '70 years' '28 years'\n '13 years' '47 years' '36 years' '18 months' '1 year and 4 months'\n '8 years and 6 months' '6 years and 6 months' '58 years' '95 years'\n '7 years' '34 years' '62 years' '27 years' '69 years' '57 years'\n '50 to 100 years' '4 months' '4 years and 3 months' '37 years' '10 years'\n '67 years' '46 years' '17 years' '10 to 22 years' '6 years and 7 months'\n '5 years and 6 months' '2 Years']\n\n\n\n\nThe sentence column is cleaned to convert textual descriptions into numerical values: 1. Probation and Not Sentenced are set to 0. 2. Life sentences and the death penalty are represented as 100 for placeholder analysis. 3. Ranges (e.g., “25 to 50 years”) are averaged to a single value. 4. Years and months are combined into total years for uniformity.\nThis standardization facilitates meaningful comparisons and quantitative analysis of sentencing patterns.\n\ndef clean_sentence(value):\n    \"\"\" Cleans the 'sentence' column values to numeric years for numerical EDA \n    - Probation is represented as 0.\n    - 'Not sentenced' is converted to np.nan.\n    - 'Life' and 'Death' sentences are represented as 100 (placeholder).\n    - Years and months are converted to a numeric value in years. \"\"\"\n\n    if value == 'Probation':\n        return 0\n    elif value == 'Not sentenced':\n        return np.nan  # NaN for not sentenced\n    elif 'Life' in value or value == 'Death':\n        return 100  # Placeholder for life sentences or death penalty\n    elif 'year' in value or 'month' in value:\n\n        # Handles ranges like '25 to 50 years'\n        if 'to' in value:\n            years = [int(num) for num in re.findall(r'\\d+', value)]\n            return sum(years) / len(years)  # Average the range \n        \n        # Handle \"X years and Y months\"\n        elif 'and' in value:\n            numbers = [float(num) for num in re.findall(r'\\d+', value)]\n            if len(numbers) == 2:  # Both years and months are present\n                years, months = numbers\n                return years + (months / 12)  # Convert months to years\n            elif len(numbers) == 1:  # Only one number is present\n                return numbers[0]  # Treat it as years\n            \n        # Handle only months or only years\n        elif 'months' in value:\n            months = int(re.search(r'\\d+', value).group())\n            return months / 12  # Convert months to years\n        else:  # Only years\n            return int(re.search(r'\\d+', value).group())\n    else:\n        return np.nan  # Anything unexpected as None\n    \ndf['sentence_in_years'] = df['sentence'].apply(clean_sentence)\n\n# Check results\ndf[['sentence', 'sentence_in_years']].head(10)\n\n\n\n\n\n\n\n\nsentence\nsentence_in_years\n\n\n\n\n1\nProbation\n0.0\n\n\n3\nLife without parole\n100.0\n\n\n5\n90 years\n90.0\n\n\n10\n1 year\n1.0\n\n\n15\n75 years\n75.0\n\n\n21\nProbation\n0.0\n\n\n22\nProbation\n0.0\n\n\n24\n30 years\n30.0\n\n\n25\n55 years\n55.0\n\n\n45\n1 year\n1.0\n\n\n\n\n\n\n\n\n# Check updated data types\nprint(df.dtypes)\n\nlast_name                 string[python]\nfirst_name                string[python]\nage                              float64\nrace                      string[python]\nsex                       string[python]\nstate                     string[python]\ncounty                    string[python]\ntags                              object\nworst_crime_display       string[python]\nsentence                          object\nposting_date              datetime64[ns]\nom_tags                           object\nom                                object\ndate_of_exoneration       datetime64[ns]\ndate_of_1st_conviction    datetime64[ns]\ndate_of_release           datetime64[ns]\nsentence_in_years                float64\ndtype: object\n\n\nAfter transforming the sentence column into a numerical format, the new column, sentence_in_years is now represented as a float64, which aligns with the desired structure for numerical analysis. This conversion allows for quantitative exploration of the sentencing data, such as aggregations and comparisons, during later stages of analysis. The original sentence column is retained for reference purposes, as it preserves the detailed textual descriptions that might be useful for contextual insights. The tags, OM, and OM_tags columns will be addressed later, so for now the datatype may remain as an object.\n\n\n\n\n\nThe tags and OM-tags columns contain important categorical information about each exoneration case. To make this data more useful for analysis, both columns were transformed into multiple binary columns, where each tag indicates the presence (1) or absence (0) of a specific feature. Additionally, a tag_sum column was created to capture the total number of tags associated with each case, providing a summary metric.\nThe cleaning process involved the following steps:\n\nRemoving Unnecessary Characters:\nUnwanted characters such as # were removed, and delimiters were standardized to ensure consistency in the data.\nSplitting Tags:\nThe tags and OM-tags columns were split into individual values to facilitate binary encoding.\nRenaming Binary Columns:\nEach binary column was renamed using clear and descriptive labels by mapping the original tags to their definitions. This mapping process translated short tag codes into their full meanings, improving interpretability. For reference, the definitions of the tags are based on the descriptions provided by the National Registry of Exonerations2.\nAdding a tag_sum Column:\nA new column was created to calculate the total number of tags for each case, enabling easier analysis of case complexity.\n\nThis transformation ensures the data is well-structured and ready for exploratory analysis, providing detailed insights into the systemic patterns in exoneration cases.\n\n# Clean 'tags' column:\ndf['tags'] = df['tags'].str.replace('#', '', regex=False).str.replace(\";\", \",\")\ndf['OM-tags'] = df['om_tags'].str.replace('#', '', regex=False).str.replace(\";\", \",\")\n\n# Define the mapping for tags:\ntag_mapping = {\n    \"A\": \"arson\",\n    \"BM\": \"bitemark\",\n    \"CDC\": \"co_defendant_confessed\",\n    \"CIU\": \"conviction_integrity_unit\",\n    \"CSH\": \"child_sex_abuse_hysteria_case\",\n    \"CV\": \"child_victim\",\n    \"F\": \"female_exoneree\",\n    \"FED\": \"federal_case\",\n    \"H\": \"homicide\",\n    \"IO\": \"innocence_organization\",\n    \"JI\": \"jailhouse_informant\",\n    \"JV\": \"juvenile_defendant\",\n    \"M\": \"misdemeanor\",\n    \"NC\": \"no_crime_case\",\n    \"P\": \"guilty_plea_case\",\n    \"PH\": \"posthumous_exoneration\",\n    \"SA\": \"sexual_assault\",\n    \"SBS\": \"shaken_baby_syndrome_case\",\n    \"PR\": \"prosecutor_misconduct\",\n    \"OF\": \"police_officer_misconduct\",\n    \"FA\": \"forensic_analyst_misconduct\",\n    \"CW\": \"child_welfare_worker_misconduct\",\n    \"WH\": \"withheld_exculpatory_evidence\",\n    \"NW\": \"misconduct_that_is_not_withholding_evidence\",\n    \"KP\": \"knowingly_permitting_perjury\",\n    \"WT\": \"witness_tampering_or_misconduct_interrogating_co_defendant\",\n    \"INT\": \"misconduct_in_interrogation_of_exoneree\",\n    \"PJ\": \"perjury_by_official\",\n    \"PL\": \"prosecutor_lied_in_court\"\n}\n\n# Split 'tags' and 'OM-tags' into lists:\ndf['tags'] = df['tags'].apply(lambda x: x.split(',') if isinstance(x, str) else x)\ndf['OM-tags'] = df['OM-tags'].apply(lambda x: x.split(',') if isinstance(x, str) else x)\n\n# Create binary columns for tags from both 'tags' and 'OM-tags':\nfor tag in tag_mapping.keys():\n    # Check if the tag exists in 'tags' or 'OM-tags':\n    df[tag] = df.apply(\n        lambda row: 1 if (isinstance(row['tags'], list) and tag in row['tags']) or \n                          (isinstance(row['OM-tags'], list) and tag in row['OM-tags']) else 0,\n        axis=1\n    )\n\n# Rename the binary columns using the tag_mapping dictionary:\ndf.rename(columns=tag_mapping, inplace=True)\n\n# Create `tag_sum` column to count the total number of tags for each exoneree:\ndf['tag_sum'] = df[list(tag_mapping.values())].sum(axis=1)\n\n# Drop the original 'tags' and 'OM-tags' columns:\ndf.drop(columns=['tags', 'om_tags', 'OM-tags'], inplace=True)\n\ndf.head()  \n\n\n\n\n\n\n\n\nlast_name\nfirst_name\nage\nrace\nsex\nstate\ncounty\nworst_crime_display\nsentence\nposting_date\nom\ndate_of_exoneration\ndate_of_1st_conviction\ndate_of_release\nsentence_in_years\narson\nbitemark\nco_defendant_confessed\nconviction_integrity_unit\nchild_sex_abuse_hysteria_case\nchild_victim\nfemale_exoneree\nfederal_case\nhomicide\ninnocence_organization\njailhouse_informant\njuvenile_defendant\nmisdemeanor\nno_crime_case\nguilty_plea_case\nposthumous_exoneration\nsexual_assault\nshaken_baby_syndrome_case\nprosecutor_misconduct\npolice_officer_misconduct\nforensic_analyst_misconduct\nchild_welfare_worker_misconduct\nwithheld_exculpatory_evidence\nmisconduct_that_is_not_withholding_evidence\nknowingly_permitting_perjury\nwitness_tampering_or_misconduct_interrogating_co_defendant\nmisconduct_in_interrogation_of_exoneree\nperjury_by_official\nprosecutor_lied_in_court\ntag_sum\n\n\n\n\n1\nAbbott\nCinque\n19.0\nBlack\nmale\nIllinois\nCook\nDrug Possession or Sale\nProbation\n2022-02-14\nOM\n2022-02-01\n2008-03-25\n2008-03-25\n0.0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n7\n\n\n3\nAbernathy\nChristopher\n17.0\nWhite\nmale\nIllinois\nCook\nMurder\nLife without parole\n2015-02-13\nOM\n2015-02-11\n1987-01-15\n2015-02-11\n100.0\n0\n0\n0\n1\n0\n1\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n1\n0\n0\n1\n0\n0\n10\n\n\n5\nAbrego\nEruby\n20.0\nHispanic\nmale\nIllinois\nCook\nMurder\n90 years\n2022-08-25\nOM\n2022-07-21\n2004-09-22\n2022-07-21\n90.0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n1\n1\n1\n0\n9\n\n\n10\nAdams\nDemetris\n22.0\nBlack\nmale\nIllinois\nCook\nDrug Possession or Sale\n1 year\n2020-04-13\nOM\n2020-02-11\n2004-09-08\n2004-12-26\n1.0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n7\n\n\n15\nAdams\nKenneth\n22.0\nBlack\nmale\nIllinois\nCook\nMurder\n75 years\n2011-08-29\nOM\n1996-07-02\n1978-10-20\n1996-06-14\n75.0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n1\n0\n1\n1\n0\n0\n1\n1\n1\n1\n0\n0\n0\n11\n\n\n\n\n\n\n\n\n# Convert 'OM' column to binary (1 if \"OM\" is present, 0 otherwise)\ndf['om'] = df['om'].apply(lambda x: 1 if str(x).strip().upper() == \"OM\" else 0)\n\n# Verify the transformation\nprint(df['om'].value_counts())\n\nom\n1    478\n0     70\nName: count, dtype: int64\n\n\n\n\n\nThe geocoded Illinois counties from Data Collection were merged into the main dataset:\n\nLoad and Standardize Data: Geocoded data was loaded, and column names were standardized to lowercase for consistency.\nFilter Relevant Counties: The geocoded data was filtered to include only counties present in the main dataset.\nMerge Data: Using a left join on county and state, geographic details (geocode_address, latitude, and longitude) were added to the dataset.\n\n\n# Read the geocoded population data from Data Collection\ngeocode_unique = pd.read_csv(\"../../data/raw-data/geocoded_population_counties.csv\")\n\n# Rename columns to lowercase for consistency\ngeocode_unique.rename(columns={\"County\": \"county\", \"State\": \"state\"}, inplace=True)\n\n# Filter geocode_unique to only include counties present in df\ngeocode_unique_filtered = geocode_unique[\n    geocode_unique[['county', 'state']].apply(tuple, axis=1).isin(df[['county', 'state']].apply(tuple, axis=1))\n]\n\n# Merge the filtered geocoding data into df\ndf = df.merge(geocode_unique_filtered, on=['county', 'state'], how='left')\n\n# Display the relevant columns to verify the merge\nprint(df[['state', 'county', 'geocode_address', 'latitude', 'longitude']].head())\n\n      state county                       geocode_address   latitude  longitude\n0  Illinois   Cook  Cook County, Illinois, United States  41.819738 -87.756525\n1  Illinois   Cook  Cook County, Illinois, United States  41.819738 -87.756525\n2  Illinois   Cook  Cook County, Illinois, United States  41.819738 -87.756525\n3  Illinois   Cook  Cook County, Illinois, United States  41.819738 -87.756525\n4  Illinois   Cook  Cook County, Illinois, United States  41.819738 -87.756525\n\n\n\n\n\nTo quantify the years_lost due to wrongful conviction, this step calculates the difference in years between an individual’s date_of_1st_conviction and their date_of_release.\n\n# Calculate \"years lost\" as the difference in years between release and conviction:\ndf['years_lost'] = (df['date_of_release'] - df['date_of_1st_conviction']).dt.days / 365.25 # Dividing by 365.25 accounts for leap years\n\n# Round the years lost to 2 decimal places:\ndf['years_lost'] = df['years_lost'].round(2)\n\n# Updated DataFrame:\nprint(df[['date_of_1st_conviction', 'date_of_release', 'years_lost']])\n\n    date_of_1st_conviction date_of_release  years_lost\n0               2008-03-25      2008-03-25        0.00\n1               1987-01-15      2015-02-11       28.07\n2               2004-09-22      2022-07-21       17.83\n3               2004-09-08      2004-12-26        0.30\n4               1978-10-20      1996-06-14       17.65\n..                     ...             ...         ...\n543             2005-04-13      2005-04-13        0.00\n544             2005-01-11      2006-07-12        1.50\n545             2005-04-11      2006-12-07        1.66\n546             2003-04-21      2005-03-10        1.89\n547             1994-09-20      2005-01-31       10.37\n\n[548 rows x 3 columns]\n\n\n\n\n\nTo improve readability and logical flow, the following changes were made to the column order:\n\nAlign Sentencing Data:\nThe sentence_in_years column was moved to appear immediately after sentence, ensuring that the cleaned numerical representation of sentencing data is logically aligned with its original textual description.\nReorganize Release and Years Lost:\nThe years_lost column was moved to appear immediately after date_of_release, facilitating easier comparison of release dates and the calculated time lost due to wrongful incarceration.\nGroup Geographic Data:\nThe latitude and longitude columns were moved to follow the county column, grouping geographic information together.\n\n\n# Reordering columns:\ncolumns = list(df.columns)  \n\n#Aligning sentencing data:  \ncolumns.insert(columns.index('sentence') + 1, columns.pop(columns.index('sentence_in_years')))  # Move 'sentence_in_years'\n\n#Reorganizing release and years lost\ncolumns.insert(columns.index('date_of_release') +1, columns.pop(columns.index('years_lost'))) #Move 'years_lost' \n\n# Move 'latitude' and 'longitude' after 'county'\ncolumns.insert(columns.index('county') + 1, columns.pop(columns.index('latitude')))\ncolumns.insert(columns.index('county') + 2, columns.pop(columns.index('longitude')))\n\ndf = df[columns]  # Reorder DataFrame\n\ndf.head(10)\n\n\n\n\n\n\n\n\nlast_name\nfirst_name\nage\nrace\nsex\nstate\ncounty\nlatitude\nlongitude\nworst_crime_display\nsentence\nsentence_in_years\nposting_date\nom\ndate_of_exoneration\ndate_of_1st_conviction\ndate_of_release\nyears_lost\narson\nbitemark\nco_defendant_confessed\nconviction_integrity_unit\nchild_sex_abuse_hysteria_case\nchild_victim\nfemale_exoneree\nfederal_case\nhomicide\ninnocence_organization\njailhouse_informant\njuvenile_defendant\nmisdemeanor\nno_crime_case\nguilty_plea_case\nposthumous_exoneration\nsexual_assault\nshaken_baby_syndrome_case\nprosecutor_misconduct\npolice_officer_misconduct\nforensic_analyst_misconduct\nchild_welfare_worker_misconduct\nwithheld_exculpatory_evidence\nmisconduct_that_is_not_withholding_evidence\nknowingly_permitting_perjury\nwitness_tampering_or_misconduct_interrogating_co_defendant\nmisconduct_in_interrogation_of_exoneree\nperjury_by_official\nprosecutor_lied_in_court\ntag_sum\ngeocode_address\n\n\n\n\n0\nAbbott\nCinque\n19.0\nBlack\nmale\nIllinois\nCook\n41.819738\n-87.756525\nDrug Possession or Sale\nProbation\n0.0\n2022-02-14\n1\n2022-02-01\n2008-03-25\n2008-03-25\n0.00\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n7\nCook County, Illinois, United States\n\n\n1\nAbernathy\nChristopher\n17.0\nWhite\nmale\nIllinois\nCook\n41.819738\n-87.756525\nMurder\nLife without parole\n100.0\n2015-02-13\n1\n2015-02-11\n1987-01-15\n2015-02-11\n28.07\n0\n0\n0\n1\n0\n1\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n1\n0\n0\n1\n0\n0\n10\nCook County, Illinois, United States\n\n\n2\nAbrego\nEruby\n20.0\nHispanic\nmale\nIllinois\nCook\n41.819738\n-87.756525\nMurder\n90 years\n90.0\n2022-08-25\n1\n2022-07-21\n2004-09-22\n2022-07-21\n17.83\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n1\n1\n1\n0\n9\nCook County, Illinois, United States\n\n\n3\nAdams\nDemetris\n22.0\nBlack\nmale\nIllinois\nCook\n41.819738\n-87.756525\nDrug Possession or Sale\n1 year\n1.0\n2020-04-13\n1\n2020-02-11\n2004-09-08\n2004-12-26\n0.30\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n7\nCook County, Illinois, United States\n\n\n4\nAdams\nKenneth\n22.0\nBlack\nmale\nIllinois\nCook\n41.819738\n-87.756525\nMurder\n75 years\n75.0\n2011-08-29\n1\n1996-07-02\n1978-10-20\n1996-06-14\n17.65\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n1\n0\n1\n1\n0\n0\n1\n1\n1\n1\n0\n0\n0\n11\nCook County, Illinois, United States\n\n\n5\nAdams\nSeneca\n20.0\nBlack\nmale\nIllinois\nCook\n41.819738\n-87.756525\nAssault\nProbation\n0.0\n2014-12-08\n1\n2006-12-19\n2006-05-18\n2006-05-18\n0.00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n1\n0\n6\nCook County, Illinois, United States\n\n\n6\nAdams\nTari\n18.0\nBlack\nmale\nIllinois\nCook\n41.819738\n-87.756525\nAssault\nProbation\n0.0\n2014-12-08\n1\n2006-12-19\n2006-05-18\n2006-05-18\n0.00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n1\n0\n6\nCook County, Illinois, United States\n\n\n7\nAgnew\nGregory\n28.0\nBlack\nmale\nIllinois\nLake\n42.332738\n-87.993955\nRobbery\n30 years\n30.0\n2018-01-18\n0\n2001-11-07\n1988-06-14\n2001-11-07\n13.40\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\nLake County, Illinois, United States\n\n\n8\nAguirre\nOmar\n28.0\nHispanic\nmale\nIllinois\nCook\n41.819738\n-87.756525\nMurder\n55 years\n55.0\n2011-08-29\n1\n2003-02-18\n1999-03-09\n2002-12-18\n3.78\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n1\n1\n0\n0\n7\nCook County, Illinois, United States\n\n\n9\nAli\nChauncey\n37.0\nBlack\nmale\nIllinois\nCook\n41.819738\n-87.756525\nDrug Possession or Sale\n1 year\n1.0\n2020-04-13\n1\n2020-02-11\n2007-01-17\n2007-06-27\n0.44\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n7\nCook County, Illinois, United States\n\n\n\n\n\n\n\n\n\n\nThe final cleaned dataset is saved as illinois_exoneration_data.csv, ensuring that all preprocessing steps are reproducible, and the dataset can be used consistently across various analysis stages.\n\ndf.to_csv('../../data/processed-data/illinois_exoneration_data.csv', index=False)\nprint(\"Data saved to 'illinois_exoneration_data.csv'\")\n\nData saved to 'illinois_exoneration_data.csv'"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#introduction-and-motivation",
    "href": "technical-details/data-cleaning/main.html#introduction-and-motivation",
    "title": "Data Cleaning",
    "section": "",
    "text": "This section outlines the steps taken to clean and preprocess the U.S. exoneration dataset, with a specific focus on Illinois cases. The cleaning process organizes the raw data into a structured and usable format for exploratory data analysis (EDA) and subsequent analysis workflows. By the end of this phase, the dataset will be well-structured, free of inconsistencies, and ready for further exploratory data analysis and machine learning workflows."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#initial-overview-of-the-dataset",
    "href": "technical-details/data-cleaning/main.html#initial-overview-of-the-dataset",
    "title": "Data Cleaning",
    "section": "",
    "text": "The dataset is first loaded to inspect its structure and contents. The purpose of this step is to get an initial sense of the data types, potential missing values, and the distribution of key variables which helps inform the cleaning steps required to make the data consistent and analyzable.\n\n# Import necessary Libraries:\nimport pandas as pd  # Used for data management, exploration, and manipulation\nimport numpy as np  # Used for numerical operations and array-based data processing\nimport seaborn as sns  # Used for data visualization, especially for missing values\nimport matplotlib.pyplot as plt  # Used for plotting and visualizing data\nimport re  # Used for handling and processing regular expressions, e.g., date cleaning\n\n# Load exoneration dataset:\ndf = pd.read_csv('../../data/raw-data/US_exoneration_data.csv')\nprint(\"Initial Dataset: \")\npd.set_option('display.max_columns', None) # Enables display of every column\ndf.head()\n\nInitial Dataset: \n\n\n\n\n\n\n\n\n\nLast Name\nFirst Name\nAge\nRace\nSex\nState\nCounty\nTags\nWorst Crime Display\nSentence\nPosting Date\nOM Tags\nF/MFE\nFC\nILD\nP/FA\nDNA\nMWID\nOM\nDate of Exoneration\nDate of 1st Conviction\nDate of Release\n\n\n\n\n0\nAbbitt\nJoseph\n31.0\nBlack\nMale\nNorth Carolina\nForsyth\nCV;#IO;#SA\nChild Sex Abuse\nLife\n9/1/11\nNaN\nNaN\nNaN\nNaN\nNaN\nDNA\nMWID\nNaN\n9/2/09\n6/22/95\n9/2/09\n\n\n1\nAbbott\nCinque\n19.0\nBlack\nMale\nIllinois\nCook\nCIU;#IO;#NC;#P\nDrug Possession or Sale\nProbation\n2/14/22\nOF;#WH;#NW\nNaN\nNaN\nNaN\nP/FA\nNaN\nNaN\nOM\n2/1/22\n3/25/08\n3/25/08\n\n\n2\nAbdal\nWarith Habib\n43.0\nBlack\nMale\nNew York\nErie\nIO;#SA\nSexual Assault\n20 to Life\n8/29/11\nOF;#WH;#NW;#WT\nF/MFE\nNaN\nNaN\nNaN\nDNA\nMWID\nOM\n9/1/99\n6/6/83\n9/1/99\n\n\n3\nAbernathy\nChristopher\n17.0\nWhite\nMale\nIllinois\nCook\nCIU;#CV;#H;#IO;#JV;#SA\nMurder\nLife without parole\n2/13/15\nOF;#WH;#NW;#INT\nNaN\nFC\nNaN\nP/FA\nDNA\nNaN\nOM\n2/11/15\n1/15/87\n2/11/15\n\n\n4\nAbney\nQuentin\n32.0\nBlack\nMale\nNew York\nNew York\nCV\nRobbery\n20 to Life\n5/13/19\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nMWID\nNaN\n1/19/12\n3/20/06\n1/19/12"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#subsetting-the-data-illinois",
    "href": "technical-details/data-cleaning/main.html#subsetting-the-data-illinois",
    "title": "Data Cleaning",
    "section": "",
    "text": "Before diving into the broader data cleaning process, I decided to narrow the scope of my research question to Illinois. This choice was intentional to focus the analysis on a specific region, ensuring that the findings are both relevant and manageable within the scope of this project. Illinois was selected because of its extensive record of exoneration cases, particularly in Cook Count (Chicago), which provides a valuable dataset for analyzing systemic issues within the criminal justice system. Chicago, in particular, has long been associated with significant racial disparities and deeply entrenched problems in policing and prosecution, making it a critical focal point for this analysis1. By focusing on Illinois, the dataset remains consistent in terms of jurisdictional laws and practices, allowing for a more accurate and concentrated exploration of patterns and trends in over-policing and wrongful convictions. This regional focus highlights the broader systemic failures of the criminal justice system while enabling a detailed examination of one of the most historically inequitable jurisdictions in terms of racial justice.\n\n\nTo isolate Illinois cases, the dataset was filtered by the state column, retaining only rows where the value matched “Illinois.” This step reduced the dataset to 548 rows, making it more manageable for analysis and visualization. Below is a preview of the filtered dataset:\n\n# Filter Data for Illinois: \ndf = df[df['State'] == 'Illinois']\nprint(\"Number of exonerees for Illinois subset: \" , df.shape[0]) \ndf.head()\n\nNumber of exonerees for Illinois subset:  548\n\n\n\n\n\n\n\n\n\nLast Name\nFirst Name\nAge\nRace\nSex\nState\nCounty\nTags\nWorst Crime Display\nSentence\nPosting Date\nOM Tags\nF/MFE\nFC\nILD\nP/FA\nDNA\nMWID\nOM\nDate of Exoneration\nDate of 1st Conviction\nDate of Release\n\n\n\n\n1\nAbbott\nCinque\n19.0\nBlack\nMale\nIllinois\nCook\nCIU;#IO;#NC;#P\nDrug Possession or Sale\nProbation\n2/14/22\nOF;#WH;#NW\nNaN\nNaN\nNaN\nP/FA\nNaN\nNaN\nOM\n2/1/22\n3/25/08\n3/25/08\n\n\n3\nAbernathy\nChristopher\n17.0\nWhite\nMale\nIllinois\nCook\nCIU;#CV;#H;#IO;#JV;#SA\nMurder\nLife without parole\n2/13/15\nOF;#WH;#NW;#INT\nNaN\nFC\nNaN\nP/FA\nDNA\nNaN\nOM\n2/11/15\n1/15/87\n2/11/15\n\n\n5\nAbrego\nEruby\n20.0\nHispanic\nMale\nIllinois\nCook\nCDC;#H;#IO\nMurder\n90 years\n8/25/22\nOF;#WH;#NW;#WT;#INT;#PJ\nNaN\nFC\nNaN\nP/FA\nNaN\nMWID\nOM\n7/21/22\n9/22/04\n7/21/22\n\n\n10\nAdams\nDemetris\n22.0\nBlack\nMale\nIllinois\nCook\nCIU;#IO;#NC;#P\nDrug Possession or Sale\n1 year\n4/13/20\nOF;#WH;#NW\nNaN\nNaN\nNaN\nP/FA\nNaN\nNaN\nOM\n2/11/20\n9/8/04\n12/26/04\n\n\n15\nAdams\nKenneth\n22.0\nBlack\nMale\nIllinois\nCook\nCDC;#H;#IO;#JI;#SA\nMurder\n75 years\n8/29/11\nPR;#OF;#WH;#NW;#KP;#WT\nF/MFE\nNaN\nNaN\nP/FA\nDNA\nMWID\nOM\n7/2/96\n10/20/78\n6/14/96"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#handling-missing-data",
    "href": "technical-details/data-cleaning/main.html#handling-missing-data",
    "title": "Data Cleaning",
    "section": "",
    "text": "Missing values are identified using isnull() to determine their extent and distribution across the dataset. The goal is to ensure that no critical data gaps remain unaddressed before proceeding with analysis.\n\n# Managing Missing Data - Identifying which columns have a lot of missing data:\nna_counts = df.isna().sum()\nprint(na_counts)\n\nLast Name                   0\nFirst Name                  0\nAge                         1\nRace                        0\nSex                         0\nState                       0\nCounty                      0\nTags                       15\nWorst Crime Display         0\nSentence                    0\nPosting Date                0\nOM Tags                    70\nF/MFE                     474\nFC                        410\nILD                       440\nP/FA                       67\nDNA                       482\nMWID                      442\nOM                         70\nDate of Exoneration         0\nDate of 1st Conviction      0\nDate of Release             0\ndtype: int64\n\n\n\n\n\n\n\nThe following columns were removed due to excessive missing data:\n\nF/MFE, ILD, P/FA, DNA, MWID, FC: Each of these columns had more than 50% missing values, which made them unreliable for meaningful analysis. Removing them ensures the dataset remains robust and manageable without introducing bias from imputation.\n\nRetaining “OM” and “OM Tags” Columns Initially, I removed the OM (Official Misconduct) and OM Tags columns, assuming their information would be captured in the general Tags column. However, during the exploratory data analysis (EDA), I discovered that these columns contained unique and valuable insights not present in the Tags column; as a result I retained them for further analysis.\n\n# Drop columns with excessive missing values: \ndf_original = df.copy()\ndf.drop(columns = ['F/MFE', 'ILD', 'P/FA', 'DNA', 'MWID', 'FC'], inplace = True)\n\n\n\n\nTo better understand the distribution of missing values, a heatmap is generated. This visualization provides a clear overview of where missing values occur, helping to decide which columns or rows to address in subsequent steps.\n\n\nA heatmap is generated to visualize the extent of missing data before cleaning. Columns with a high proportion of missing values are easily identifiable, providing a clear justification for their removal.\n\n# Heatmap of missing data before cleaning:\nplt.figure(figsize=(12, 6))\nsns.heatmap(df_original.isnull(), cbar=False, cmap='viridis')\nplt.title('Heatmap of Missing Data (Before Cleaning)')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nA second heatmap is generated after cleaning to confirm that all unnecessary columns with excessive missing values have been removed. This ensures the dataset is now complete and ready for further analysis.\n\n# Heatmap of missing data after cleaning:\nplt.figure(figsize=(12, 6))\nsns.heatmap(df.isnull(), cbar=False, cmap='viridis')\nplt.title('Heatmap of Missing Data (After Cleaning)')\nplt.show()"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#column-standardization",
    "href": "technical-details/data-cleaning/main.html#column-standardization",
    "title": "Data Cleaning",
    "section": "",
    "text": "To ensure consistency and simplify future operations, all column names were standardized by converting them to lowercase and replacing spaces with underscores (_). This transformation enhances readability, aligns with Python’s naming conventions, and makes column names easier to reference in code. For example, a column originally labeled First Name is now first_name.\n\n# Standardize column names by converting to lowercase and replacing spaces with '_':\ndf.columns = df.columns.str.lower().str.replace(' ', '_')\nprint(df.columns)\n\nIndex(['last_name', 'first_name', 'age', 'race', 'sex', 'state', 'county',\n       'tags', 'worst_crime_display', 'sentence', 'posting_date', 'om_tags',\n       'om', 'date_of_exoneration', 'date_of_1st_conviction',\n       'date_of_release'],\n      dtype='object')\n\n\n\n\nAdditionally, the sex column was converted to lowercase to maintain uniformity across textual data. This step ensures that values like “Male” and “male” are treated equivalently during analysis, reducing potential discrepancies caused by case sensitivity.\n\n# Convert sex values to lowercase: \ndf['sex'] = df['sex'].str.lower()\ndf['sex'].head()\n\n1     male\n3     male\n5     male\n10    male\n15    male\nName: sex, dtype: object"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#data-type-correction-and-formatting",
    "href": "technical-details/data-cleaning/main.html#data-type-correction-and-formatting",
    "title": "Data Cleaning",
    "section": "",
    "text": "Accurate data type formatting is essential for effective analysis. This section ensures that all variables are correctly identified as numerical, categorical, or date-time types so that they are ready for further processing.\n\n# Display data types for each column:\nprint(df.dtypes)\n\nlast_name                  object\nfirst_name                 object\nage                       float64\nrace                       object\nsex                        object\nstate                      object\ncounty                     object\ntags                       object\nworst_crime_display        object\nsentence                   object\nposting_date               object\nom_tags                    object\nom                         object\ndate_of_exoneration        object\ndate_of_1st_conviction     object\ndate_of_release            object\ndtype: object\n\n\n\n\nUpon reviewing the data types, it was noted that several columns, such as Last Name, First Name, Race, State, County, and Worst Crime Display, were classified as object. While this is acceptable for textual data, converting these columns to string ensures consistency and prevents potential issues when performing text-specific operations. This transformation also allows for better optimization and clarity in the data processing pipeline. The changes were necessary to standardize the dataset and ensure compatibility with downstream analysis tasks.\n\n# Convert relevant columns to string type\nstring_columns = ['last_name', 'first_name', 'race', 'sex', 'state', 'county', 'worst_crime_display']\ndf[string_columns] = df[string_columns].astype('string')\n\n# Check updated data types\nprint(df.dtypes)\n\nlast_name                 string[python]\nfirst_name                string[python]\nage                              float64\nrace                      string[python]\nsex                       string[python]\nstate                     string[python]\ncounty                    string[python]\ntags                              object\nworst_crime_display       string[python]\nsentence                          object\nposting_date                      object\nom_tags                           object\nom                                object\ndate_of_exoneration               object\ndate_of_1st_conviction            object\ndate_of_release                   object\ndtype: object\n\n\n\n\n\nAll date columns (posting_date, date_of_exoneration, date_of_1st_conviction, date_of_release) are converted to datetime format. This transformation ensures consistency and allows for easier time-based calculations, such as measuring the time between conviction and exoneration.\n\n# Convert date columns into datetime format:\nfor col in ['posting_date', 'date_of_exoneration', 'date_of_1st_conviction', 'date_of_release']:\n    # Convert with explicit format (MM/DD/YY):\n    df[col] = pd.to_datetime(df[col], format='%m/%d/%y', errors='coerce')\n\nprint(df[['posting_date', 'date_of_exoneration', 'date_of_1st_conviction', 'date_of_release']].head())\n\n   posting_date date_of_exoneration date_of_1st_conviction date_of_release\n1    2022-02-14          2022-02-01             2008-03-25      2008-03-25\n3    2015-02-13          2015-02-11             1987-01-15      2015-02-11\n5    2022-08-25          2022-07-21             2004-09-22      2022-07-21\n10   2020-04-13          2020-02-11             2004-09-08      2004-12-26\n15   2011-08-29          1996-07-02             1978-10-20      1996-06-14\n\n\n\n# Verify updated data types:\nprint(df.dtypes)\n\nlast_name                 string[python]\nfirst_name                string[python]\nage                              float64\nrace                      string[python]\nsex                       string[python]\nstate                     string[python]\ncounty                    string[python]\ntags                              object\nworst_crime_display       string[python]\nsentence                          object\nposting_date              datetime64[ns]\nom_tags                           object\nom                                object\ndate_of_exoneration       datetime64[ns]\ndate_of_1st_conviction    datetime64[ns]\ndate_of_release           datetime64[ns]\ndtype: object\n\n\n\n\n\nThe sentence column contains textual descriptions of sentencing outcomes, including terms like “Life without parole,” “Death,” or a specified number of years. To facilitate analysis, this column is transformed into a numerical format (sentence_in_years) by converting life sentences and probation to placeholder values and handling ranges or mixed units (e.g., years and months).\n\n# Print unique sentencing values for a better idea on how to best clean column: \nunique_sentences = df['sentence'].unique()\nprint(unique_sentences)\n\n['Probation' 'Life without parole' '90 years' '1 year' '75 years'\n '30 years' '55 years' '2 years' '3 years' '6 years' '45 years'\n '1 year and 6 months' '50 years' '60 years' 'Life' '80 years' '18 years'\n '4 years' '85 years' '20 years' '35 years' '2 years and 6 months'\n '82 years' '12 years' 'Not sentenced' '22 years' '32 years' 'Death'\n '5 years' '40 years' '25 years' '26 years' '4 years and 6 months'\n '9 years' '48 years' '30 days' '84 years' '3 months and 25 days'\n '2 years and 2 months' '3 months' '44 years' '6 months' '25 to 50 years'\n '29 years' '23 years' '31 years' '11 years' '8 years' '24 years'\n '3 years and four months' '42 years' '3 years and 6 months' '65 years'\n '76 years' '15 years' '50 to Life' '86 years' '70 years' '28 years'\n '13 years' '47 years' '36 years' '18 months' '1 year and 4 months'\n '8 years and 6 months' '6 years and 6 months' '58 years' '95 years'\n '7 years' '34 years' '62 years' '27 years' '69 years' '57 years'\n '50 to 100 years' '4 months' '4 years and 3 months' '37 years' '10 years'\n '67 years' '46 years' '17 years' '10 to 22 years' '6 years and 7 months'\n '5 years and 6 months' '2 Years']\n\n\n\n\nThe sentence column is cleaned to convert textual descriptions into numerical values: 1. Probation and Not Sentenced are set to 0. 2. Life sentences and the death penalty are represented as 100 for placeholder analysis. 3. Ranges (e.g., “25 to 50 years”) are averaged to a single value. 4. Years and months are combined into total years for uniformity.\nThis standardization facilitates meaningful comparisons and quantitative analysis of sentencing patterns.\n\ndef clean_sentence(value):\n    \"\"\" Cleans the 'sentence' column values to numeric years for numerical EDA \n    - Probation is represented as 0.\n    - 'Not sentenced' is converted to np.nan.\n    - 'Life' and 'Death' sentences are represented as 100 (placeholder).\n    - Years and months are converted to a numeric value in years. \"\"\"\n\n    if value == 'Probation':\n        return 0\n    elif value == 'Not sentenced':\n        return np.nan  # NaN for not sentenced\n    elif 'Life' in value or value == 'Death':\n        return 100  # Placeholder for life sentences or death penalty\n    elif 'year' in value or 'month' in value:\n\n        # Handles ranges like '25 to 50 years'\n        if 'to' in value:\n            years = [int(num) for num in re.findall(r'\\d+', value)]\n            return sum(years) / len(years)  # Average the range \n        \n        # Handle \"X years and Y months\"\n        elif 'and' in value:\n            numbers = [float(num) for num in re.findall(r'\\d+', value)]\n            if len(numbers) == 2:  # Both years and months are present\n                years, months = numbers\n                return years + (months / 12)  # Convert months to years\n            elif len(numbers) == 1:  # Only one number is present\n                return numbers[0]  # Treat it as years\n            \n        # Handle only months or only years\n        elif 'months' in value:\n            months = int(re.search(r'\\d+', value).group())\n            return months / 12  # Convert months to years\n        else:  # Only years\n            return int(re.search(r'\\d+', value).group())\n    else:\n        return np.nan  # Anything unexpected as None\n    \ndf['sentence_in_years'] = df['sentence'].apply(clean_sentence)\n\n# Check results\ndf[['sentence', 'sentence_in_years']].head(10)\n\n\n\n\n\n\n\n\nsentence\nsentence_in_years\n\n\n\n\n1\nProbation\n0.0\n\n\n3\nLife without parole\n100.0\n\n\n5\n90 years\n90.0\n\n\n10\n1 year\n1.0\n\n\n15\n75 years\n75.0\n\n\n21\nProbation\n0.0\n\n\n22\nProbation\n0.0\n\n\n24\n30 years\n30.0\n\n\n25\n55 years\n55.0\n\n\n45\n1 year\n1.0\n\n\n\n\n\n\n\n\n# Check updated data types\nprint(df.dtypes)\n\nlast_name                 string[python]\nfirst_name                string[python]\nage                              float64\nrace                      string[python]\nsex                       string[python]\nstate                     string[python]\ncounty                    string[python]\ntags                              object\nworst_crime_display       string[python]\nsentence                          object\nposting_date              datetime64[ns]\nom_tags                           object\nom                                object\ndate_of_exoneration       datetime64[ns]\ndate_of_1st_conviction    datetime64[ns]\ndate_of_release           datetime64[ns]\nsentence_in_years                float64\ndtype: object\n\n\nAfter transforming the sentence column into a numerical format, the new column, sentence_in_years is now represented as a float64, which aligns with the desired structure for numerical analysis. This conversion allows for quantitative exploration of the sentencing data, such as aggregations and comparisons, during later stages of analysis. The original sentence column is retained for reference purposes, as it preserves the detailed textual descriptions that might be useful for contextual insights. The tags, OM, and OM_tags columns will be addressed later, so for now the datatype may remain as an object."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#cleaning-the-tags-column-update-this",
    "href": "technical-details/data-cleaning/main.html#cleaning-the-tags-column-update-this",
    "title": "Data Cleaning",
    "section": "",
    "text": "The tags column contains important categorical information about each exoneration case. To make this data more useful for analysis, I transformed the column into multiple binary columns, where each tag indicates the presence (1) or absence (0) of a specific feature. Additionally, I created a tag_sum column to capture the total number of tags associated with each case, providing a quick summary metric.\nThe cleaning process involved the following steps:\n\nRemoving Unnecessary Characters: Unwanted characters such as # were removed, and delimiters were standardized to ensure consistency in the data.\nSplitting Tags: The tags column was split into individual values to facilitate binary encoding.\nRenaming Binary Columns: Each binary column was renamed using clear and descriptive labels by mapping the original tags to their definitions. This mapping process translated short tag codes into their full meanings, improving interpretability. For reference, the definitions of the tags are based on the descriptions provided on the National Registry of Exonerations: Exoneration Detail List .\nAdding a tag_sum Column: A new column was created to calculate the total number of tags for each case, enabling easier analysis of case complexity.\n\nThis transformation ensures the data is well-structured and ready for exploratory analysis, providing detailed insights into the systemic patterns in exoneration cases.\n\n# Clean 'tags' column\ndf['tags'] = df['tags'].str.replace('#', '', regex=False).str.replace(\";\", \",\")\ndf['OM-tags'] = df['om_tags'].str.replace('#', '', regex=False).str.replace(\";\", \",\")\n\n# Define the mapping for tags\ntag_mapping = {\n    \"A\": \"arson\",\n    \"BM\": \"bitemark\",\n    \"CDC\": \"co_defendant_confessed\",\n    \"CIU\": \"conviction_integrity_unit\",\n    \"CSH\": \"child_sex_abuse_hysteria_case\",\n    \"CV\": \"child_victim\",\n    \"F\": \"female_exoneree\",\n    \"FED\": \"federal_case\",\n    \"H\": \"homicide\",\n    \"IO\": \"innocence_organization\",\n    \"JI\": \"jailhouse_informant\",\n    \"JV\": \"juvenile_defendant\",\n    \"M\": \"misdemeanor\",\n    \"NC\": \"no_crime_case\",\n    \"P\": \"guilty_plea_case\",\n    \"PH\": \"posthumous_exoneration\",\n    \"SA\": \"sexual_assault\",\n    \"SBS\": \"shaken_baby_syndrome_case\",\n    \"PR\": \"prosecutor_misconduct\",\n    \"OF\": \"police_officer_misconduct\",\n    \"FA\": \"forensic_analyst_misconduct\",\n    \"CW\": \"child_welfare_worker_misconduct\",\n    \"WH\": \"withheld_exculpatory_evidence\",\n    \"NW\": \"misconduct_that_is_not_withholding_evidence\",\n    \"KP\": \"knowingly_permitting_perjury\",\n    \"WT\": \"witness_tampering_or_misconduct_interrogating_co_defendant\",\n    \"INT\": \"misconduct_in_interrogation_of_exoneree\",\n    \"PJ\": \"perjury_by_official\",\n    \"PL\": \"prosecutor_lied_in_court\"\n}\n\n# Split 'tags' and 'OM-tags' into lists\ndf['tags'] = df['tags'].apply(lambda x: x.split(',') if isinstance(x, str) else x)\ndf['OM-tags'] = df['OM-tags'].apply(lambda x: x.split(',') if isinstance(x, str) else x)\n\n# Create binary columns for tags from both 'tags' and 'OM-tags'\nfor tag in tag_mapping.keys():\n    # Check if the tag exists in 'tags' or 'OM-tags'\n    df[tag] = df.apply(\n        lambda row: 1 if (isinstance(row['tags'], list) and tag in row['tags']) or \n                          (isinstance(row['OM-tags'], list) and tag in row['OM-tags']) else 0,\n        axis=1\n    )\n\n# Rename the binary columns using the tag_mapping dictionary\ndf.rename(columns=tag_mapping, inplace=True)\n\n# Create `tag_sum` column to count the total number of tags for each exoneree\ndf['tag_sum'] = df[list(tag_mapping.values())].sum(axis=1)\n\n# Drop the original 'tags' and 'OM-tags' columns\ndf.drop(columns=['tags', 'om_tags', 'OM-tags'], inplace=True)\n\ndf.head()  # Preview the updated DataFrame\n\n\n\n\n\n\n\n\nlast_name\nfirst_name\nage\nrace\nsex\nstate\ncounty\nworst_crime_display\nsentence\nposting_date\nom\ndate_of_exoneration\ndate_of_1st_conviction\ndate_of_release\nsentence_in_years\narson\nbitemark\nco_defendant_confessed\nconviction_integrity_unit\nchild_sex_abuse_hysteria_case\nchild_victim\nfemale_exoneree\nfederal_case\nhomicide\ninnocence_organization\njailhouse_informant\njuvenile_defendant\nmisdemeanor\nno_crime_case\nguilty_plea_case\nposthumous_exoneration\nsexual_assault\nshaken_baby_syndrome_case\nprosecutor_misconduct\npolice_officer_misconduct\nforensic_analyst_misconduct\nchild_welfare_worker_misconduct\nwithheld_exculpatory_evidence\nmisconduct_that_is_not_withholding_evidence\nknowingly_permitting_perjury\nwitness_tampering_or_misconduct_interrogating_co_defendant\nmisconduct_in_interrogation_of_exoneree\nperjury_by_official\nprosecutor_lied_in_court\ntag_sum\n\n\n\n\n1\nAbbott\nCinque\n19.0\nBlack\nmale\nIllinois\nCook\nDrug Possession or Sale\nProbation\n2022-02-14\nOM\n2022-02-01\n2008-03-25\n2008-03-25\n0.0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n7\n\n\n3\nAbernathy\nChristopher\n17.0\nWhite\nmale\nIllinois\nCook\nMurder\nLife without parole\n2015-02-13\nOM\n2015-02-11\n1987-01-15\n2015-02-11\n100.0\n0\n0\n0\n1\n0\n1\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n1\n0\n0\n1\n0\n0\n10\n\n\n5\nAbrego\nEruby\n20.0\nHispanic\nmale\nIllinois\nCook\nMurder\n90 years\n2022-08-25\nOM\n2022-07-21\n2004-09-22\n2022-07-21\n90.0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n1\n1\n1\n0\n9\n\n\n10\nAdams\nDemetris\n22.0\nBlack\nmale\nIllinois\nCook\nDrug Possession or Sale\n1 year\n2020-04-13\nOM\n2020-02-11\n2004-09-08\n2004-12-26\n1.0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n7\n\n\n15\nAdams\nKenneth\n22.0\nBlack\nmale\nIllinois\nCook\nMurder\n75 years\n2011-08-29\nOM\n1996-07-02\n1978-10-20\n1996-06-14\n75.0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n1\n0\n1\n1\n0\n0\n1\n1\n1\n1\n0\n0\n0\n11\n\n\n\n\n\n\n\n\n# Convert 'OM' column to binary (1 if \"OM\" is present, 0 otherwise)\ndf['om'] = df['om'].apply(lambda x: 1 if str(x).strip().upper() == \"OM\" else 0)\n\n# Verify the transformation\nprint(df['om'].value_counts())\n\nom\n1    478\n0     70\nName: count, dtype: int64"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#county-geocoding",
    "href": "technical-details/data-cleaning/main.html#county-geocoding",
    "title": "Data Cleaning",
    "section": "",
    "text": "To enrich the dataset and enable more advanced geographic analysis, I incorporated geocoded data, including latitude, longitude, and full address (geocode). This addition allows for deeper exploratory data analysis (EDA) of geographic patterns within Illinois. By including location data, I can examine trends and disparities across counties, evaluate geographic clustering of exoneration cases, and explore how systemic factors vary by region. The geocoded data adds a crucial spatial dimension to the analysis, providing the foundation for mapping, visualizations, and further geographic exploration.\nTo perform the geocoding, I used GeoPy, a Python client for geocoding web services . GeoPy provides an easy-to-use interface for accessing geocoding services, including the ability to retrieve latitude, longitude, and full address information from place names.\n\n\nTo minimize redundant API calls and enhance efficiency, I focused on unique combinations of county and state. Instead of geocoding every record in the dataset (which would result in repeated calls for the same county, such as Cook County), I first extracted unique county-state pairs. These unique combinations were geocoded, capturing the address, latitude, and longitude for each pair with the results then mapped back to the full dataset based on county and state.\nThis approach not only reduced the number of API calls, saving both time and resources, but also ensured consistency in the geocoded results. By geocoding only unique counties,unnecessary repetition was avoided and a streamlined process was maintained while enriching the dataset with geographic context for further EDA.\n\n# Strip extra spaces and ensure consistent capitalization:\ndf['county'] = df['county'].str.strip().str.title()\ndf['state'] = df['state'].str.strip().str.title()\n\n# Initialize the geolocator:\ngeolocator = Nominatim(user_agent=\"illinois_exoneration_geocode\") \n\n# Get unique combinations of county and state - avoids repition and extra work for geocoder:\nunique_counties = df[['county', 'state']].drop_duplicates()\n\n#  Define a function to geocode unique counties:\ndef geocode_unique(row):\n    try:\n        #print(f\"Geocoding: {row['county']} County, {row['state']}, USA\") # Debugging\n        location = geolocator.geocode(f\"{row['county']} County, {row['state']}, USA\")\n        if location:\n            #print(f\"Success: {location.address}\") #debugging\n            return {\n                'address': location.address,\n                'latitude': location.latitude,\n                'longitude': location.longitude\n            }\n        else:\n            print(f\"Failed: No result for {row['county']} County, {row['state']}\")\n            return None\n    except Exception as e:\n        print(f\"Error geocoding {row['county']}, {row['state']}: {e}\")\n        return None\n\n# Apply the geocoding function to unique counties:\ngeocoded_results = unique_counties.apply(geocode_unique, axis=1)\n\n# Split results into separate columns:\nunique_counties['geocode_address'] = geocoded_results.apply(lambda x: x['address'] if isinstance(x, dict) and 'address' in x else None)\nunique_counties['latitude'] = geocoded_results.apply(lambda x: x['latitude'] if isinstance(x, dict) and 'latitude' in x else None)\nunique_counties['longitude'] = geocoded_results.apply(lambda x: x['longitude'] if isinstance(x, dict) and 'longitude' in x else None)\n\n# Merge the geocoded results back into the original dataset:\ndf = df.merge(unique_counties, on=['county', 'state'], how='left')\n\nprint(df[['state', 'county', 'geocode_address', 'latitude', 'longitude']].head())\n\n      state county                       geocode_address   latitude  longitude\n0  Illinois   Cook  Cook County, Illinois, United States  41.819738 -87.756525\n1  Illinois   Cook  Cook County, Illinois, United States  41.819738 -87.756525\n2  Illinois   Cook  Cook County, Illinois, United States  41.819738 -87.756525\n3  Illinois   Cook  Cook County, Illinois, United States  41.819738 -87.756525\n4  Illinois   Cook  Cook County, Illinois, United States  41.819738 -87.756525"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#column-reorganization",
    "href": "technical-details/data-cleaning/main.html#column-reorganization",
    "title": "Data Cleaning",
    "section": "",
    "text": "To improve readability and logical flow, the following changes were made to the column order:\n\nAlign Sentencing Data:\nThe sentence_in_years column was moved to appear immediately after sentence, ensuring that the cleaned numerical representation of sentencing data is logically aligned with its original textual description.\nReorganize Release and Years Lost:\nThe years_lost column was moved to appear immediately after date_of_release, facilitating easier comparison of release dates and the calculated time lost due to wrongful incarceration.\nGroup Geographic Data:\nThe latitude and longitude columns were moved to follow the county column, grouping geographic information together.\n\n\n# Reordering columns:\ncolumns = list(df.columns)  \n\n#Aligning sentencing data:  \ncolumns.insert(columns.index('sentence') + 1, columns.pop(columns.index('sentence_in_years')))  # Move 'sentence_in_years'\n\n#Reorganizing release and years lost\ncolumns.insert(columns.index('date_of_release') +1, columns.pop(columns.index('years_lost'))) #Move 'years_lost' \n\n# Move 'latitude' and 'longitude' after 'county'\ncolumns.insert(columns.index('county') + 1, columns.pop(columns.index('latitude')))\ncolumns.insert(columns.index('county') + 2, columns.pop(columns.index('longitude')))\n\ndf = df[columns]  # Reorder DataFrame\n\ndf.head(10)\n\n\n\n\n\n\n\n\nlast_name\nfirst_name\nage\nrace\nsex\nstate\ncounty\nlatitude\nlongitude\nworst_crime_display\nsentence\nsentence_in_years\nposting_date\nom\ndate_of_exoneration\ndate_of_1st_conviction\ndate_of_release\nyears_lost\narson\nbitemark\nco_defendant_confessed\nconviction_integrity_unit\nchild_sex_abuse_hysteria_case\nchild_victim\nfemale_exoneree\nfederal_case\nhomicide\ninnocence_organization\njailhouse_informant\njuvenile_defendant\nmisdemeanor\nno_crime_case\nguilty_plea_case\nposthumous_exoneration\nsexual_assault\nshaken_baby_syndrome_case\nprosecutor_misconduct\npolice_officer_misconduct\nforensic_analyst_misconduct\nchild_welfare_worker_misconduct\nwithheld_exculpatory_evidence\nmisconduct_that_is_not_withholding_evidence\nknowingly_permitting_perjury\nwitness_tampering_or_misconduct_interrogating_co_defendant\nmisconduct_in_interrogation_of_exoneree\nperjury_by_official\nprosecutor_lied_in_court\ntag_sum\ngeocode_address\n\n\n\n\n0\nAbbott\nCinque\n19.0\nBlack\nmale\nIllinois\nCook\n41.819738\n-87.756525\nDrug Possession or Sale\nProbation\n0.0\n2022-02-14\n1\n2022-02-01\n2008-03-25\n2008-03-25\n0.00\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n7\nCook County, Illinois, United States\n\n\n1\nAbernathy\nChristopher\n17.0\nWhite\nmale\nIllinois\nCook\n41.819738\n-87.756525\nMurder\nLife without parole\n100.0\n2015-02-13\n1\n2015-02-11\n1987-01-15\n2015-02-11\n28.07\n0\n0\n0\n1\n0\n1\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n1\n0\n0\n1\n0\n0\n10\nCook County, Illinois, United States\n\n\n2\nAbrego\nEruby\n20.0\nHispanic\nmale\nIllinois\nCook\n41.819738\n-87.756525\nMurder\n90 years\n90.0\n2022-08-25\n1\n2022-07-21\n2004-09-22\n2022-07-21\n17.83\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n1\n1\n1\n0\n9\nCook County, Illinois, United States\n\n\n3\nAdams\nDemetris\n22.0\nBlack\nmale\nIllinois\nCook\n41.819738\n-87.756525\nDrug Possession or Sale\n1 year\n1.0\n2020-04-13\n1\n2020-02-11\n2004-09-08\n2004-12-26\n0.30\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n7\nCook County, Illinois, United States\n\n\n4\nAdams\nKenneth\n22.0\nBlack\nmale\nIllinois\nCook\n41.819738\n-87.756525\nMurder\n75 years\n75.0\n2011-08-29\n1\n1996-07-02\n1978-10-20\n1996-06-14\n17.65\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n1\n0\n1\n1\n0\n0\n1\n1\n1\n1\n0\n0\n0\n11\nCook County, Illinois, United States\n\n\n5\nAdams\nSeneca\n20.0\nBlack\nmale\nIllinois\nCook\n41.819738\n-87.756525\nAssault\nProbation\n0.0\n2014-12-08\n1\n2006-12-19\n2006-05-18\n2006-05-18\n0.00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n1\n0\n6\nCook County, Illinois, United States\n\n\n6\nAdams\nTari\n18.0\nBlack\nmale\nIllinois\nCook\n41.819738\n-87.756525\nAssault\nProbation\n0.0\n2014-12-08\n1\n2006-12-19\n2006-05-18\n2006-05-18\n0.00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n1\n0\n6\nCook County, Illinois, United States\n\n\n7\nAgnew\nGregory\n28.0\nBlack\nmale\nIllinois\nLake\n42.332738\n-87.993955\nRobbery\n30 years\n30.0\n2018-01-18\n0\n2001-11-07\n1988-06-14\n2001-11-07\n13.40\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\nLake County, Illinois, United States\n\n\n8\nAguirre\nOmar\n28.0\nHispanic\nmale\nIllinois\nCook\n41.819738\n-87.756525\nMurder\n55 years\n55.0\n2011-08-29\n1\n2003-02-18\n1999-03-09\n2002-12-18\n3.78\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n1\n1\n0\n0\n7\nCook County, Illinois, United States\n\n\n9\nAli\nChauncey\n37.0\nBlack\nmale\nIllinois\nCook\n41.819738\n-87.756525\nDrug Possession or Sale\n1 year\n1.0\n2020-04-13\n1\n2020-02-11\n2007-01-17\n2007-06-27\n0.44\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n7\nCook County, Illinois, United States"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#exporting-the-cleaned-dataset",
    "href": "technical-details/data-cleaning/main.html#exporting-the-cleaned-dataset",
    "title": "Data Cleaning",
    "section": "",
    "text": "The final cleaned dataset is saved as illinois_exoneration_data.csv, ensuring that all preprocessing steps are reproducible, and the dataset can be used consistently across various analysis stages.\n\ndf.to_csv('../../data/processed-data/illinois_exoneration_data.csv', index=False)\nprint(\"Data saved to 'illinois_exoneration_data.csv'\")\n\nData saved to 'illinois_exoneration_data.csv'"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#summary-and-next-steps",
    "href": "technical-details/data-cleaning/main.html#summary-and-next-steps",
    "title": "Data Cleaning",
    "section": "",
    "text": "This data cleaning process ensures the Illinois exoneration dataset is free of inconsistencies and ready for analysis. The cleaned data will now be used to conduct EDA and investigate patterns of racial disparity and judicial misconduct in wrongful convictions."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html",
    "href": "technical-details/supervised-learning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results.\n\n\n\n\nNormalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling.\n\n\n\n\n\nModel Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used\n\n\n\n\n\nSplit Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset.\n\n\n\n\n\nBinary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc.\n\n\n\n\n\nModel Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots).\n\n\n\n\n\nResult Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#what-to-address",
    "href": "technical-details/supervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "href": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "title": "Instructions",
    "section": "",
    "text": "Normalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-selection",
    "href": "technical-details/supervised-learning/instructions.html#model-selection",
    "title": "Instructions",
    "section": "",
    "text": "Model Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "href": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "title": "Instructions",
    "section": "",
    "text": "Split Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "href": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "title": "Instructions",
    "section": "",
    "text": "Binary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#results",
    "href": "technical-details/supervised-learning/instructions.html#results",
    "title": "Instructions",
    "section": "",
    "text": "Model Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots)."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#discussion",
    "href": "technical-details/supervised-learning/instructions.html#discussion",
    "title": "Instructions",
    "section": "",
    "text": "Result Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/llm-usage-log.html",
    "href": "technical-details/llm-usage-log.html",
    "title": "LLM usage log",
    "section": "",
    "text": "This page can serve as a “catch-all” for LLM use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\nLLM tools were used in the following way for the tasks below"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#brainstorming",
    "href": "technical-details/llm-usage-log.html#brainstorming",
    "title": "LLM usage log",
    "section": "Brainstorming",
    "text": "Brainstorming\n\nI came up with my initial, broad project idea of over-policing and the criminalization of race and poverty as it’s a topic I’ve always been passionate about, however LLM helped me narrow down my topic to focus on a specific locality (e.g. Chicago)."
  },
  {
    "objectID": "technical-details/llm-usage-log.html#literature-review",
    "href": "technical-details/llm-usage-log.html#literature-review",
    "title": "LLM usage log",
    "section": "Literature Review:",
    "text": "Literature Review:\n\nLLM tools were used extensively to streamline the literature review process. Specifically, I utilized ChatGPT to:\n\nSummarize key points from academic papers and credible news articles, making it easier to integrate multiple perspectives on over-policing, socioeconomic factors, and racial disparities in the criminal justice system.\nExtract relevant data and highlight findings from each source, such as the impact of income inequality on crime rates, racial disparities in wrongful convictions, and community perceptions of policing.\nReorganize summaries and refine content to ensure a cohesive narrative for the literature review section, effectively condensing long, complex papers into concise, useful summaries that align with my project’s goals.\nSupport citation management by providing brief summaries for each source, which I then used to create a more structured references file (references.bib).\n\nUsing LLM for these tasks allowed me to efficiently review and synthesize a larger volume of literature, ensuring comprehensive coverage of critical studies relevant to my project’s focus on race, poverty, and policing in Chicago."
  },
  {
    "objectID": "technical-details/llm-usage-log.html#writing",
    "href": "technical-details/llm-usage-log.html#writing",
    "title": "LLM usage log",
    "section": "Writing:",
    "text": "Writing:\n\nReformating text from bulleted lists into proses\nProofreading\nText summarization for literature review"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#code",
    "href": "technical-details/llm-usage-log.html#code",
    "title": "LLM usage log",
    "section": "Code:",
    "text": "Code:\n\nCode commenting and explanatory documentation"
  },
  {
    "objectID": "technical-details/eda/draft_EDA_structure.html",
    "href": "technical-details/eda/draft_EDA_structure.html",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "The goal of this section is to perform a thorough exploratory data analysis (EDA) on the Illinois exoneration dataset. EDA allows me to better understand the data’s structure, uncover patterns, and identify any issues that may require additional cleaning or transformation. By systematically examining the dataset, I can ensure that future analyses are grounded in a solid understanding of the data.\n\n\n\n\n\n\n\n\nSummary Statistics: Calculate mean, median, and standard deviation for age and sentence_in_years.\nVisualization:\n\nUse histograms to visualize the distribution of age and sentence_in_years.\nOverlay density plots to understand the spread of the data.\n\nKey Insights: Look for trends such as whether certain age groups are overrepresented among exonerees or whether sentences tend to cluster around specific lengths (e.g., life sentences, short-term sentences).\n\n\n\n\n\n\nPurpose: Examine the tag_sum column to understand the distribution of the number of tags associated with each case.\nVisualization: Use a bar plot or histogram to visualize the counts.\nKey Insights: Identify whether cases with higher tag counts correlate with specific crimes or systemic issues.\n\n\n\n\n\n\n\n\nFrequency Counts: Calculate the counts and proportions for each category.\nVisualization:\n\nUse bar plots to visualize the distribution of race and sex.\nCreate a bar plot for the top 10 most common crimes in the worst_crime_display column.\n\nKey Insights: Look for overrepresentation of specific groups or patterns in the crimes leading to wrongful convictions.\n\n\n\n\n\n\n\n\n\n\n\n\nMethod: Use a correlation matrix to examine relationships between numerical variables.\nVisualization: Create a heatmap to highlight strong or weak correlations.\nKey Insights: Determine whether age or tag count correlates strongly with sentence length.\n\n\n\n\n\n\n\n\n\nPurpose: Explore the relationship between race and the types of crimes leading to exonerations.\nVisualization: Use a grouped bar plot or mosaic plot to show the distribution.\nKey Insights: Identify systemic biases in the criminal justice system, such as racial disparities in the types of crimes charged.\n\n\n\n\n\n\n\n\n\nAnalysis: Calculate skewness and kurtosis for sentence_in_years.\nVisualization: Use box plots to show the spread of data.\nTransformation: Apply log transformation or other techniques to handle extreme skewness, if needed.\n\n\n\n\n\n\n\n\n\n\nKey Visualizations:\n\nAge distribution among exonerees.\nBreakdown of crimes by race and sex.\nRelationship between sentence length and tag count.\n\nInterpretation: Annotate visualizations with clear explanations to connect them back to the project’s objectives.\n\n\n\n\n\n\n\n\n\nHighlight key trends, such as racial disparities, systemic biases in sentencing, or patterns in the number of tags.\n\n\n\n\n\nDiscuss how findings from EDA inform the next steps in analysis:\n\nFeatures to include in modeling (e.g., sentence_in_years, tag_sum, categorical encodings for race and sex).\nData transformations or further cleaning needed before modeling.\n\n\n\n\n\n\nTransition from EDA to feature engineering and modeling with a clear plan informed by the insights gathered here."
  },
  {
    "objectID": "technical-details/eda/draft_EDA_structure.html#introduction-and-motivation",
    "href": "technical-details/eda/draft_EDA_structure.html#introduction-and-motivation",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "The goal of this section is to perform a thorough exploratory data analysis (EDA) on the Illinois exoneration dataset. EDA allows me to better understand the data’s structure, uncover patterns, and identify any issues that may require additional cleaning or transformation. By systematically examining the dataset, I can ensure that future analyses are grounded in a solid understanding of the data."
  },
  {
    "objectID": "technical-details/eda/draft_EDA_structure.html#univariate-analysis",
    "href": "technical-details/eda/draft_EDA_structure.html#univariate-analysis",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Summary Statistics: Calculate mean, median, and standard deviation for age and sentence_in_years.\nVisualization:\n\nUse histograms to visualize the distribution of age and sentence_in_years.\nOverlay density plots to understand the spread of the data.\n\nKey Insights: Look for trends such as whether certain age groups are overrepresented among exonerees or whether sentences tend to cluster around specific lengths (e.g., life sentences, short-term sentences).\n\n\n\n\n\n\nPurpose: Examine the tag_sum column to understand the distribution of the number of tags associated with each case.\nVisualization: Use a bar plot or histogram to visualize the counts.\nKey Insights: Identify whether cases with higher tag counts correlate with specific crimes or systemic issues.\n\n\n\n\n\n\n\n\nFrequency Counts: Calculate the counts and proportions for each category.\nVisualization:\n\nUse bar plots to visualize the distribution of race and sex.\nCreate a bar plot for the top 10 most common crimes in the worst_crime_display column.\n\nKey Insights: Look for overrepresentation of specific groups or patterns in the crimes leading to wrongful convictions."
  },
  {
    "objectID": "technical-details/eda/draft_EDA_structure.html#bivariate-analysis",
    "href": "technical-details/eda/draft_EDA_structure.html#bivariate-analysis",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Method: Use a correlation matrix to examine relationships between numerical variables.\nVisualization: Create a heatmap to highlight strong or weak correlations.\nKey Insights: Determine whether age or tag count correlates strongly with sentence length.\n\n\n\n\n\n\n\n\n\nPurpose: Explore the relationship between race and the types of crimes leading to exonerations.\nVisualization: Use a grouped bar plot or mosaic plot to show the distribution.\nKey Insights: Identify systemic biases in the criminal justice system, such as racial disparities in the types of crimes charged.\n\n\n\n\n\n\n\n\n\nAnalysis: Calculate skewness and kurtosis for sentence_in_years.\nVisualization: Use box plots to show the spread of data.\nTransformation: Apply log transformation or other techniques to handle extreme skewness, if needed."
  },
  {
    "objectID": "technical-details/eda/draft_EDA_structure.html#data-visualization-and-storytelling",
    "href": "technical-details/eda/draft_EDA_structure.html#data-visualization-and-storytelling",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Key Visualizations:\n\nAge distribution among exonerees.\nBreakdown of crimes by race and sex.\nRelationship between sentence length and tag count.\n\nInterpretation: Annotate visualizations with clear explanations to connect them back to the project’s objectives."
  },
  {
    "objectID": "technical-details/eda/draft_EDA_structure.html#conclusions-and-next-steps",
    "href": "technical-details/eda/draft_EDA_structure.html#conclusions-and-next-steps",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Highlight key trends, such as racial disparities, systemic biases in sentencing, or patterns in the number of tags.\n\n\n\n\n\nDiscuss how findings from EDA inform the next steps in analysis:\n\nFeatures to include in modeling (e.g., sentence_in_years, tag_sum, categorical encodings for race and sex).\nData transformations or further cleaning needed before modeling.\n\n\n\n\n\n\nTransition from EDA to feature engineering and modeling with a clear plan informed by the insights gathered here."
  },
  {
    "objectID": "technical-details/data-collection/methods.html",
    "href": "technical-details/data-collection/methods.html",
    "title": "Methods",
    "section": "",
    "text": "The methods combined direct downloads, web scraping, and API-based geocoding to assemble a robust dataset for analysis. The exoneration and arrest datasets served as the foundation, while geocoding and additional scraping added valuable spatial and demographic context.\n\n\nThe Illinois arrest dataset was sourced from the Illinois Criminal Justice Information Authority’s (ICJIA) Arrest Explorer, a platform providing aggregate arrest data from the Criminal History Record Information (CHRI) system—a statewide resource for demographic and offense-related variables.1\nTo ensure privacy and confidentiality, ICJIA applied the following modifications:\n- Counts under 10 are approximated (e.g., 1 for counts 0–4, 6 for counts 5–9),\n- Subtotals, such as arrests by race or county, are accurate within +1/-1, and\n- Statewide totals align exactly with the CHRI database at the time of retrieval, which occurs twice annually.\nFurther, the dataset excludes juvenile arrests, class C misdemeanors, and cases with missing demographic details. For this project, the data was first filtered by race, county, and year, and then downloaded directly to examine patterns relevant to my analysis.2\n\n\n\nThe exoneration dataset was downloaded directly from the National Registry of Exonerations, which collects and publishes searchable, online statistical data and case details for known exonerations of innocent criminal defendants in the United States from 1989 to the present.3\nThe Registry defines exonerations as cases where a person, following new evidence of innocence, is officially cleared through actions like factual declarations of innocence, pardons, or the dismissal/acquittal of charges.4\nTo access the data, a spreadsheet request form had to be submitted, and the dataset was provided under specific conditions to ensure its proper use:\n\nNo retransmission: The spreadsheet or any substantial portion of it cannot be shared with anyone who has not agreed to the conditions.\n\nAdvance notice: The National Registry must be informed in advance of any publication or distribution of data derived from the spreadsheet.\n\nCorrections and additions: Recipients agree to report any errors or missing data they identify to the Registry.\n\n\n\n\nThe population and incarceration data for Illinois counties were obtained by scraping Prison Policy Initiative’s website which provides information on total population and incarcerated populations broken down by race for counties across the United States5.\nTo extract the data, the requests library was used to retrieve the webpage’s HTML content, and BeautifulSoup was employed to parse the HTML and locate the relevant table which was then converted into a Pandas DataFrame for cleaning and analysis. Here is the code used:"
  },
  {
    "objectID": "technical-details/data-collection/closing.html",
    "href": "technical-details/data-collection/closing.html",
    "title": "Summary",
    "section": "",
    "text": "The foundation for meaningful data analysis is accurate and reliable crime data, yet long-standing challenges in criminal history record systems continue to undermine their precision and dependability1. As outlined in the Use and Management of Criminal History Record Information Report (1993), issues such as incomplete data reporting, delays in recording arrests and case dispositions, and inconsistent fingerprint submissions have plagued the system for decades. Criminal History Record Information (CHRI), the backbone for arrest datasets like those from the Illinois Criminal Justice Information Authority (ICJIA), is vulnerable to these shortcomings. Dispositions, or the outcomes of arrests, are often missing or delayed, leaving critical gaps in the data that make it difficult to analyze systemic trends. While advances in technology, such as digital fingerprinting, have improved some processes, data fragmentation and insufficient oversight remain persistent barriers to comprehensive and accurate reporting.\nFast forward thirty years, and many of these challenges remain unresolved—now further complicated by uneven implementation of modern systems. The Marshall Project highlights a striking example: in 2022, the FBI’s shift to the National Incident-Based Reporting System (NIBRS) created a significant data gap in national crime statistics2. Over 6,000 police agencies failed to submit their data, representing nearly one-third of all agencies and leaving vast portions of the U.S. population unaccounted for. This includes major departments like the NYPD and LAPD, alongside countless smaller agencies2. These gaps reflect broader systemic failures—inconsistent adoption of updated systems, lack of adequate funding, and minimal oversight—that echo the same issues identified decades ago.\nIn short, crime data, even when sourced from official systems, remains inherently flawed and incomplete. Whether due to outdated processes, inconsistent reporting practices, privacy-driven modifications, or gaps in modern collection systems, crime data often falls short of providing a fully accurate or comprehensive picture. As a result, any analysis relying on this data must account for these imperfections, recognizing that while the data can uncover critical trends and systemic disparities, it is rarely a perfect representation of reality.\n\n\nThe Illinois arrest dataset obtained from the ICJIA Arrest Explorer highlights the trade-offs between privacy and precision, adding to the broader challenges of criminal justice data. To protect confidentiality, counts under 10 are approximated—values between 0 and 4 are replaced with 1, while counts from 5 to 9 are replaced with 6. Though this approach is necessary to safeguard sensitive data, it can introduce distortions, particularly in smaller counties or demographic groups, where even slight approximations can significantly skew trends and reduce the accuracy of analysis. Compounding these issues, the lack of oversight in reporting further undermines data consistency and reliability, reflecting the broader systemic limitations that continue to plague many criminal justice datasets.\n\n\n\n\nThe National Registry of Exonerations is a trusted and widely used resource, frequently cited in academic and legal research. Unlike government databases, the Registry is run by a team of researchers and academics, which brings a level of precision and thoroughness often missing in state-managed systems3. Its foundation in meticulous documentation and independent research makes it less vulnerable to political or institutional biases. As a result, the Registry stands out as a more reliable and comprehensive tool for understanding wrongful convictions.\n\n\n\nThe data collected from various sources provide a foundation for examining systemic racial disparities in over-policing and wrongful convictions. However, challenges such as the lack of precision in Illinois arrest datasets and broader gaps in national crime reporting underscore the need to approach findings with caution. Even with these limitations, the use of reliable resources like the National Registry of Exonerations adds credibility to the analysis. Moving forward, future research could focus on compiling crime data from a single state across local, state, and federal agencies to better understand inconsistencies and uncover patterns of underreporting. Expanding this work to compare data collection practices across states could also reveal regional differences in crime reporting and highlight systemic disparities in how data is recorded and shared."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#challenges",
    "href": "technical-details/data-collection/closing.html#challenges",
    "title": "Summary",
    "section": "",
    "text": "The foundation for meaningful data analysis is accurate and reliable crime data, yet long-standing challenges in criminal history record systems continue to undermine their precision and dependability1. As outlined in the Use and Management of Criminal History Record Information Report (1993), issues such as incomplete data reporting, delays in recording arrests and case dispositions, and inconsistent fingerprint submissions have plagued the system for decades. Criminal History Record Information (CHRI), the backbone for arrest datasets like those from the Illinois Criminal Justice Information Authority (ICJIA), is vulnerable to these shortcomings. Dispositions, or the outcomes of arrests, are often missing or delayed, leaving critical gaps in the data that make it difficult to analyze systemic trends. While advances in technology, such as digital fingerprinting, have improved some processes, data fragmentation and insufficient oversight remain persistent barriers to comprehensive and accurate reporting.\nFast forward thirty years, and many of these challenges remain unresolved—now further complicated by uneven implementation of modern systems. The Marshall Project highlights a striking example: in 2022, the FBI’s shift to the National Incident-Based Reporting System (NIBRS) created a significant data gap in national crime statistics2. Over 6,000 police agencies failed to submit their data, representing nearly one-third of all agencies and leaving vast portions of the U.S. population unaccounted for. This includes major departments like the NYPD and LAPD, alongside countless smaller agencies2. These gaps reflect broader systemic failures—inconsistent adoption of updated systems, lack of adequate funding, and minimal oversight—that echo the same issues identified decades ago.\nIn short, crime data, even when sourced from official systems, remains inherently flawed and incomplete. Whether due to outdated processes, inconsistent reporting practices, privacy-driven modifications, or gaps in modern collection systems, crime data often falls short of providing a fully accurate or comprehensive picture. As a result, any analysis relying on this data must account for these imperfections, recognizing that while the data can uncover critical trends and systemic disparities, it is rarely a perfect representation of reality.\n\n\nThe Illinois arrest dataset obtained from the ICJIA Arrest Explorer highlights the trade-offs between privacy and precision, adding to the broader challenges of criminal justice data. To protect confidentiality, counts under 10 are approximated—values between 0 and 4 are replaced with 1, while counts from 5 to 9 are replaced with 6. Though this approach is necessary to safeguard sensitive data, it can introduce distortions, particularly in smaller counties or demographic groups, where even slight approximations can significantly skew trends and reduce the accuracy of analysis. Compounding these issues, the lack of oversight in reporting further undermines data consistency and reliability, reflecting the broader systemic limitations that continue to plague many criminal justice datasets."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#benchmarks",
    "href": "technical-details/data-collection/closing.html#benchmarks",
    "title": "Summary",
    "section": "",
    "text": "The National Registry of Exonerations is a trusted and widely used resource, frequently cited in academic and legal research. Unlike government databases, the Registry is run by a team of researchers and academics, which brings a level of precision and thoroughness often missing in state-managed systems3. Its foundation in meticulous documentation and independent research makes it less vulnerable to political or institutional biases. As a result, the Registry stands out as a more reliable and comprehensive tool for understanding wrongful convictions."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "title": "Summary",
    "section": "",
    "text": "The data collected from various sources provide a foundation for examining systemic racial disparities in over-policing and wrongful convictions. However, challenges such as the lack of precision in Illinois arrest datasets and broader gaps in national crime reporting underscore the need to approach findings with caution. Even with these limitations, the use of reliable resources like the National Registry of Exonerations adds credibility to the analysis. Moving forward, future research could focus on compiling crime data from a single state across local, state, and federal agencies to better understand inconsistencies and uncover patterns of underreporting. Expanding this work to compare data collection practices across states could also reveal regional differences in crime reporting and highlight systemic disparities in how data is recorded and shared."
  },
  {
    "objectID": "report/report.html",
    "href": "report/report.html",
    "title": "Final Report",
    "section": "",
    "text": "Audio instructions:\nIf you want, you can listen to the instructions:\n\n\n\n Source: Text-to-speech conversion done with Amazon Polly on AWS \nNote: These audio instructions should not be included in your final submission or repository, once you are done wiht them, please delete the files and remove them from the website.\nThis report is designed for a non-technical audience (e.g., the general public, executives, marketing teams, or clients), focusing on high-level insights, actionable results, and visualizations to convey the impact without requiring technical knowledge. The goal is to highlight how a model affects business strategy or revenue without diving into complex methods.\n\n\n\nClear Purpose: Define the core message or objective early on.\nKnow Your Audience: Adjust language, tone, and detail based on the audience’s understanding.\nStrong Opening: Start with a hook that sets context and stakes.\nLogical Flow: Structure with a clear beginning, middle, and end.\nKey Insights: Highlight the most important points; avoid unnecessary details or jargon.\nData Support: Use data to enhance the narrative without overwhelming.\nVisuals: Incorporate charts to simplify ideas and engage the audience.\nActionable Takeaways: Conclude with recommendations or next steps.\nAuthenticity: Use storytelling to make the content engaging and relatable.\nRevise: Edit for clarity and impact, removing unnecessary content.\n\n\n\n\nThese are just examples, you can use any structure that is suitable for your project.\n\n\n\nIntroduction: Provide an accessible overview and explain the motivation and importance of the research. Example: “This study explores how climate change affects local ecosystems, vital for wildlife conservation.”\nObjective: Clearly define the research goal and relate it to real-world challenges. Example: “We aim to analyze the effects of air pollution on public health in urban areas.”\nKey Findings: Present insights without technical terms, focusing on the impact. Example: “Air pollution increases the risk of respiratory diseases by 20%.”\nMethodology Overview: Briefly explain relevant methods. Example: “We analyzed air quality data from 50 cities and surveyed 10,000 residents.”\nVisualizations: Use simple graphs and infographics to convey findings. Example: A map showing pollution levels and a bar chart of health risks.\nSocietal Implications: Highlight the broader impact. Example: “This study highlights the need for better air quality policies.”\nCall to Action: Offer recommendations based on findings. Example: “We recommend city planners invest in green spaces.”\nConclusion: Recap the main findings and societal impact. Example: “Understanding pollution’s health impacts will help create healthier cities.”\n\n\n\n\n\nExecutive Summary: Summarize key findings and their relevance to business goals. Example: “This report predicts customer churn and offers strategies to reduce churn by 15%.”\nObjective: Define the problem or question addressed. Example: “This project aims to identify the drivers of customer churn.”\nKey Insights: Present the most important, actionable insights. Example: “Customers who interact with support twice within 30 days are 25% less likely to churn.”\nVisualizations: Use clear graphs to convey key insights. Example: A bar chart showing churn likelihood based on engagement.\nBusiness Implications: Explain how findings impact business outcomes and offer specific recommendations. Example: “Focus retention efforts on low-engagement customers.”\nRecommendations: Provide clear, actionable steps with projections. Example: “Implement an automated retention campaign, reducing churn by 10%.”\nConclusion: Summarize findings and suggest next steps. Example: “Addressing churn drivers can reduce loss and improve profitability.”\nAppendix (Optional): Additional charts or explanations for further insights.\n\n\n\n\n\n\nSimplicity: Avoid jargon; focus on business-relevant insights.\nVisual Focus: Prioritize charts and graphs over dense text.\nEmphasize Impact: Always link data insights to business outcomes."
  },
  {
    "objectID": "report/report.html#guidelines-for-creating-a-good-narrative",
    "href": "report/report.html#guidelines-for-creating-a-good-narrative",
    "title": "Final Report",
    "section": "",
    "text": "Clear Purpose: Define the core message or objective early on.\nKnow Your Audience: Adjust language, tone, and detail based on the audience’s understanding.\nStrong Opening: Start with a hook that sets context and stakes.\nLogical Flow: Structure with a clear beginning, middle, and end.\nKey Insights: Highlight the most important points; avoid unnecessary details or jargon.\nData Support: Use data to enhance the narrative without overwhelming.\nVisuals: Incorporate charts to simplify ideas and engage the audience.\nActionable Takeaways: Conclude with recommendations or next steps.\nAuthenticity: Use storytelling to make the content engaging and relatable.\nRevise: Edit for clarity and impact, removing unnecessary content."
  },
  {
    "objectID": "report/report.html#report-content",
    "href": "report/report.html#report-content",
    "title": "Final Report",
    "section": "",
    "text": "These are just examples, you can use any structure that is suitable for your project.\n\n\n\nIntroduction: Provide an accessible overview and explain the motivation and importance of the research. Example: “This study explores how climate change affects local ecosystems, vital for wildlife conservation.”\nObjective: Clearly define the research goal and relate it to real-world challenges. Example: “We aim to analyze the effects of air pollution on public health in urban areas.”\nKey Findings: Present insights without technical terms, focusing on the impact. Example: “Air pollution increases the risk of respiratory diseases by 20%.”\nMethodology Overview: Briefly explain relevant methods. Example: “We analyzed air quality data from 50 cities and surveyed 10,000 residents.”\nVisualizations: Use simple graphs and infographics to convey findings. Example: A map showing pollution levels and a bar chart of health risks.\nSocietal Implications: Highlight the broader impact. Example: “This study highlights the need for better air quality policies.”\nCall to Action: Offer recommendations based on findings. Example: “We recommend city planners invest in green spaces.”\nConclusion: Recap the main findings and societal impact. Example: “Understanding pollution’s health impacts will help create healthier cities.”\n\n\n\n\n\nExecutive Summary: Summarize key findings and their relevance to business goals. Example: “This report predicts customer churn and offers strategies to reduce churn by 15%.”\nObjective: Define the problem or question addressed. Example: “This project aims to identify the drivers of customer churn.”\nKey Insights: Present the most important, actionable insights. Example: “Customers who interact with support twice within 30 days are 25% less likely to churn.”\nVisualizations: Use clear graphs to convey key insights. Example: A bar chart showing churn likelihood based on engagement.\nBusiness Implications: Explain how findings impact business outcomes and offer specific recommendations. Example: “Focus retention efforts on low-engagement customers.”\nRecommendations: Provide clear, actionable steps with projections. Example: “Implement an automated retention campaign, reducing churn by 10%.”\nConclusion: Summarize findings and suggest next steps. Example: “Addressing churn drivers can reduce loss and improve profitability.”\nAppendix (Optional): Additional charts or explanations for further insights."
  },
  {
    "objectID": "report/report.html#final-tips",
    "href": "report/report.html#final-tips",
    "title": "Final Report",
    "section": "",
    "text": "Simplicity: Avoid jargon; focus on business-relevant insights.\nVisual Focus: Prioritize charts and graphs over dense text.\nEmphasize Impact: Always link data insights to business outcomes."
  },
  {
    "objectID": "instructions/topic-selection.html",
    "href": "instructions/topic-selection.html",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena\n\n\n\n\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation.\n\n\n\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/topic-selection.html#select-a-broad-topic-area",
    "href": "instructions/topic-selection.html#select-a-broad-topic-area",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/topic-selection.html#narrow-your-focus",
    "href": "instructions/topic-selection.html#narrow-your-focus",
    "title": "Topic selection",
    "section": "",
    "text": "Narrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/topic-selection.html#data-science-questions",
    "href": "instructions/topic-selection.html#data-science-questions",
    "title": "Topic selection",
    "section": "",
    "text": "The data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/overview.html",
    "href": "instructions/overview.html",
    "title": "Project instruction:",
    "section": "",
    "text": "Author(s): Dr. H and Gerard Pendleton Thurston the 4th\nNote: You can delete this folder and remove it from the project website once you have read and understood the instructions. It shouldn’t be part of your final submission.\nAudio instructions:\nIf you want, you can listen to the instructions:\n\n\n\n Source: Text-to-speech conversion done with Amazon Polly on AWS \nNote: These audio instructions should not be included in your final submission or repository, once you are done wiht them, please delete the files and remove them from the website."
  },
  {
    "objectID": "instructions/overview.html#python-package-optional",
    "href": "instructions/overview.html#python-package-optional",
    "title": "Project instruction:",
    "section": "Python package (optional)",
    "text": "Python package (optional)\nThis is an optional component of the project, if you’d like, you can create a dedicated Python package for your project. The source folder for this package should be included in the root of your repository, and it can be imported into your processing scripts used in the various technical-details sections. If you create a package, it should be well-documented, with an additional tab on the navigation bar for the package documentation.\nWhile a package would typically have its own GitHub repository, for this project, please include it within the same repository.\nThe skeleton for the package is not provided in the repo, but you can recycle what you created in past assignments."
  },
  {
    "objectID": "instructions/overview.html#select-a-broad-topic-area",
    "href": "instructions/overview.html#select-a-broad-topic-area",
    "title": "Project instruction:",
    "section": "Select a broad topic area",
    "text": "Select a broad topic area\nStart by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/overview.html#narrow-your-focus",
    "href": "instructions/overview.html#narrow-your-focus",
    "title": "Project instruction:",
    "section": "Narrow your focus",
    "text": "Narrow your focus\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/overview.html#data-science-questions",
    "href": "instructions/overview.html#data-science-questions",
    "title": "Project instruction:",
    "section": "Data Science Questions",
    "text": "Data Science Questions\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/overview.html#repository-setup",
    "href": "instructions/overview.html#repository-setup",
    "title": "Project instruction:",
    "section": "Repository Setup",
    "text": "Repository Setup\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/overview.html#expectations-for-github-usage",
    "href": "instructions/overview.html#expectations-for-github-usage",
    "title": "Project instruction:",
    "section": "Expectations for GitHub Usage",
    "text": "Expectations for GitHub Usage\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n1. Use a Logical Repository Structure\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n2. Commit Regularly\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n3. Data Storage\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n4. Syncing with GU Domains\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n5. Code Documentation\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "href": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "title": "Project instruction:",
    "section": "Collaboration in Groups (If Applicable)",
    "text": "Collaboration in Groups (If Applicable)\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/overview.html#website-development",
    "href": "instructions/overview.html#website-development",
    "title": "Project instruction:",
    "section": "Website Development",
    "text": "Website Development\nIt is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/overview.html#website-hosting",
    "href": "instructions/overview.html#website-hosting",
    "title": "Project instruction:",
    "section": "Website Hosting",
    "text": "Website Hosting\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/overview.html#the-two-audiences",
    "href": "instructions/overview.html#the-two-audiences",
    "title": "Project instruction:",
    "section": "The two audiences",
    "text": "The two audiences\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/overview.html#get-started-early",
    "href": "instructions/overview.html#get-started-early",
    "title": "Project instruction:",
    "section": "Get started early",
    "text": "Get started early\nRemember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/overview.html#graduate-level-work",
    "href": "instructions/overview.html#graduate-level-work",
    "title": "Project instruction:",
    "section": "Graduate level work",
    "text": "Graduate level work\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "Project instruction:",
    "section": "The intersection of skills and domain knowledge",
    "text": "The intersection of skills and domain knowledge\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/overview.html#what-is-impact",
    "href": "instructions/overview.html#what-is-impact",
    "title": "Project instruction:",
    "section": "What is “impact”?",
    "text": "What is “impact”?\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/overview.html#what-makes-a-good-research-project",
    "href": "instructions/overview.html#what-makes-a-good-research-project",
    "title": "Project instruction:",
    "section": "What makes a good research project?",
    "text": "What makes a good research project?\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/overview.html#time-management",
    "href": "instructions/overview.html#time-management",
    "title": "Project instruction:",
    "section": "Time management",
    "text": "Time management\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/overview.html#visualization-guidelines",
    "href": "instructions/overview.html#visualization-guidelines",
    "title": "Project instruction:",
    "section": "Visualization guidelines",
    "text": "Visualization guidelines\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/overview.html#coding",
    "href": "instructions/overview.html#coding",
    "title": "Project instruction:",
    "section": "Coding",
    "text": "Coding\n\nPractice First\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\nPrototype and develop on a small data set\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/overview.html#debugging",
    "href": "instructions/overview.html#debugging",
    "title": "Project instruction:",
    "section": "Debugging",
    "text": "Debugging\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/overview.html#file-types",
    "href": "instructions/overview.html#file-types",
    "title": "Project instruction:",
    "section": "File Types",
    "text": "File Types\nYou can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/overview.html#quarto-includes",
    "href": "instructions/overview.html#quarto-includes",
    "title": "Project instruction:",
    "section": "Quarto Includes",
    "text": "Quarto Includes\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\nWhy Use Quarto Includes?\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/overview.html#citation",
    "href": "instructions/overview.html#citation",
    "title": "Project instruction:",
    "section": "Citation",
    "text": "Citation\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/overview.html#acceptable-use-cases",
    "href": "instructions/overview.html#acceptable-use-cases",
    "title": "Project instruction:",
    "section": "Acceptable use cases",
    "text": "Acceptable use cases\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/overview.html#unacceptable-use-cases",
    "href": "instructions/overview.html#unacceptable-use-cases",
    "title": "Project instruction:",
    "section": "Unacceptable use cases",
    "text": "Unacceptable use cases\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\nEffect on grade\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\nPlagiarism investigation\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/github-usage.html",
    "href": "instructions/github-usage.html",
    "title": "GitHub",
    "section": "",
    "text": "Your project will be fully transparent, with all source code hosted on GitHub. This platform will serve as the main repository for your project code, documentation, and website. Proper organization and regular updates are key for effective collaboration and project management.\n\nIMPORTANT: Proficiency in GitHub for collaboration is a valuable addition to your resume. Being able to join a team and immediately contribute by solving problems and adding value is a highly sought-after skill. Now is the time to develop this expertise—embrace Git fully, become proficient, and graduate with a critical skill for your future career.\n\n\n\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure.\n\n\n\n\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies.\n\n\n\n\n\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/github-usage.html#repository-setup",
    "href": "instructions/github-usage.html#repository-setup",
    "title": "GitHub",
    "section": "",
    "text": "You MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/github-usage.html#expectations-for-github-usage",
    "href": "instructions/github-usage.html#expectations-for-github-usage",
    "title": "GitHub",
    "section": "",
    "text": "Your grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "href": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "title": "GitHub",
    "section": "",
    "text": "If you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Landing page",
    "section": "",
    "text": "Audio instructions:\nIf you want, you can listen to the instructions:\nSource: Text-to-speech conversion done with Amazon Polly on AWS\nNote: These audio instructions should not be included in your final submission or repository, once you are done wiht them, please delete the files and remove them from the website."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Landing page",
    "section": "Getting Started",
    "text": "Getting Started\nTo begin the project, first read the instruction document (click here). This document is also accessible from the navigation bar.\nOnce you’ve completed that, you can proceed with the instructions found throughout the website."
  },
  {
    "objectID": "index.html#what-to-include-on-this-page",
    "href": "index.html#what-to-include-on-this-page",
    "title": "Landing page",
    "section": "What to Include on This Page",
    "text": "What to Include on This Page\nThis is the landing page for your project. Content from this page can be reused in sections of your final report.\n\nCreate an “About You” Page\n\nDevelop your “About You” page. You can reuse content from previous assignments.\nYou can include the content here or on a separate page.\n\nIt’s recommended to create one “About You” page for all DSAN projects, with links to your various class projects.\n\n\n\n\nCreate a Landing Page for Your Project\n\nSummarize your topic, its significance, related work, and the questions you plan to explore.\nDraft an introduction with at least 5 research questions. These may evolve as your project progresses, since data science is an iterative process.\nInclude your data science questions on this page.\n\n\n\nLiterature review\nOnce you decide on a topic, you should ALWAYS START WITH A LITERATURE REVIEW, this is particularly important for academic projects.\nThe literature review is the most important part of most projects.\nIt allows you to;\n\n\nDetermine what is already known and what has already been tried, so that you don't re-invent the wheel.\n\n\nIt makes you more of a subject matter expert, allowing you to ask the right questions, target impactful projects, and communicate with other professionals.\n\n\n\nDoing a project that you think will change the world, only to find at the end that a very similar version of your project was already done in the 1980’s, isn’t a great use of time.\n\nIn this section, please do a literature review and cite at least 3 academic publications per group member, and include internal academic citations.\nOptional: Consider using LLM tools to 10X your literature review, e.g. instead of focusing on 3 papers, aim for 30 or more\nBy following these steps, you can 10X the efficiency of your literature review process, gaining more insights while minimizing the time spent on manual reading.\n\nExpand Your Literature Search\nAim to gather a larger pool of papers. Use academic databases (Google Scholar, arXiv, etc.) to find relevant studies and ensure a broad scope.\nSkim the Abstracts\nRead the abstracts of each paper to quickly understand their focus and identify those most relevant to your topic. Prioritize these papers for deeper analysis.\nUse LLM Tools for Summarization\nUpload the selected papers to an LLM tool capable of text summarization. Have it condense the main points of each paper into a concise, manageable summary (e.g., condense hundreds of pages into a 10-page summary). Carefully review this summary to absorb the key insights.\nLeverage Interactive LLM Tools\nUse tools like NotebookLM or other AI-based text digesters to ask specific, targeted questions about the papers:\n\nExample questions:\n\n“In the papers uploaded, did any of them explore XYZ?”\n“Which paper is most closely related to the following project idea: [explain idea]?” These tools will help you quickly extract relevant information without re-reading entire papers."
  },
  {
    "objectID": "index.html#additional-ideas-for-things-to-include",
    "href": "index.html#additional-ideas-for-things-to-include",
    "title": "Landing page",
    "section": "Additional Ideas for things to include",
    "text": "Additional Ideas for things to include\n\nAudience: Who is this for? Data professionals, businesses, researchers, or curious readers.\nHeadline: A captivating title introducing the data science theme (e.g., “Unlocking Insights Through Data Stories”).\nIntroduction: A brief, engaging overview of what the website offers (e.g., data-driven stories, insights, or case studies).\nQuestions You Are Addressing: What do you hope to learn?\nMotivation: Explain why this topic matters, highlighting the importance of data in solving real-world problems.\nKey Topics: List the main focus areas (e.g., machine learning, data visualization, predictive modeling).\nUse Cases/Examples: A brief teaser of compelling stories or case studies you’ve worked on.\nCall to Action: Invite visitors to explore the content, follow along, or contact you for more information.\nVisual/Infographic: Add a simple graphic or visual element to make the page more dynamic."
  },
  {
    "objectID": "development/literature-review.html",
    "href": "development/literature-review.html",
    "title": "Literature Review",
    "section": "",
    "text": "The study by Boehme, Cann, and Isom (2022) provides an in-depth examination of how race, ethnicity, and community characteristics influence public perceptions of policing, with a focus on both over- and under-policing. This research is highly relevant to my project on the intersection of race, poverty, and policing in Chicago, offering insights into how community-level factors contribute to divergent policing experiences1.\n\n\n\nThe Over-Under-Policing Paradox in Marginalized Communities: Boehme et al. highlight the duality of policing in marginalized communities, where residents often feel simultaneously over-policed through aggressive surveillance and under-policed in terms of protection and support. This paradox aligns with my project’s goal of investigating systemic biases in policing, revealing how this dual dynamic fosters a feedback loop of distrust and isolation from law enforcement1.\nRace as a Predictor of Perceptions: The study demonstrates that race consistently predicts perceptions of policing, with Black and Latinx communities reporting higher levels of over-policing regardless of economic conditions or other neighborhood factors. This finding supports the view that systemic racial biases in policing are pervasive, directly impacting Black and Latinx residents’ lived experiences. This insight strengthens my project’s focus on understanding the racialized nature of policing in Chicago1.\nPolicy Recommendations for Community-Based Reforms: Boehme and colleagues advocate for culturally sensitive reforms to improve police-community relations. Suggested reforms include adopting community-oriented policing models, reallocating resources to social services, and using non-police responders for non-violent incidents. These recommendations align with my project’s exploration of solutions to address both over- and under-policing by reimagining the role of police within marginalized communities1.\n\n\n\n\nBoehme et al. utilize data from the PHDCN (Project on Human Development in Chicago Neighborhoods) Community Survey, which provides a robust dataset that informs my own project’s focus on Chicago’s community dynamics.\n\nSurvey Scope and Sampling: The PHDCN survey includes 8,782 respondents across 343 neighborhood clusters in Chicago, derived from the city’s 847 census tracts. This sampling method offers a granular view of neighborhood-level factors, which is essential for understanding perceptions of policing across different community types in Chicago1.\nSurvey Timing and Context: Data collection for the PHDCN survey took place shortly after the 1994 Violent Crime Control and Law Enforcement Act, capturing early responses to increased policing efforts. This context is crucial for understanding how federal policies may have intensified policing in Black communities, informing my analysis of systemic policing disparities in Chicago1.\n\n\n\n\nBoehme et al. use Hierarchical Linear Modeling (HLM) to explore how individual and community characteristics shape perceptions of policing. Key analytical methods include:\n\nUnconditional Models: These models revealed that a significant portion of the variance in perceptions of over- and under-policing could be attributed to neighborhood-level factors, underscoring the importance of community context1.\nMeans-as-Outcomes Models: These models analyzed how neighborhood characteristics—such as economic disadvantage and ethnic diversity—influence residents’ perceptions of policing1.\nRandom Coefficient Models: Findings showed that race and ethnicity significantly impacted perceptions of policing, even after controlling for neighborhood factors. This reinforces the racialized experiences of policing, supporting my project’s objective to highlight the socio-economic roots of policing disparities in Chicago1.\n\n\n\n\nBoehme et al. provide valuable insights for understanding the dual impacts of over- and under-policing on marginalized communities. Here’s how these findings integrate with my project:\n\nReinforcing Racial Disparities in Policing: The study consistently shows that Black and Latinx residents report both over-policing and under-policing more frequently than their White counterparts. This underscores the racialized nature of policing and supports my project’s focus on the disproportionate impact of over-policing on Black communities in Chicago1.\nNeighborhood Characteristics as Predictors: Factors such as concentrated disadvantage and ethnic diversity emerged as significant predictors of policing perceptions. These findings are aligned with my project’s goal to explore the socio-economic roots of policing disparities, suggesting that addressing these factors may mitigate negative perceptions of policing in marginalized communities1.\nFeedback Loop Insight: The study highlights a feedback loop where high legal cynicism and under-policing exacerbate community distrust, fueling a cycle of under-service and over-surveillance. This dynamic is critical to my project’s focus on understanding how systemic issues perpetuate harmful feedback loops in marginalized communities1.\n\n\n\n\n\nThe paper by Jones-Brown and Williams (2021) provides a comprehensive analysis of systemic over-policing in Black communities across the U.S., with findings that are particularly relevant for exploring the dynamics in Chicago2. This review highlights the key findings and data sources from the study, aligning with my focus on race, poverty, and policing.\n\n\n\nEmpirical Evidence of Racial Disparities: The authors reference a range of empirical studies that provide quantitative evidence on racial disparities in police interactions, such as traffic stops and arrests. Data shows that Black individuals experience higher rates of police encounters, including consent searches and minor stops, reinforcing stereotypes that link Black identity to criminality2.\nHigh-Profile Cases and Lived Experience: Citing cases such as George Floyd and Breonna Taylor, the study captures the traumatic impact of over-policing on Black communities. These high-profile incidents serve as symbolic representations of a broader, systemic issue, fostering a legacy of fear and distrust toward law enforcement2.\nCommunity Distrust and Psychological Impact: Qualitative accounts from Black community members, including former Black police officers, reveal that recurrent negative police encounters create an environment of fear and mistrust. Black residents in areas such as Ferguson report feeling subjected to racialized social control, highlighting the psychosocial toll of over-policing in marginalized communities2.\nInfluence of Media Narratives: The study discusses how social media platforms have become a powerful tool for documenting police brutality and over-policing. Instances that gain traction online often reveal patterns of aggressive policing toward Black individuals, which the authors argue are reinforced by biased mainstream media portrayals that mischaracterize Black-led protests as violent, unlike predominantly White protests2.\nHistorical Context and Policy Analysis: The authors incorporate historical data, such as findings from the 1967 Katzenbach Report and the impact of the “War on Drugs,” to contextualize the persistence of racialized policing. These historical analyses reveal how longstanding policies have disproportionately impacted Black communities, establishing a precedent for examining similar issues in Chicago2.\nData-Driven Reform Recommendations: Drawing from statistical summaries, the study suggests reforms like eliminating consent searches without probable cause, reallocating police funds to community services, and removing racial identifiers in crime statistics. These proposed changes aim to transform policing practices and align with the goal of addressing the criminalization of poverty2.\n\n\n\n\nThe study integrates multiple data types to illustrate the scope of over-policing and its effects on Black communities:\n\nQuantitative Data: The authors use data on traffic stops, consent searches, and arrest rates from prior studies, highlighting a pattern of disproportionately high police encounters for Black individuals.\nQualitative Accounts: Narrative data from interviews with Black community members and former Black police officers bring a human perspective to the statistical data, demonstrating how over-policing affects personal safety and trust in law enforcement.\nSocial Media and Media Analysis: The authors examine social media documentation of police brutality and mainstream media portrayals of Black-led protests, emphasizing the role of media in reinforcing stereotypes that contribute to over-policing.\nHistorical Data: Referencing documents such as the 1967 Katzenbach Report, the study contextualizes racialized policing as a deeply rooted structural issue.\nPolicy Evaluations: By aggregating data from prior studies on policy impact, the authors evaluate the efficacy of different reforms, grounding their recommendations in empirical evidence.\n\n\n\n\n\nStructural Racism in Policing: The study underscores that racial biases are deeply embedded within policing practices, leading to a disproportionately high number of police interactions with Black individuals. This structural racism aligns with my project’s goal of examining how race and poverty intersect to perpetuate over-policing2.\nFeedback Loop of Fear and Distrust: Frequent and negative interactions with police contribute to a feedback loop where Black communities are more likely to distrust law enforcement, further isolating these communities and reinforcing harmful stereotypes2.\nPolicy Reform Recommendations: The study’s suggestions, such as reallocating police funds to social services and prohibiting consent searches without probable cause, offer practical solutions to reduce racial disparities. These recommendations could inform policy proposals in my project aimed at addressing systemic biases in Chicago policing2.\n\n\n\n\nJones-Brown and Williams (2021) provide a foundational view of over-policing in Black communities, supporting my project by offering:\n\nQuantitative and Qualitative Dual Perspective: The study’s combination of empirical data and personal accounts highlights the multifaceted nature of racial disparities in policing. This approach can inspire a dual focus in my project, utilizing both statistical data on over-policing in Chicago and narrative elements that illustrate its human impact.\nHistorical Context and Policy Analysis: By analyzing the historical roots of racialized policing, this paper strengthens my project’s examination of the socio-economic and racial factors that contribute to high crime and police interaction rates in marginalized communities.\nBlueprint for Systemic Reform: The study’s evidence-based reform suggestions, such as community-centered policing and policy reallocation, provide a practical framework for the solutions I plan to propose. These recommendations align with my goal of addressing over-policing in Chicago through systemic, community-driven reforms.\n\n\n\n\n\nThe study by Jacob Gesin, “Socioeconomic Determinants of Violent Crime Rates in the U.S.,” investigates how various socioeconomic factors influence violent crime rates, focusing on economic and structural disparities across racial groups in the U.S. This study’s findings provide critical context for my project’s focus on race, poverty, and policing in Chicago, especially as they pertain to the socioeconomic roots of crime in marginalized communities3.\n\n\n\nIncome Inequality and Violent Crime: Gesin’s analysis reveals a strong positive correlation between income inequality and violent crime rates, particularly in 2005 and 2009. States with greater income disparities were found to experience elevated crime rates, aligning with my project’s theme that economic inequalities contribute to higher crime rates in disadvantaged communities. This finding underscores the role of systemic inequality in perpetuating crime cycles in marginalized areas3.\nUnemployment and Violent Crime: Unemployment was shown to correlate with violent crime in two out of the three years studied. Gesin suggests that economic hardship and limited employment opportunities heighten frustration and aggression, which can manifest as violent crime. This insight supports my project’s examination of how economic deprivation in Black communities contributes to a feedback loop between poverty, crime, and increased police interactions3.\nFamily Structure and Social Stability: Gesin’s study highlights family structure as a factor influencing crime rates, with higher rates of family disruption (e.g., divorce, separation) linked to violent crime. This data suggests that family stability acts as a social control mechanism, deterring criminal behavior. For my project, this finding emphasizes the role of social structures in shaping crime rates, particularly in communities facing socioeconomic challenges3.\nRacial Disparities and Community Impact: Gesin found that racial composition significantly affected violent crime rates, with higher percentages of Black residents correlating with increased crime rates in 2012. This finding supports the idea that economically disadvantaged, racially segregated neighborhoods are more susceptible to violent crime, which could inform my focus on the unique challenges Black communities in Chicago face under systemic economic strain and over-policing3.\nEducation as a Protective Factor: Education emerged as a mitigating factor for crime, with higher levels of educational attainment generally associated with lower violent crime rates. Although this correlation varied in significance, it aligns with research suggesting that educational access reduces criminal behavior by improving job prospects and economic stability. This finding can support my project’s recommendations on promoting educational investment in marginalized communities as a potential crime prevention measure3.\n\n\n\n\nGesin’s study draws on data from several sources to analyze violent crime rates across the U.S.:\n\nU.S. Census Bureau and FBI UCR: Primary data was sourced from the Census Bureau and FBI’s Uniform Crime Reporting (UCR) program, covering crime and socioeconomic data from 2005, 2009, and 2012. These years capture different economic phases, offering insight into how economic shifts affect crime.\nSocioeconomic Variables:\n\nViolent Crime Rate (VCR): The dependent variable, capturing incidents of violent crime per 100,000 people.\nIncome Inequality (INC): Quantified by the Gini coefficient, illustrating disparities in wealth distribution.\nUnemployment (EMPLOY): Measured by the unemployment rate for individuals aged 16+, representing economic stability.\nFamily Structure (FAM): The percentage of families with divorced or separated parents, used to capture social stability.\nEducation (EDU): The proportion of individuals over 25 with some college education or higher.\nRacial Demographics (BPOP, WPOP, HPOP): Percentages of Black, White, and Hispanic populations, providing context on racial disparities.\n\n\n\n\n\n\nIncome Inequality as a Driver of Crime: Gesin’s study reinforces the concept that economic disparities play a significant role in crime rates. This finding supports my project’s focus on examining how poverty and economic disadvantage in Black communities contribute to higher crime and policing rates3.\nSocial and Family Structure as Crime Factors: The correlation between family structure and crime emphasizes that socioeconomic and social stability are crucial in understanding crime rates. This insight is particularly relevant for exploring how social stability (or instability) within Chicago’s marginalized communities impacts crime and policing3.\nRacial Dynamics in Crime Rates: The study’s findings on racial disparities in crime rates provide a foundation for analyzing how systemic factors, like poverty and segregation, uniquely affect Black communities. This aligns with my project’s aim of contextualizing over-policing in racialized and economically disadvantaged neighborhoods3.\n\n\n\n\nGesin’s research provides valuable empirical support for understanding the socioeconomic factors driving crime in marginalized communities, aligning closely with the aims of this project:\n\nSupporting Socioeconomic Analysis: The data on income inequality and unemployment directly support my project’s focus on the economic roots of crime. Gesin’s findings underscore how financial hardship exacerbates crime, emphasizing the importance of addressing economic disparities in crime prevention strategies.\nSocial and Family Stability as Predictors: The relationship between family structure and violent crime rates aligns with my project’s focus on structural factors contributing to crime. This insight suggests that fostering social stability in marginalized communities could help reduce crime rates, supporting broader efforts to mitigate over-policing.\nRacial Dynamics and Community Segregation: Gesin’s examination of racial disparities in crime rates provides a foundation for analyzing how Black communities in Chicago are disproportionately impacted by economic disadvantage and systemic law enforcement bias. This reinforces the significance of racial dynamics in studying over-policing and its effects on community well-being.\n\n\n\n\n\nThe article by Heather Cherone (2023) documents Chicago’s troubling status as the leading city for wrongful convictions in the U.S., accounting for more than half of all exonerations nationwide in 2022. This analysis, based on data from the National Registry of Exonerations, reveals systemic patterns of police misconduct and racial disparities in wrongful convictions, aligning closely with my project’s exploration of over-policing and its broader impact on marginalized communities4.\n\n\n\nExoneration Statistics: In 2022, Cook County recorded 124 overturned convictions, representing over half of all U.S. exonerations. This statistic underscores the prevalence of wrongful convictions within Chicago’s justice system, setting a comparative context for examining Chicago’s unique struggle with systemic misconduct4.\nRacial Disparities: The National Registry of Exonerations reports that nearly all individuals exonerated in Cook County in 2022 were Black or Latino, highlighting racial biases within Chicago’s criminal justice system. This detail strengthens my project’s focus on racialized over-policing and criminalization of marginalized communities4.\nPattern of Police Misconduct: The article identifies significant police misconduct by former officers Ronald Watts and Reynaldo Guevara, linked to the majority of 2022 exonerations. Their actions included extortion, framing suspects, and other corrupt practices. This misconduct exemplifies how individual abuses can perpetuate systemic injustice, impacting community trust4.\nFinancial Costs of Misconduct: Wrongful convictions and police misconduct impose heavy financial burdens on Chicago taxpayers. In 2022 alone, Chicago spent $98 million to settle police misconduct lawsuits, with the 2023 budget allocating an additional $82 million. This economic impact provides a powerful argument for preventive reforms to reduce costs and improve community well-being4.\nHistorical Context and Ongoing Compliance Issues: Despite a federal consent decree from 2017, the Chicago Police Department remains only 3% compliant with required reforms, indicating institutional resistance to change. This minimal compliance rate highlights the significant barriers to reform, supporting my project’s exploration of systemic obstacles in addressing police misconduct4.\n\n\n\n\n\nSystemic Racial Disparities in Wrongful Convictions: The data shows that Black and Latino individuals face disproportionate impacts from wrongful convictions, aligning with the project’s focus on how racial biases in policing can criminalize marginalized communities.\nFinancial Costs as a Social and Economic Issue: The taxpayer burden underscores that police misconduct is not only a community concern but also an economic one. This financial data offers strong support for advocating preventive reforms to reduce long-term costs associated with wrongful convictions.\nChallenges in Achieving Reform: The low compliance rate with the federal consent decree underscores the institutional challenges in achieving genuine reform within the Chicago Police Department. This finding supports the project’s argument that overcoming systemic issues in policing requires community-driven reforms alongside judicial oversight.\n\n\n\n\nThe data from the National Registry of Exonerations provides both quantitative and qualitative evidence that is essential to my project:\n\nEmpirical Support for Racial Disparities: The high percentage of Black and Latino individuals wrongfully convicted supports the project’s thesis that systemic bias within the Chicago police force disproportionately affects these communities.\nEconomic and Policy Implications: The financial data on settlements underscores that wrongful convictions impact the entire city’s budget, supporting the argument for long-term investments in systemic reforms.\nStructural Challenges to Reform: The minimal compliance with the federal consent decree suggests that achieving lasting change requires community accountability, highlighting the need for internal restructuring within the police department."
  },
  {
    "objectID": "development/literature-review.html#citizens-perceptions-of-over-and-under-policing-a-look-at-race-ethnicity-and-community-characteristics",
    "href": "development/literature-review.html#citizens-perceptions-of-over-and-under-policing-a-look-at-race-ethnicity-and-community-characteristics",
    "title": "Literature Review",
    "section": "",
    "text": "The study by Boehme, Cann, and Isom (2022) provides an in-depth examination of how race, ethnicity, and community characteristics influence public perceptions of policing, with a focus on both over- and under-policing. This research is highly relevant to my project on the intersection of race, poverty, and policing in Chicago, offering insights into how community-level factors contribute to divergent policing experiences1.\n\n\n\nThe Over-Under-Policing Paradox in Marginalized Communities: Boehme et al. highlight the duality of policing in marginalized communities, where residents often feel simultaneously over-policed through aggressive surveillance and under-policed in terms of protection and support. This paradox aligns with my project’s goal of investigating systemic biases in policing, revealing how this dual dynamic fosters a feedback loop of distrust and isolation from law enforcement1.\nRace as a Predictor of Perceptions: The study demonstrates that race consistently predicts perceptions of policing, with Black and Latinx communities reporting higher levels of over-policing regardless of economic conditions or other neighborhood factors. This finding supports the view that systemic racial biases in policing are pervasive, directly impacting Black and Latinx residents’ lived experiences. This insight strengthens my project’s focus on understanding the racialized nature of policing in Chicago1.\nPolicy Recommendations for Community-Based Reforms: Boehme and colleagues advocate for culturally sensitive reforms to improve police-community relations. Suggested reforms include adopting community-oriented policing models, reallocating resources to social services, and using non-police responders for non-violent incidents. These recommendations align with my project’s exploration of solutions to address both over- and under-policing by reimagining the role of police within marginalized communities1.\n\n\n\n\nBoehme et al. utilize data from the PHDCN (Project on Human Development in Chicago Neighborhoods) Community Survey, which provides a robust dataset that informs my own project’s focus on Chicago’s community dynamics.\n\nSurvey Scope and Sampling: The PHDCN survey includes 8,782 respondents across 343 neighborhood clusters in Chicago, derived from the city’s 847 census tracts. This sampling method offers a granular view of neighborhood-level factors, which is essential for understanding perceptions of policing across different community types in Chicago1.\nSurvey Timing and Context: Data collection for the PHDCN survey took place shortly after the 1994 Violent Crime Control and Law Enforcement Act, capturing early responses to increased policing efforts. This context is crucial for understanding how federal policies may have intensified policing in Black communities, informing my analysis of systemic policing disparities in Chicago1.\n\n\n\n\nBoehme et al. use Hierarchical Linear Modeling (HLM) to explore how individual and community characteristics shape perceptions of policing. Key analytical methods include:\n\nUnconditional Models: These models revealed that a significant portion of the variance in perceptions of over- and under-policing could be attributed to neighborhood-level factors, underscoring the importance of community context1.\nMeans-as-Outcomes Models: These models analyzed how neighborhood characteristics—such as economic disadvantage and ethnic diversity—influence residents’ perceptions of policing1.\nRandom Coefficient Models: Findings showed that race and ethnicity significantly impacted perceptions of policing, even after controlling for neighborhood factors. This reinforces the racialized experiences of policing, supporting my project’s objective to highlight the socio-economic roots of policing disparities in Chicago1.\n\n\n\n\nBoehme et al. provide valuable insights for understanding the dual impacts of over- and under-policing on marginalized communities. Here’s how these findings integrate with my project:\n\nReinforcing Racial Disparities in Policing: The study consistently shows that Black and Latinx residents report both over-policing and under-policing more frequently than their White counterparts. This underscores the racialized nature of policing and supports my project’s focus on the disproportionate impact of over-policing on Black communities in Chicago1.\nNeighborhood Characteristics as Predictors: Factors such as concentrated disadvantage and ethnic diversity emerged as significant predictors of policing perceptions. These findings are aligned with my project’s goal to explore the socio-economic roots of policing disparities, suggesting that addressing these factors may mitigate negative perceptions of policing in marginalized communities1.\nFeedback Loop Insight: The study highlights a feedback loop where high legal cynicism and under-policing exacerbate community distrust, fueling a cycle of under-service and over-surveillance. This dynamic is critical to my project’s focus on understanding how systemic issues perpetuate harmful feedback loops in marginalized communities1."
  },
  {
    "objectID": "development/literature-review.html#over-policing-black-bodies-the-need-for-multidimensional-and-transformative-reforms",
    "href": "development/literature-review.html#over-policing-black-bodies-the-need-for-multidimensional-and-transformative-reforms",
    "title": "Literature Review",
    "section": "",
    "text": "The paper by Jones-Brown and Williams (2021) provides a comprehensive analysis of systemic over-policing in Black communities across the U.S., with findings that are particularly relevant for exploring the dynamics in Chicago2. This review highlights the key findings and data sources from the study, aligning with my focus on race, poverty, and policing.\n\n\n\nEmpirical Evidence of Racial Disparities: The authors reference a range of empirical studies that provide quantitative evidence on racial disparities in police interactions, such as traffic stops and arrests. Data shows that Black individuals experience higher rates of police encounters, including consent searches and minor stops, reinforcing stereotypes that link Black identity to criminality2.\nHigh-Profile Cases and Lived Experience: Citing cases such as George Floyd and Breonna Taylor, the study captures the traumatic impact of over-policing on Black communities. These high-profile incidents serve as symbolic representations of a broader, systemic issue, fostering a legacy of fear and distrust toward law enforcement2.\nCommunity Distrust and Psychological Impact: Qualitative accounts from Black community members, including former Black police officers, reveal that recurrent negative police encounters create an environment of fear and mistrust. Black residents in areas such as Ferguson report feeling subjected to racialized social control, highlighting the psychosocial toll of over-policing in marginalized communities2.\nInfluence of Media Narratives: The study discusses how social media platforms have become a powerful tool for documenting police brutality and over-policing. Instances that gain traction online often reveal patterns of aggressive policing toward Black individuals, which the authors argue are reinforced by biased mainstream media portrayals that mischaracterize Black-led protests as violent, unlike predominantly White protests2.\nHistorical Context and Policy Analysis: The authors incorporate historical data, such as findings from the 1967 Katzenbach Report and the impact of the “War on Drugs,” to contextualize the persistence of racialized policing. These historical analyses reveal how longstanding policies have disproportionately impacted Black communities, establishing a precedent for examining similar issues in Chicago2.\nData-Driven Reform Recommendations: Drawing from statistical summaries, the study suggests reforms like eliminating consent searches without probable cause, reallocating police funds to community services, and removing racial identifiers in crime statistics. These proposed changes aim to transform policing practices and align with the goal of addressing the criminalization of poverty2.\n\n\n\n\nThe study integrates multiple data types to illustrate the scope of over-policing and its effects on Black communities:\n\nQuantitative Data: The authors use data on traffic stops, consent searches, and arrest rates from prior studies, highlighting a pattern of disproportionately high police encounters for Black individuals.\nQualitative Accounts: Narrative data from interviews with Black community members and former Black police officers bring a human perspective to the statistical data, demonstrating how over-policing affects personal safety and trust in law enforcement.\nSocial Media and Media Analysis: The authors examine social media documentation of police brutality and mainstream media portrayals of Black-led protests, emphasizing the role of media in reinforcing stereotypes that contribute to over-policing.\nHistorical Data: Referencing documents such as the 1967 Katzenbach Report, the study contextualizes racialized policing as a deeply rooted structural issue.\nPolicy Evaluations: By aggregating data from prior studies on policy impact, the authors evaluate the efficacy of different reforms, grounding their recommendations in empirical evidence.\n\n\n\n\n\nStructural Racism in Policing: The study underscores that racial biases are deeply embedded within policing practices, leading to a disproportionately high number of police interactions with Black individuals. This structural racism aligns with my project’s goal of examining how race and poverty intersect to perpetuate over-policing2.\nFeedback Loop of Fear and Distrust: Frequent and negative interactions with police contribute to a feedback loop where Black communities are more likely to distrust law enforcement, further isolating these communities and reinforcing harmful stereotypes2.\nPolicy Reform Recommendations: The study’s suggestions, such as reallocating police funds to social services and prohibiting consent searches without probable cause, offer practical solutions to reduce racial disparities. These recommendations could inform policy proposals in my project aimed at addressing systemic biases in Chicago policing2.\n\n\n\n\nJones-Brown and Williams (2021) provide a foundational view of over-policing in Black communities, supporting my project by offering:\n\nQuantitative and Qualitative Dual Perspective: The study’s combination of empirical data and personal accounts highlights the multifaceted nature of racial disparities in policing. This approach can inspire a dual focus in my project, utilizing both statistical data on over-policing in Chicago and narrative elements that illustrate its human impact.\nHistorical Context and Policy Analysis: By analyzing the historical roots of racialized policing, this paper strengthens my project’s examination of the socio-economic and racial factors that contribute to high crime and police interaction rates in marginalized communities.\nBlueprint for Systemic Reform: The study’s evidence-based reform suggestions, such as community-centered policing and policy reallocation, provide a practical framework for the solutions I plan to propose. These recommendations align with my goal of addressing over-policing in Chicago through systemic, community-driven reforms."
  },
  {
    "objectID": "development/literature-review.html#socioeconomic-determinants-of-violent-crime-rates-in-the-u.s.-by-jacob-gesin",
    "href": "development/literature-review.html#socioeconomic-determinants-of-violent-crime-rates-in-the-u.s.-by-jacob-gesin",
    "title": "Literature Review",
    "section": "",
    "text": "The study by Jacob Gesin, “Socioeconomic Determinants of Violent Crime Rates in the U.S.,” investigates how various socioeconomic factors influence violent crime rates, focusing on economic and structural disparities across racial groups in the U.S. This study’s findings provide critical context for my project’s focus on race, poverty, and policing in Chicago, especially as they pertain to the socioeconomic roots of crime in marginalized communities3.\n\n\n\nIncome Inequality and Violent Crime: Gesin’s analysis reveals a strong positive correlation between income inequality and violent crime rates, particularly in 2005 and 2009. States with greater income disparities were found to experience elevated crime rates, aligning with my project’s theme that economic inequalities contribute to higher crime rates in disadvantaged communities. This finding underscores the role of systemic inequality in perpetuating crime cycles in marginalized areas3.\nUnemployment and Violent Crime: Unemployment was shown to correlate with violent crime in two out of the three years studied. Gesin suggests that economic hardship and limited employment opportunities heighten frustration and aggression, which can manifest as violent crime. This insight supports my project’s examination of how economic deprivation in Black communities contributes to a feedback loop between poverty, crime, and increased police interactions3.\nFamily Structure and Social Stability: Gesin’s study highlights family structure as a factor influencing crime rates, with higher rates of family disruption (e.g., divorce, separation) linked to violent crime. This data suggests that family stability acts as a social control mechanism, deterring criminal behavior. For my project, this finding emphasizes the role of social structures in shaping crime rates, particularly in communities facing socioeconomic challenges3.\nRacial Disparities and Community Impact: Gesin found that racial composition significantly affected violent crime rates, with higher percentages of Black residents correlating with increased crime rates in 2012. This finding supports the idea that economically disadvantaged, racially segregated neighborhoods are more susceptible to violent crime, which could inform my focus on the unique challenges Black communities in Chicago face under systemic economic strain and over-policing3.\nEducation as a Protective Factor: Education emerged as a mitigating factor for crime, with higher levels of educational attainment generally associated with lower violent crime rates. Although this correlation varied in significance, it aligns with research suggesting that educational access reduces criminal behavior by improving job prospects and economic stability. This finding can support my project’s recommendations on promoting educational investment in marginalized communities as a potential crime prevention measure3.\n\n\n\n\nGesin’s study draws on data from several sources to analyze violent crime rates across the U.S.:\n\nU.S. Census Bureau and FBI UCR: Primary data was sourced from the Census Bureau and FBI’s Uniform Crime Reporting (UCR) program, covering crime and socioeconomic data from 2005, 2009, and 2012. These years capture different economic phases, offering insight into how economic shifts affect crime.\nSocioeconomic Variables:\n\nViolent Crime Rate (VCR): The dependent variable, capturing incidents of violent crime per 100,000 people.\nIncome Inequality (INC): Quantified by the Gini coefficient, illustrating disparities in wealth distribution.\nUnemployment (EMPLOY): Measured by the unemployment rate for individuals aged 16+, representing economic stability.\nFamily Structure (FAM): The percentage of families with divorced or separated parents, used to capture social stability.\nEducation (EDU): The proportion of individuals over 25 with some college education or higher.\nRacial Demographics (BPOP, WPOP, HPOP): Percentages of Black, White, and Hispanic populations, providing context on racial disparities.\n\n\n\n\n\n\nIncome Inequality as a Driver of Crime: Gesin’s study reinforces the concept that economic disparities play a significant role in crime rates. This finding supports my project’s focus on examining how poverty and economic disadvantage in Black communities contribute to higher crime and policing rates3.\nSocial and Family Structure as Crime Factors: The correlation between family structure and crime emphasizes that socioeconomic and social stability are crucial in understanding crime rates. This insight is particularly relevant for exploring how social stability (or instability) within Chicago’s marginalized communities impacts crime and policing3.\nRacial Dynamics in Crime Rates: The study’s findings on racial disparities in crime rates provide a foundation for analyzing how systemic factors, like poverty and segregation, uniquely affect Black communities. This aligns with my project’s aim of contextualizing over-policing in racialized and economically disadvantaged neighborhoods3.\n\n\n\n\nGesin’s research provides valuable empirical support for understanding the socioeconomic factors driving crime in marginalized communities, aligning closely with the aims of this project:\n\nSupporting Socioeconomic Analysis: The data on income inequality and unemployment directly support my project’s focus on the economic roots of crime. Gesin’s findings underscore how financial hardship exacerbates crime, emphasizing the importance of addressing economic disparities in crime prevention strategies.\nSocial and Family Stability as Predictors: The relationship between family structure and violent crime rates aligns with my project’s focus on structural factors contributing to crime. This insight suggests that fostering social stability in marginalized communities could help reduce crime rates, supporting broader efforts to mitigate over-policing.\nRacial Dynamics and Community Segregation: Gesin’s examination of racial disparities in crime rates provides a foundation for analyzing how Black communities in Chicago are disproportionately impacted by economic disadvantage and systemic law enforcement bias. This reinforces the significance of racial dynamics in studying over-policing and its effects on community well-being."
  },
  {
    "objectID": "development/literature-review.html#chicago-ranks-no.-1-in-exonerations-for-5th-year-in-a-row-accounting-for-more-than-half-of-national-total",
    "href": "development/literature-review.html#chicago-ranks-no.-1-in-exonerations-for-5th-year-in-a-row-accounting-for-more-than-half-of-national-total",
    "title": "Literature Review",
    "section": "",
    "text": "The article by Heather Cherone (2023) documents Chicago’s troubling status as the leading city for wrongful convictions in the U.S., accounting for more than half of all exonerations nationwide in 2022. This analysis, based on data from the National Registry of Exonerations, reveals systemic patterns of police misconduct and racial disparities in wrongful convictions, aligning closely with my project’s exploration of over-policing and its broader impact on marginalized communities4.\n\n\n\nExoneration Statistics: In 2022, Cook County recorded 124 overturned convictions, representing over half of all U.S. exonerations. This statistic underscores the prevalence of wrongful convictions within Chicago’s justice system, setting a comparative context for examining Chicago’s unique struggle with systemic misconduct4.\nRacial Disparities: The National Registry of Exonerations reports that nearly all individuals exonerated in Cook County in 2022 were Black or Latino, highlighting racial biases within Chicago’s criminal justice system. This detail strengthens my project’s focus on racialized over-policing and criminalization of marginalized communities4.\nPattern of Police Misconduct: The article identifies significant police misconduct by former officers Ronald Watts and Reynaldo Guevara, linked to the majority of 2022 exonerations. Their actions included extortion, framing suspects, and other corrupt practices. This misconduct exemplifies how individual abuses can perpetuate systemic injustice, impacting community trust4.\nFinancial Costs of Misconduct: Wrongful convictions and police misconduct impose heavy financial burdens on Chicago taxpayers. In 2022 alone, Chicago spent $98 million to settle police misconduct lawsuits, with the 2023 budget allocating an additional $82 million. This economic impact provides a powerful argument for preventive reforms to reduce costs and improve community well-being4.\nHistorical Context and Ongoing Compliance Issues: Despite a federal consent decree from 2017, the Chicago Police Department remains only 3% compliant with required reforms, indicating institutional resistance to change. This minimal compliance rate highlights the significant barriers to reform, supporting my project’s exploration of systemic obstacles in addressing police misconduct4.\n\n\n\n\n\nSystemic Racial Disparities in Wrongful Convictions: The data shows that Black and Latino individuals face disproportionate impacts from wrongful convictions, aligning with the project’s focus on how racial biases in policing can criminalize marginalized communities.\nFinancial Costs as a Social and Economic Issue: The taxpayer burden underscores that police misconduct is not only a community concern but also an economic one. This financial data offers strong support for advocating preventive reforms to reduce long-term costs associated with wrongful convictions.\nChallenges in Achieving Reform: The low compliance rate with the federal consent decree underscores the institutional challenges in achieving genuine reform within the Chicago Police Department. This finding supports the project’s argument that overcoming systemic issues in policing requires community-driven reforms alongside judicial oversight.\n\n\n\n\nThe data from the National Registry of Exonerations provides both quantitative and qualitative evidence that is essential to my project:\n\nEmpirical Support for Racial Disparities: The high percentage of Black and Latino individuals wrongfully convicted supports the project’s thesis that systemic bias within the Chicago police force disproportionately affects these communities.\nEconomic and Policy Implications: The financial data on settlements underscores that wrongful convictions impact the entire city’s budget, supporting the argument for long-term investments in systemic reforms.\nStructural Challenges to Reform: The minimal compliance with the federal consent decree suggests that achieving lasting change requires community accountability, highlighting the need for internal restructuring within the police department."
  },
  {
    "objectID": "development/project-planning.html",
    "href": "development/project-planning.html",
    "title": "Project Planning Document",
    "section": "",
    "text": "“Race, Poverty, and Policing in Chicago: Analyzing the Socioeconomic Roots of Crime, Over-Policing in Marginalized Communities”\nGoal: Demonstrate how systemic issues create a feedback loop where poverty (which is intertwined with race) leads to more police interactions, resulting in higher arrest rates for minor infractions. With this project, using Chicago, a microcosm representing the broader dynamics of racial and socioeconomic disparities in policing in the U.S, I hope to highlight how these dynamics disproportionately affect Black communities, inflating crime statistics and criminalizing poverty.\nWhy this matters: Many fail to grasp the deeper context behind statistics showing higher conviction and incarceration rates for Black individuals, which reinforces harmful stereotypes that portray Black people as inherently more violent and criminal. Too often, crime data is oversimplified, failing to account for longstanding systemic issues, such as the over-policing of marginalized communities, economic disparities, and racism. Through this project I want to bridge that gap by providing a data-driven narrative that explins why these disparities exist and persist."
  },
  {
    "objectID": "development/project-planning.html#working-idea-1",
    "href": "development/project-planning.html#working-idea-1",
    "title": "Project Planning Document",
    "section": "",
    "text": "“Race, Poverty, and Policing in Chicago: Analyzing the Socioeconomic Roots of Crime, Over-Policing in Marginalized Communities”\nGoal: Demonstrate how systemic issues create a feedback loop where poverty (which is intertwined with race) leads to more police interactions, resulting in higher arrest rates for minor infractions. With this project, using Chicago, a microcosm representing the broader dynamics of racial and socioeconomic disparities in policing in the U.S, I hope to highlight how these dynamics disproportionately affect Black communities, inflating crime statistics and criminalizing poverty.\nWhy this matters: Many fail to grasp the deeper context behind statistics showing higher conviction and incarceration rates for Black individuals, which reinforces harmful stereotypes that portray Black people as inherently more violent and criminal. Too often, crime data is oversimplified, failing to account for longstanding systemic issues, such as the over-policing of marginalized communities, economic disparities, and racism. Through this project I want to bridge that gap by providing a data-driven narrative that explins why these disparities exist and persist."
  },
  {
    "objectID": "development/project-planning.html#working-idea-2",
    "href": "development/project-planning.html#working-idea-2",
    "title": "Project Planning Document",
    "section": "Working Idea #2:",
    "text": "Working Idea #2:\n“Race, Poverty, and Policing in Chicago: Analyzing the Socioeconomic Roots of Crime, Over-Policing, and Wrongful Convictions in Marginalized Communities”\nGoal:\nDemonstrate how systemic issues create a feedback loop where poverty (which is intertwined with race) leads to more police interactions, resulting in higher arrest rates for minor infractions. This project uses Chicago as a representation of broader dynamics in the U.S., focusing on how these patterns disproportionately affect Black communities, inflating crime statistics and, in some cases, leading to wrongful convictions that criminalize poverty and perpetuate racial injustice.\nWhy This Matters:\nMany fail to grasp the deeper context behind statistics showing higher conviction and incarceration rates for Black individuals, reinforcing harmful stereotypes that portray Black people as inherently more violent and criminal. Crime data is often oversimplified, failing to account for systemic factors like over-policing, economic disparities, and racial bias. By exploring both the over-policing of marginalized communities and the frequency of wrongful convictions, this project aims to provide a nuanced, data-driven narrative that explains why these disparities exist and persist."
  },
  {
    "objectID": "development/project-planning.html#considerations-for-including-wrongful-conviction-data",
    "href": "development/project-planning.html#considerations-for-including-wrongful-conviction-data",
    "title": "Project Planning Document",
    "section": "Considerations for Including Wrongful Conviction Data",
    "text": "Considerations for Including Wrongful Conviction Data\nI’m currently contemplating whether to expand the project to include data on wrongful convictions and exonerations, or to keep the focus on over-policing and criminalization of poverty. Here are some of the factors influencing this decision:\n\nPros of Including Wrongful Convictions:\n\nAdds depth to the project by showcasing the long-term impacts of systemic bias, including how over-policing can lead to wrongful convictions.\nStrengthens the narrative by illustrating the severe consequences of systemic issues beyond immediate interactions with police.\nCoincides with my current internship, where I work firsthand with exonerated and currently incarcerated individuals. Aligning this project with my work experience would provide valuable insights and make the subject even more personally meaningful.\n\nCons of Including Wrongful Convictions:\n\nIncreased complexity and workload, as managing multiple datasets may require additional data wrangling and cleaning.\nPotential dilution of focus, as balancing both over-policing and wrongful convictions might make it challenging to tell a clear, cohesive story."
  },
  {
    "objectID": "instructions/expectations.html",
    "href": "instructions/expectations.html",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare.\n\n\n\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable.\n\n\n\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields.\n\n\n\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field.\n\n\n\n\n\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important.\n\n\n\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point.\n\n\n\n\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual.\n\n\n\n\n\n\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project .\n\n\n\n\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/expectations.html#get-started-early",
    "href": "instructions/expectations.html#get-started-early",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/expectations.html#graduate-level-work",
    "href": "instructions/expectations.html#graduate-level-work",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "This is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/expectations.html#what-is-impact",
    "href": "instructions/expectations.html#what-is-impact",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Impact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/expectations.html#what-makes-a-good-research-project",
    "href": "instructions/expectations.html#what-makes-a-good-research-project",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Professional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/expectations.html#time-management",
    "href": "instructions/expectations.html#time-management",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/expectations.html#visualization-guidelines",
    "href": "instructions/expectations.html#visualization-guidelines",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Visualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/expectations.html#coding",
    "href": "instructions/expectations.html#coding",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "While not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/expectations.html#debugging",
    "href": "instructions/expectations.html#debugging",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Always remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/llm-usage.html",
    "href": "instructions/llm-usage.html",
    "title": "LLM usage",
    "section": "",
    "text": "We believe that the adoption of LLM tools is inevitable and will be an important skill for success in your future career. Therefore, appropriate and acceptable use of LLM tools is encouraged for this project. Use them to accelerate your workflow and learning, but not as a replacement for critical thinking and understanding. Carefully review and process their output, use them judiciously, and avoid bloating your text with LLM-generated content. Overusing these tools often degrades the quality of your work rather than enhancing it.\nRemember the following guidelines:\n\nUse common sense: If you feel like you’re doing something questionable, you probably are. A good test is to ask yourself, “Would I openly tell the professor or classmates what I’m doing right now?” If the answer is no, you’re probably doing something you shouldn’t.\nCite your LLM use cases: Always cite when and how you’ve used LLM tools. This is a requirement for the project.\nIs your use helping you grow professionally?: If your use of LLM tools is making you a more competent, efficient, and knowledgeable professional, you’re probably using them in an appropriate manner. If you’re using them as a shortcut to avoid work and gain free time, you’re using them incorrectly.\n\n\n\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1\n\n\n\n\n\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code.\n\n\n\n\n\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/llm-usage.html#citation",
    "href": "instructions/llm-usage.html#citation",
    "title": "LLM usage",
    "section": "",
    "text": "ALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/llm-usage.html#acceptable-use-cases",
    "href": "instructions/llm-usage.html#acceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "Note: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/llm-usage.html#unacceptable-use-cases",
    "href": "instructions/llm-usage.html#unacceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "DO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/quarto-tips.html",
    "href": "instructions/quarto-tips.html",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton.\n\n\n\n\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/quarto-tips.html#file-types",
    "href": "instructions/quarto-tips.html#file-types",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/quarto-tips.html#quarto-includes",
    "href": "instructions/quarto-tips.html#quarto-includes",
    "title": "Quarto Tips",
    "section": "",
    "text": "Quarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/website-structure.html",
    "href": "instructions/website-structure.html",
    "title": "Website project structure",
    "section": "",
    "text": "Please at miniumum include the following pages in your website:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\nSupervised-learning\nLLM-usage\nProgress-log\n\n\nPlease adhere closely to this structure, for consistency accross projects.\nSub-sections can be handles as markdown headers in the respective pages.\nYou can add more pages,and if you want, you can merge EDA and unsupervised learning into one page, since the are similar. Or make these section headers in the dropdown menu, for further sub-sections creation.\nFor example:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\n\nClustering\nDimensionality Reduction\n\nSupervised-learning\n\nFeature selection\n\nregression\nclassification\n\nClassification\n\nBinary classification\nMulti-class classification\n\nRegression\n\nLLM-usage\nProgress-log\n\n\nImportant: Exactly what you put on these pages will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\nA skeleton of the recommended version of the website is provided in the github classroom repository.\n./\n├── README.md\n├── _quarto.yml\n├── assets\n│   ├── gu-logo.png\n│   ├── nature.csl\n│   └── references.bib\n├── build.sh\n├── data\n│   ├── processed-data\n│   │   └── countries_population.csv\n│   └── raw-data\n│       └── countries_population.csv\n├── index.qmd\n├── instructions\n│   ├── expectations.qmd\n│   ├── github-usage.qmd\n│   ├── llm-usage.qmd\n│   ├── overview.qmd\n│   ├── quarto-tips.qmd\n│   ├── topic-selection.qmd\n│   └── website-structure.qmd\n├── report\n│   └── report.qmd\n└── technical-details\n    ├── data-cleaning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── data-collection\n    │   ├── closing.qmd\n    │   ├── instructions.qmd\n    │   ├── main.ipynb\n    │   ├── methods.qmd\n    │   └── overview.qmd\n    ├── eda\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── llm-usage-log.qmd\n    ├── progress-log.qmd\n    ├── supervised-learning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    └── unsupervised-learning\n        ├── instructions.qmd\n        └── main.ipynb\nAlways strive to incorporate the following:\n\nStructure: Use clear headings and subheadings to break down each section of your EDA.\nClarity: Provide concise explanations for all tables and visualizations, ensuring they are easy to interpret.\nCode Links: Link to relevant code (e.g., GitHub) or embed code snippets for transparency and reproducibility.\nReproducibility: Make your EDA reproducible by providing access to the dataset, scripts, and tools you used.\nVisualization: Use visualizations to convey key insights\n\n\n\nIt is required that you build your website with Quarto.\n\n\n\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO.\n\n\n\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/website-structure.html#website-development",
    "href": "instructions/website-structure.html#website-development",
    "title": "Website project structure",
    "section": "",
    "text": "It is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/website-structure.html#website-hosting",
    "href": "instructions/website-structure.html#website-hosting",
    "title": "Website project structure",
    "section": "",
    "text": "You MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/website-structure.html#the-two-audiences",
    "href": "instructions/website-structure.html#the-two-audiences",
    "title": "Website project structure",
    "section": "",
    "text": "Knowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html",
    "href": "technical-details/data-cleaning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you on this page will be specific you your project and data. Some things might “make more sense” on other pages, depending on your workflow, for example, you might feel that normalization and scaling should be included in a later section, dealing with machine learning, rather than here, that is totally fine. Organize your project in the way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\n\nIterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#suggested-page-structure",
    "href": "technical-details/data-cleaning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#general-comments",
    "href": "technical-details/data-cleaning/instructions.html#general-comments",
    "title": "Instructions",
    "section": "",
    "text": "Iterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#what-to-address",
    "href": "technical-details/data-cleaning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html",
    "href": "technical-details/data-collection/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling.\n\n\n\nBegin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work.\n\n\n\n\nDuring the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder\n\n\n\n\n\nYour data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "href": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#what-to-address",
    "href": "technical-details/data-collection/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#start-collecting-data",
    "href": "technical-details/data-collection/instructions.html#start-collecting-data",
    "title": "Instructions",
    "section": "",
    "text": "Begin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "href": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "title": "Instructions",
    "section": "",
    "text": "During the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#requirements",
    "href": "technical-details/data-collection/instructions.html#requirements",
    "title": "Instructions",
    "section": "",
    "text": "Your data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/overview.html",
    "href": "technical-details/data-collection/overview.html",
    "title": "Overview",
    "section": "",
    "text": "For the data collection process, I gathered multiple datasets, including arrest records from the ICJIA Arrest Explorer, exoneration data from the National Registry of Exonerations, incarceration demographics from the Prison Policy Initiative, and geospatial information. This diverse collection enabled an in-depth examination of systemic racial inequities that drive the over-policing of marginalized communities while also facilitating a critical analysis of nuanced racial patterns in exoneration data, exploring their connection to broader societal structures. Together, these datasets provide a framework for understanding the interconnected dynamics of racial injustice in the criminal justice system.\n\n\nMy goal in the data collection process is to gather the datasets needed to examine systemic racial disparities in policing and exonerations. I’m focused on sourcing reliable arrest records, exoneration data, incarceration demographics, and geospatial information. Ultimately, I aim to compile data that can stand alone or work together to analyze over-policing trends, uncover racial patterns in exonerations, and explore the geographic dynamics connected to these issues. I’m particularly driven by the need to ground these systemic inequities in hard numbers and statistical analysis. Too often, people refuse to acknowledge the reality of racism without concrete evidence. By putting real data behind these patterns, I hope to demonstrate the scope and impact of these injustices in a way that cannot be ignored.\n\n\n\n\nIdentify and collect datasets needed to analyze racial disparities in over-policing and wrongful convictions in Illinois, including arrest records, exoneration data, incarceration demographics, and geospatial information.\n\nEnsure datasets are accurate, up-to-date, and aligned with the project’s goals by prioritizing reliable sources like official registries and well-documented repositories.\n\nStructure the data to support analysis of racial patterns in policing and exonerations, allowing for both independent and interconnected evaluations.\n\nDocument all steps in the data collection process, including sourcing methods and preprocessing workflows, to ensure transparency and reproducibility.\n\nCollect and preprocess geocoded data and Illinois county shapefiles to enable geographic exploratory data analysis and visualizations."
  },
  {
    "objectID": "technical-details/eda/instructions.html",
    "href": "technical-details/eda/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/eda/instructions.html#suggested-page-structure",
    "href": "technical-details/eda/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/eda/instructions.html#what-to-address",
    "href": "technical-details/eda/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/progress-log.html",
    "href": "technical-details/progress-log.html",
    "title": "Progress log",
    "section": "",
    "text": "Use this page to track your progress and keep a log of your contributions to the project, please update this each time you work on your project, it is generally a good habit to adopt."
  },
  {
    "objectID": "technical-details/progress-log.html#to-do",
    "href": "technical-details/progress-log.html#to-do",
    "title": "Progress log",
    "section": "To-do",
    "text": "To-do\n\nWrite explanation prose for EDA\nMake edits to data cleaning document, including the inclusion of OM tags and then cleaning the arrest data + explain\nComplete data collection documents\nstart unsupervised learning\nfor supervised learning on original dataset to test probabiity of police misconduct by race"
  },
  {
    "objectID": "technical-details/progress-log.html#weekly-progress",
    "href": "technical-details/progress-log.html#weekly-progress",
    "title": "Progress log",
    "section": "Weekly Progress:",
    "text": "Weekly Progress:\nTue: 10-20-2024\n\nDecided on a project topic related to criminal justice reform, narrowing focus to over-policing, race, and poverty in Chicago.\n\nLogged my goal for his project and why the topic is extremely significant in project-planning.qmd.\n\nWed: 10-30:2024\n\nStarted the literature review by summarizing and analyzing key sources, including research on over-policing, wrongful convictions, and socioeconomic factors that contribute to disparities in Chicago’s criminal justice system.\n\nTue: 11-05-2024\n\nContinued review of literature, focusing on research about over-policing, racial disparities, and socioeconomic factors contributing to policing dynamics.\nSummarized and analyzed key sources using LLM, including comprehensive studies on systemic biases, police misconduct, wrongful convictions, and socioeconomic determinants of crime.\nAdded detailed notes from sources to my literature review.qmd file, focusing on findings relevant to Chicago, such as socioeconomic disparities, racial disparities, and historical context.\nCreated a references.bib and added references from literature review to ensure proper citation for each source, integrating new academic and news sources into my project plan.\nDocumented and expanded key insights from sources to inform project direction.\n\nFri: 11-08-2024\n\nMet with Prof. Jacobs to discuss project topic.\n\nMon: 11-11-2024\n\nMet with Prof. Jacobs to go over datasets and ask research questions.\nSelected datasets related to over-policing, race, and exonerations in Illinois.\n\nThurs: 11-21-2024\n\nLoaded in datasets and started cleaning exoneration data.\n\nMon: 11-25-2024\n\nOffice Hours with Prof. Jacobs to go over possibilities for supervised and unsupervised learning.\n\nThurs: 11-28-2024\n\nFinished Cleaning exoneration data.\nCreated about me page for multiclass-portfolio website.\n\nSun: 12-1-2024\n\nCompleted multi-class-portfolio website rough draft and pushed to GU domains.\nAdded in-line prose to data-cleaning exoneration data to explain data cleaning process.\nStarted EDA, beginning with a GIS to analyze the locations of arrests and exonerees\n\nTues: 12-3-2024\n\nMet with professor Jacobs to answer questions about my EDA and normalization/scaling + sharing misconduct results + help with supervised learning\nCompleted EDA (still have to add in-text prose to explain)\n\nSun: 12-8-2024 * [x] Creation of Exoneration Counterfactuals * [x] Completed Supervised Learning (still have to add in-text prose to explain)"
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html",
    "href": "technical-details/unsupervised-learning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "href": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/data-collection/main.html",
    "href": "technical-details/data-collection/main.html",
    "title": "Data Collection",
    "section": "",
    "text": "For the data collection process, I gathered multiple datasets, including arrest records from the ICJIA Arrest Explorer, exoneration data from the National Registry of Exonerations, incarceration demographics from the Prison Policy Initiative, and geospatial information. This diverse collection enabled an in-depth examination of systemic racial inequities that drive the over-policing of marginalized communities while also facilitating a critical analysis of nuanced racial patterns in exoneration data, exploring their connection to broader societal structures. Together, these datasets provide a framework for understanding the interconnected dynamics of racial injustice in the criminal justice system.\n\n\nMy goal in the data collection process is to gather the datasets needed to examine systemic racial disparities in policing and exonerations. I’m focused on sourcing reliable arrest records, exoneration data, incarceration demographics, and geospatial information. Ultimately, I aim to compile data that can stand alone or work together to analyze over-policing trends, uncover racial patterns in exonerations, and explore the geographic dynamics connected to these issues. I’m particularly driven by the need to ground these systemic inequities in hard numbers and statistical analysis. Too often, people refuse to acknowledge the reality of racism without concrete evidence. By putting real data behind these patterns, I hope to demonstrate the scope and impact of these injustices in a way that cannot be ignored.\n\n\n\n\nIdentify and collect datasets needed to analyze racial disparities in over-policing and wrongful convictions in Illinois, including arrest records, exoneration data, incarceration demographics, and geospatial information.\n\nEnsure datasets are accurate, up-to-date, and aligned with the project’s goals by prioritizing reliable sources like official registries and well-documented repositories.\n\nStructure the data to support analysis of racial patterns in policing and exonerations, allowing for both independent and interconnected evaluations.\n\nDocument all steps in the data collection process, including sourcing methods and preprocessing workflows, to ensure transparency and reproducibility.\n\nCollect and preprocess geocoded data and Illinois county shapefiles to enable geographic exploratory data analysis and visualizations."
  },
  {
    "objectID": "technical-details/data-collection/main.html#suggested-page-structure",
    "href": "technical-details/data-collection/main.html#suggested-page-structure",
    "title": "Data Collection",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag."
  },
  {
    "objectID": "technical-details/data-collection/main.html#what-to-address",
    "href": "technical-details/data-collection/main.html#what-to-address",
    "title": "Data Collection",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling."
  },
  {
    "objectID": "technical-details/data-collection/main.html#start-collecting-data",
    "href": "technical-details/data-collection/main.html#start-collecting-data",
    "title": "Data Collection",
    "section": "",
    "text": "Begin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work."
  },
  {
    "objectID": "technical-details/data-collection/main.html#saving-the-raw-data",
    "href": "technical-details/data-collection/main.html#saving-the-raw-data",
    "title": "Data Collection",
    "section": "",
    "text": "During the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder"
  },
  {
    "objectID": "technical-details/data-collection/main.html#requirements",
    "href": "technical-details/data-collection/main.html#requirements",
    "title": "Data Collection",
    "section": "",
    "text": "Your data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/main.html#example",
    "href": "technical-details/data-collection/main.html#example",
    "title": "Data Collection",
    "section": "Example",
    "text": "Example\nIn the following code, we first utilized the requests library to retrieve the HTML content from the Wikipedia page. Afterward, we employed BeautifulSoup to parse the HTML and locate the specific table of interest by using the find function. Once the table was identified, we extracted the relevant data by iterating through its rows, gathering country names and their respective populations. Finally, we used Pandas to store the collected data in a DataFrame, allowing for easy analysis and visualization. The data could also be optionally saved as a CSV file for further use."
  },
  {
    "objectID": "technical-details/data-collection/main.html#challenges",
    "href": "technical-details/data-collection/main.html#challenges",
    "title": "Data Collection",
    "section": "Challenges",
    "text": "Challenges\nThe foundation for meaningful data analysis is accurate and reliable crime data, yet long-standing challenges in criminal history record systems continue to undermine their precision and dependability7. As outlined in the Use and Management of Criminal History Record Information Report (1993), issues such as incomplete data reporting, delays in recording arrests and case dispositions, and inconsistent fingerprint submissions have plagued the system for decades7 . Criminal History Record Information (CHRI), the backbone for arrest datasets like those from the Illinois Criminal Justice Information Authority (ICJIA), is vulnerable to these shortcomings. Dispositions, or the outcomes of arrests, are often missing or delayed, leaving critical gaps in the data that make it difficult to analyze systemic trends. While advances in technology, such as digital fingerprinting, have improved some processes, data fragmentation and insufficient oversight remain persistent barriers to comprehensive and accurate reporting.\nFast forward thirty years, and many of these challenges remain unresolved—now further complicated by uneven implementation of modern systems. The Marshall Project highlights a striking example: in 2022, the FBI’s shift to the National Incident-Based Reporting System (NIBRS) created a significant data gap in national crime statistics8. Over 6,000 police agencies failed to submit their data, representing nearly one-third of all agencies and leaving vast portions of the U.S. population unaccounted for. This includes major departments like the NYPD and LAPD, alongside countless smaller agencies8. These gaps reflect broader systemic failures—inconsistent adoption of updated systems, lack of adequate funding, and minimal oversight—that echo the same issues identified decades ago.\nIn short, crime data, even when sourced from official systems, remains inherently flawed and incomplete. Whether due to outdated processes, inconsistent reporting practices, privacy-driven modifications, or gaps in modern collection systems, crime data often falls short of providing a fully accurate or comprehensive picture. As a result, any analysis relying on this data must account for these imperfections, recognizing that while the data can uncover critical trends and systemic disparities, it is rarely a perfect representation of reality.\n\nArrest Data Precision\nThe Illinois arrest dataset obtained from the ICJIA Arrest Explorer highlights the trade-offs between privacy and precision, adding to the broader challenges of criminal justice data. To protect confidentiality, counts under 10 are approximated—values between 0 and 4 are replaced with 1, while counts from 5 to 9 are replaced with 6. Though this approach is necessary to safeguard sensitive data, it can introduce distortions, particularly in smaller counties or demographic groups, where even slight approximations can significantly skew trends and reduce the accuracy of analysis. Compounding these issues, the lack of oversight in reporting further undermines data consistency and reliability, reflecting the broader systemic limitations that continue to plague many criminal justice datasets."
  },
  {
    "objectID": "technical-details/data-collection/main.html#benchmarks",
    "href": "technical-details/data-collection/main.html#benchmarks",
    "title": "Data Collection",
    "section": "Benchmarks",
    "text": "Benchmarks\nThe National Registry of Exonerations is a trusted and widely used resource, frequently cited in academic and legal research. Unlike government databases, the Registry is run by a team of researchers and academics, which brings a level of precision and thoroughness often missing in state-managed systems9. Its foundation in meticulous documentation and independent research makes it less vulnerable to political or institutional biases. As a result, the Registry stands out as a more reliable and comprehensive tool for understanding wrongful convictions."
  },
  {
    "objectID": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "title": "Data Collection",
    "section": "Conclusion and Future Steps",
    "text": "Conclusion and Future Steps\nThe data collected from various sources provide a foundation for examining systemic racial disparities in over-policing and wrongful convictions. However, challenges such as the lack of precision in Illinois arrest datasets and broader gaps in national crime reporting underscore the need to approach findings with caution. Even with these limitations, the use of reliable resources like the National Registry of Exonerations adds credibility to the analysis. Moving forward, future research could focus on compiling crime data from a single state across local, state, and federal agencies to better understand inconsistencies and uncover patterns of underreporting. Expanding this work to compare data collection practices across states could also reveal regional differences in crime reporting and highlight systemic disparities in how data is recorded and shared."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html",
    "href": "technical-details/supervised-learning/main.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results.\n\n\n\n\nNormalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling.\n\n\n\n\n\nModel Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used\n\n\n\n\n\nSplit Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset.\n\n\n\n\n\nBinary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc.\n\n\n\n\n\nModel Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots).\n\n\n\n\n\nResult Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#suggested-page-structure",
    "href": "technical-details/supervised-learning/main.html#suggested-page-structure",
    "title": "Supervised Learning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#what-to-address",
    "href": "technical-details/supervised-learning/main.html#what-to-address",
    "title": "Supervised Learning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#data-preprocessing",
    "href": "technical-details/supervised-learning/main.html#data-preprocessing",
    "title": "Supervised Learning",
    "section": "",
    "text": "Normalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-selection",
    "href": "technical-details/supervised-learning/main.html#model-selection",
    "title": "Supervised Learning",
    "section": "",
    "text": "Model Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#training-and-testing-strategy",
    "href": "technical-details/supervised-learning/main.html#training-and-testing-strategy",
    "title": "Supervised Learning",
    "section": "",
    "text": "Split Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-evaluation-metrics",
    "href": "technical-details/supervised-learning/main.html#model-evaluation-metrics",
    "title": "Supervised Learning",
    "section": "",
    "text": "Binary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#results",
    "href": "technical-details/supervised-learning/main.html#results",
    "title": "Supervised Learning",
    "section": "",
    "text": "Model Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots)."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#discussion",
    "href": "technical-details/supervised-learning/main.html#discussion",
    "title": "Supervised Learning",
    "section": "",
    "text": "Result Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#cleaning-the-tags-and-om-tags-columns",
    "href": "technical-details/data-cleaning/main.html#cleaning-the-tags-and-om-tags-columns",
    "title": "Data Cleaning",
    "section": "",
    "text": "The tags and OM-tags columns contain important categorical information about each exoneration case. To make this data more useful for analysis, both columns were transformed into multiple binary columns, where each tag indicates the presence (1) or absence (0) of a specific feature. Additionally, a tag_sum column was created to capture the total number of tags associated with each case, providing a summary metric.\nThe cleaning process involved the following steps:\n\nRemoving Unnecessary Characters:\nUnwanted characters such as # were removed, and delimiters were standardized to ensure consistency in the data.\nSplitting Tags:\nThe tags and OM-tags columns were split into individual values to facilitate binary encoding.\nRenaming Binary Columns:\nEach binary column was renamed using clear and descriptive labels by mapping the original tags to their definitions. This mapping process translated short tag codes into their full meanings, improving interpretability. For reference, the definitions of the tags are based on the descriptions provided by the National Registry of Exonerations2.\nAdding a tag_sum Column:\nA new column was created to calculate the total number of tags for each case, enabling easier analysis of case complexity.\n\nThis transformation ensures the data is well-structured and ready for exploratory analysis, providing detailed insights into the systemic patterns in exoneration cases.\n\n# Clean 'tags' column:\ndf['tags'] = df['tags'].str.replace('#', '', regex=False).str.replace(\";\", \",\")\ndf['OM-tags'] = df['om_tags'].str.replace('#', '', regex=False).str.replace(\";\", \",\")\n\n# Define the mapping for tags:\ntag_mapping = {\n    \"A\": \"arson\",\n    \"BM\": \"bitemark\",\n    \"CDC\": \"co_defendant_confessed\",\n    \"CIU\": \"conviction_integrity_unit\",\n    \"CSH\": \"child_sex_abuse_hysteria_case\",\n    \"CV\": \"child_victim\",\n    \"F\": \"female_exoneree\",\n    \"FED\": \"federal_case\",\n    \"H\": \"homicide\",\n    \"IO\": \"innocence_organization\",\n    \"JI\": \"jailhouse_informant\",\n    \"JV\": \"juvenile_defendant\",\n    \"M\": \"misdemeanor\",\n    \"NC\": \"no_crime_case\",\n    \"P\": \"guilty_plea_case\",\n    \"PH\": \"posthumous_exoneration\",\n    \"SA\": \"sexual_assault\",\n    \"SBS\": \"shaken_baby_syndrome_case\",\n    \"PR\": \"prosecutor_misconduct\",\n    \"OF\": \"police_officer_misconduct\",\n    \"FA\": \"forensic_analyst_misconduct\",\n    \"CW\": \"child_welfare_worker_misconduct\",\n    \"WH\": \"withheld_exculpatory_evidence\",\n    \"NW\": \"misconduct_that_is_not_withholding_evidence\",\n    \"KP\": \"knowingly_permitting_perjury\",\n    \"WT\": \"witness_tampering_or_misconduct_interrogating_co_defendant\",\n    \"INT\": \"misconduct_in_interrogation_of_exoneree\",\n    \"PJ\": \"perjury_by_official\",\n    \"PL\": \"prosecutor_lied_in_court\"\n}\n\n# Split 'tags' and 'OM-tags' into lists:\ndf['tags'] = df['tags'].apply(lambda x: x.split(',') if isinstance(x, str) else x)\ndf['OM-tags'] = df['OM-tags'].apply(lambda x: x.split(',') if isinstance(x, str) else x)\n\n# Create binary columns for tags from both 'tags' and 'OM-tags':\nfor tag in tag_mapping.keys():\n    # Check if the tag exists in 'tags' or 'OM-tags':\n    df[tag] = df.apply(\n        lambda row: 1 if (isinstance(row['tags'], list) and tag in row['tags']) or \n                          (isinstance(row['OM-tags'], list) and tag in row['OM-tags']) else 0,\n        axis=1\n    )\n\n# Rename the binary columns using the tag_mapping dictionary:\ndf.rename(columns=tag_mapping, inplace=True)\n\n# Create `tag_sum` column to count the total number of tags for each exoneree:\ndf['tag_sum'] = df[list(tag_mapping.values())].sum(axis=1)\n\n# Drop the original 'tags' and 'OM-tags' columns:\ndf.drop(columns=['tags', 'om_tags', 'OM-tags'], inplace=True)\n\ndf.head()  \n\n\n\n\n\n\n\n\nlast_name\nfirst_name\nage\nrace\nsex\nstate\ncounty\nworst_crime_display\nsentence\nposting_date\nom\ndate_of_exoneration\ndate_of_1st_conviction\ndate_of_release\nsentence_in_years\narson\nbitemark\nco_defendant_confessed\nconviction_integrity_unit\nchild_sex_abuse_hysteria_case\nchild_victim\nfemale_exoneree\nfederal_case\nhomicide\ninnocence_organization\njailhouse_informant\njuvenile_defendant\nmisdemeanor\nno_crime_case\nguilty_plea_case\nposthumous_exoneration\nsexual_assault\nshaken_baby_syndrome_case\nprosecutor_misconduct\npolice_officer_misconduct\nforensic_analyst_misconduct\nchild_welfare_worker_misconduct\nwithheld_exculpatory_evidence\nmisconduct_that_is_not_withholding_evidence\nknowingly_permitting_perjury\nwitness_tampering_or_misconduct_interrogating_co_defendant\nmisconduct_in_interrogation_of_exoneree\nperjury_by_official\nprosecutor_lied_in_court\ntag_sum\n\n\n\n\n1\nAbbott\nCinque\n19.0\nBlack\nmale\nIllinois\nCook\nDrug Possession or Sale\nProbation\n2022-02-14\nOM\n2022-02-01\n2008-03-25\n2008-03-25\n0.0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n7\n\n\n3\nAbernathy\nChristopher\n17.0\nWhite\nmale\nIllinois\nCook\nMurder\nLife without parole\n2015-02-13\nOM\n2015-02-11\n1987-01-15\n2015-02-11\n100.0\n0\n0\n0\n1\n0\n1\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n1\n0\n0\n1\n0\n0\n10\n\n\n5\nAbrego\nEruby\n20.0\nHispanic\nmale\nIllinois\nCook\nMurder\n90 years\n2022-08-25\nOM\n2022-07-21\n2004-09-22\n2022-07-21\n90.0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n1\n1\n1\n0\n9\n\n\n10\nAdams\nDemetris\n22.0\nBlack\nmale\nIllinois\nCook\nDrug Possession or Sale\n1 year\n2020-04-13\nOM\n2020-02-11\n2004-09-08\n2004-12-26\n1.0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n7\n\n\n15\nAdams\nKenneth\n22.0\nBlack\nmale\nIllinois\nCook\nMurder\n75 years\n2011-08-29\nOM\n1996-07-02\n1978-10-20\n1996-06-14\n75.0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n1\n0\n1\n1\n0\n0\n1\n1\n1\n1\n0\n0\n0\n11\n\n\n\n\n\n\n\n\n# Convert 'OM' column to binary (1 if \"OM\" is present, 0 otherwise)\ndf['om'] = df['om'].apply(lambda x: 1 if str(x).strip().upper() == \"OM\" else 0)\n\n# Verify the transformation\nprint(df['om'].value_counts())\n\nom\n1    478\n0     70\nName: count, dtype: int64"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#calculating-years-lost",
    "href": "technical-details/data-cleaning/main.html#calculating-years-lost",
    "title": "Data Cleaning",
    "section": "",
    "text": "To quantify the years_lost due to wrongful conviction, this step calculates the difference in years between an individual’s date_of_1st_conviction and their date_of_release.\n\n# Calculate \"years lost\" as the difference in years between release and conviction:\ndf['years_lost'] = (df['date_of_release'] - df['date_of_1st_conviction']).dt.days / 365.25 # Dividing by 365.25 accounts for leap years\n\n# Round the years lost to 2 decimal places:\ndf['years_lost'] = df['years_lost'].round(2)\n\n# Updated DataFrame:\nprint(df[['date_of_1st_conviction', 'date_of_release', 'years_lost']])\n\n    date_of_1st_conviction date_of_release  years_lost\n0               2008-03-25      2008-03-25        0.00\n1               1987-01-15      2015-02-11       28.07\n2               2004-09-22      2022-07-21       17.83\n3               2004-09-08      2004-12-26        0.30\n4               1978-10-20      1996-06-14       17.65\n..                     ...             ...         ...\n543             2005-04-13      2005-04-13        0.00\n544             2005-01-11      2006-07-12        1.50\n545             2005-04-11      2006-12-07        1.66\n546             2003-04-21      2005-03-10        1.89\n547             1994-09-20      2005-01-31       10.37\n\n[548 rows x 3 columns]"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#introduction-and-motivation-1",
    "href": "technical-details/data-cleaning/main.html#introduction-and-motivation-1",
    "title": "Data Cleaning",
    "section": "Introduction and Motivation",
    "text": "Introduction and Motivation\nThis section documents the steps taken to clean and preprocess the Illinois arrest dataset. Similarly to the U.S. exoneration dataset, the goal is to transform the raw data into a structured and reliable format for analysis. However, this dataset required fewer cleaning steps due to its uniform structure and consistent formatting.\nBy the end of this phase, the Illinois arrest dataset will be well-prepared for exploratory data analysis (EDA) and integration into broader investigative workflows."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#initial-overview-of-the-dataset-1",
    "href": "technical-details/data-cleaning/main.html#initial-overview-of-the-dataset-1",
    "title": "Data Cleaning",
    "section": "Initial Overview of the Dataset",
    "text": "Initial Overview of the Dataset\nThe dataset is first loaded to inspect its structure and contents.\n\n# Load exoneration dataset:\narrest_data = pd.read_csv('../../data/raw-data/illinois_arrest_explorer_data.csv')\nprint(\"Initial Dataset: \")\npd.set_option('display.max_columns', None) # Enables display of every column\narrest_data.head()\n\nInitial Dataset: \n\n\n\n\n\n\n\n\n\nYear\nrace\ncounty_Adams\ncounty_Alexander\ncounty_Bond\ncounty_Boone\ncounty_Brown\ncounty_Bureau\ncounty_Calhoun\ncounty_Carroll\ncounty_Cass\ncounty_Champaign\ncounty_Christian\ncounty_Clark\ncounty_Clay\ncounty_Clinton\ncounty_Coles\ncounty_Cook Chicago\ncounty_Cook County Suburbs\ncounty_Crawford\ncounty_Cumberland\ncounty_Dekalb\ncounty_Dewitt\ncounty_Douglas\ncounty_Dupage\ncounty_Edgar\ncounty_Edwards\ncounty_Effingham\ncounty_Fayette\ncounty_Ford\ncounty_Franklin\ncounty_Fulton\ncounty_Gallatin\ncounty_Greene\ncounty_Grundy\ncounty_Hamilton\ncounty_Hancock\ncounty_Hardin\ncounty_Henderson\ncounty_Henry\ncounty_Iroquois\ncounty_Jackson\ncounty_Jasper\ncounty_Jefferson\ncounty_Jersey\ncounty_Jo Daviess\ncounty_Johnson\ncounty_Kane\ncounty_Kankakee\ncounty_Kendall\ncounty_Knox\ncounty_Lake\ncounty_Lasalle\ncounty_Lawrence\ncounty_Lee\ncounty_Livingston\ncounty_Logan\ncounty_Macon\ncounty_Macoupin\ncounty_Madison\ncounty_Marion\ncounty_Marshall\ncounty_Mason\ncounty_Massac\ncounty_Mcdonough\ncounty_Mchenry\ncounty_Mclean\ncounty_Menard\ncounty_Mercer\ncounty_Monroe\ncounty_Montgomery\ncounty_Morgan\ncounty_Moultrie\ncounty_Non County Agencies\ncounty_Ogle\ncounty_Peoria\ncounty_Perry\ncounty_Piatt\ncounty_Pike\ncounty_Pope\ncounty_Pulaski\ncounty_Putnam\ncounty_Randolph\ncounty_Richland\ncounty_Rock Island\ncounty_Saline\ncounty_Sangamon\ncounty_Schuyler\ncounty_Scott\ncounty_Shelby\ncounty_St. Clair\ncounty_Stark\ncounty_Stephenson\ncounty_Tazewell\ncounty_Union\ncounty_Vermilion\ncounty_Wabash\ncounty_Warren\ncounty_Washington\ncounty_Wayne\ncounty_White\ncounty_Whiteside\ncounty_Will\ncounty_Williamson\ncounty_Winnebago\ncounty_Woodford\n\n\n\n\n0\n2001\nAfrican American\n226\n147\n25\n18\n18\n48\n6\n12\n1\n2059\n16\n6\n1\n22\n130\n86781\n26301\n6\n1\n342\n11\n18\n2265\n1\n1\n108\n6\n1\n6\n25\n1\n1\n52\n1\n1\n1\n6\n92\n250\n580\n1\n246\n6\n16\n16\n2804\n1669\n122\n314\n5712\n184\n6\n86\n180\n71\n1104\n34\n1567\n146\n6\n1\n91\n105\n213\n905\n1\n6\n20\n38\n287\n6\n1\n57\n3403\n60\n1\n13\n1\n153\n1\n81\n6\n1215\n104\n2200\n6\n1\n1\n2462\n1\n355\n168\n17\n876\n6\n46\n25\n6\n16\n128\n3000\n75\n2509\n42\n\n\n1\n2001\nAsian\n1\n1\n1\n1\n1\n1\n1\n1\n1\n49\n1\n1\n1\n1\n1\n722\n460\n1\n1\n15\n1\n1\n139\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n97\n1\n1\n1\n135\n1\n1\n1\n1\n1\n1\n1\n12\n1\n1\n1\n1\n6\n6\n16\n1\n1\n1\n1\n1\n1\n1\n1\n12\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n17\n1\n1\n1\n6\n1\n1\n6\n1\n1\n1\n1\n1\n1\n1\n1\n14\n1\n28\n1\n\n\n2\n2001\nHispanic\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n3\n2001\nNative American\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n146\n30\n1\n1\n1\n1\n1\n17\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n16\n1\n1\n1\n10\n1\n1\n1\n1\n1\n1\n1\n6\n1\n1\n1\n1\n1\n1\n6\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n6\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n6\n1\n\n\n4\n2001\nWhite\n939\n52\n209\n674\n107\n509\n139\n298\n88\n2334\n335\n264\n117\n229\n1635\n37099\n36690\n470\n164\n1740\n539\n181\n13019\n301\n41\n986\n382\n217\n724\n926\n19\n57\n1014\n78\n206\n75\n134\n569\n585\n857\n87\n607\n575\n532\n198\n9634\n1607\n1465\n1160\n15136\n2689\n412\n845\n1146\n586\n1388\n523\n4359\n571\n263\n274\n455\n1007\n3879\n2285\n92\n410\n572\n749\n1053\n149\n1\n1009\n3120\n430\n211\n368\n45\n248\n151\n460\n323\n3117\n711\n3635\n274\n22\n486\n1601\n59\n644\n1804\n327\n1743\n324\n444\n265\n245\n668\n1304\n4144\n631\n4253\n462"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#column-renaming-and-consolidation",
    "href": "technical-details/data-cleaning/main.html#column-renaming-and-consolidation",
    "title": "Data Cleaning",
    "section": "Column Renaming and Consolidation",
    "text": "Column Renaming and Consolidation\nTo streamline the Illinois arrest dataset and improve clarity, the following transformations were applied:\n\nRemoving Prefixes for Simplicity:\nColumns with the county_ prefix were renamed by removing the prefix and capitalizing the remaining column names. This makes the column headers cleaner and more intuitive for analysis.\nStandardizing Race Terminology:\nThe term “African American” in the race column was replaced with “Black” to ensure consistency with the exoneration dataset.\nConsolidating Cook County Data:\nThe columns Cook Chicago and Cook County Suburbs were combined into a single Cook column. This consolidation simplifies the data and groups all arrest information related to Cook County into a unified metric.\n\n\n# Rename columns by removing 'county_' prefix:\narrest_data.columns = [col.replace('county_', '').capitalize() if col.startswith('county_') else col for col in arrest_data.columns]\n\n# Rename \"African American\" to \"Black\" in the column names:\narrest_data['race'] = arrest_data['race'].replace('African American', 'Black')\n\n# Combine \"Cook Chicago\" and \"Cook County Suburbs\" into a single \"Cook\" column:\narrest_data['Cook'] = arrest_data['Cook chicago'] + arrest_data['Cook county suburbs']\n\n# Drop the old columns:\narrest_data.drop(columns=['Cook chicago', 'Cook county suburbs'], inplace=True)\n\narrest_data.head(1)\n\n\n\n\n\n\n\n\nYear\nrace\nAdams\nAlexander\nBond\nBoone\nBrown\nBureau\nCalhoun\nCarroll\nCass\nChampaign\nChristian\nClark\nClay\nClinton\nColes\nCrawford\nCumberland\nDekalb\nDewitt\nDouglas\nDupage\nEdgar\nEdwards\nEffingham\nFayette\nFord\nFranklin\nFulton\nGallatin\nGreene\nGrundy\nHamilton\nHancock\nHardin\nHenderson\nHenry\nIroquois\nJackson\nJasper\nJefferson\nJersey\nJo daviess\nJohnson\nKane\nKankakee\nKendall\nKnox\nLake\nLasalle\nLawrence\nLee\nLivingston\nLogan\nMacon\nMacoupin\nMadison\nMarion\nMarshall\nMason\nMassac\nMcdonough\nMchenry\nMclean\nMenard\nMercer\nMonroe\nMontgomery\nMorgan\nMoultrie\nNon county agencies\nOgle\nPeoria\nPerry\nPiatt\nPike\nPope\nPulaski\nPutnam\nRandolph\nRichland\nRock island\nSaline\nSangamon\nSchuyler\nScott\nShelby\nSt. clair\nStark\nStephenson\nTazewell\nUnion\nVermilion\nWabash\nWarren\nWashington\nWayne\nWhite\nWhiteside\nWill\nWilliamson\nWinnebago\nWoodford\nCook\n\n\n\n\n0\n2001\nBlack\n226\n147\n25\n18\n18\n48\n6\n12\n1\n2059\n16\n6\n1\n22\n130\n6\n1\n342\n11\n18\n2265\n1\n1\n108\n6\n1\n6\n25\n1\n1\n52\n1\n1\n1\n6\n92\n250\n580\n1\n246\n6\n16\n16\n2804\n1669\n122\n314\n5712\n184\n6\n86\n180\n71\n1104\n34\n1567\n146\n6\n1\n91\n105\n213\n905\n1\n6\n20\n38\n287\n6\n1\n57\n3403\n60\n1\n13\n1\n153\n1\n81\n6\n1215\n104\n2200\n6\n1\n1\n2462\n1\n355\n168\n17\n876\n6\n46\n25\n6\n16\n128\n3000\n75\n2509\n42\n113082"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#handling-placeholder-values",
    "href": "technical-details/data-cleaning/main.html#handling-placeholder-values",
    "title": "Data Cleaning",
    "section": "Handling Placeholder Values",
    "text": "Handling Placeholder Values\nDuring preprocessing, it was observed that the dataset contains 1s in certain fields. To address this all 1s in the dataset were replaced with 0s to ensure the integrity of the analysis. This decision ensures that the data is not skewed by suspicious or placeholder values, which could misrepresent trends or introduce bias into the results.\n\nNote: The reasoning behind the presence of these placeholder values is discussed further in the Data Collection section.\n\n\n# Replace 1s with 0s: \narrest_data.replace(1, 0, inplace=True)\n\n# Display the updated dataset to confirm changes:\narrest_data.head()\n\narrest_data.head()\n\n\n\n\n\n\n\n\nYear\nrace\nAdams\nAlexander\nBond\nBoone\nBrown\nBureau\nCalhoun\nCarroll\nCass\nChampaign\nChristian\nClark\nClay\nClinton\nColes\nCrawford\nCumberland\nDekalb\nDewitt\nDouglas\nDupage\nEdgar\nEdwards\nEffingham\nFayette\nFord\nFranklin\nFulton\nGallatin\nGreene\nGrundy\nHamilton\nHancock\nHardin\nHenderson\nHenry\nIroquois\nJackson\nJasper\nJefferson\nJersey\nJo daviess\nJohnson\nKane\nKankakee\nKendall\nKnox\nLake\nLasalle\nLawrence\nLee\nLivingston\nLogan\nMacon\nMacoupin\nMadison\nMarion\nMarshall\nMason\nMassac\nMcdonough\nMchenry\nMclean\nMenard\nMercer\nMonroe\nMontgomery\nMorgan\nMoultrie\nNon county agencies\nOgle\nPeoria\nPerry\nPiatt\nPike\nPope\nPulaski\nPutnam\nRandolph\nRichland\nRock island\nSaline\nSangamon\nSchuyler\nScott\nShelby\nSt. clair\nStark\nStephenson\nTazewell\nUnion\nVermilion\nWabash\nWarren\nWashington\nWayne\nWhite\nWhiteside\nWill\nWilliamson\nWinnebago\nWoodford\nCook\n\n\n\n\n0\n2001\nBlack\n226\n147\n25\n18\n18\n48\n6\n12\n0\n2059\n16\n6\n0\n22\n130\n6\n0\n342\n11\n18\n2265\n0\n0\n108\n6\n0\n6\n25\n0\n0\n52\n0\n0\n0\n6\n92\n250\n580\n0\n246\n6\n16\n16\n2804\n1669\n122\n314\n5712\n184\n6\n86\n180\n71\n1104\n34\n1567\n146\n6\n0\n91\n105\n213\n905\n0\n6\n20\n38\n287\n6\n0\n57\n3403\n60\n0\n13\n0\n153\n0\n81\n6\n1215\n104\n2200\n6\n0\n0\n2462\n0\n355\n168\n17\n876\n6\n46\n25\n6\n16\n128\n3000\n75\n2509\n42\n113082\n\n\n1\n2001\nAsian\n0\n0\n0\n0\n0\n0\n0\n0\n0\n49\n0\n0\n0\n0\n0\n0\n0\n15\n0\n0\n139\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n97\n0\n0\n0\n135\n0\n0\n0\n0\n0\n0\n0\n12\n0\n0\n0\n0\n6\n6\n16\n0\n0\n0\n0\n0\n0\n0\n0\n12\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n17\n0\n0\n0\n6\n0\n0\n6\n0\n0\n0\n0\n0\n0\n0\n0\n14\n0\n28\n0\n1182\n\n\n2\n2001\nHispanic\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n\n\n3\n2001\nNative American\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n17\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n16\n0\n0\n0\n10\n0\n0\n0\n0\n0\n0\n0\n6\n0\n0\n0\n0\n0\n0\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n6\n0\n176\n\n\n4\n2001\nWhite\n939\n52\n209\n674\n107\n509\n139\n298\n88\n2334\n335\n264\n117\n229\n1635\n470\n164\n1740\n539\n181\n13019\n301\n41\n986\n382\n217\n724\n926\n19\n57\n1014\n78\n206\n75\n134\n569\n585\n857\n87\n607\n575\n532\n198\n9634\n1607\n1465\n1160\n15136\n2689\n412\n845\n1146\n586\n1388\n523\n4359\n571\n263\n274\n455\n1007\n3879\n2285\n92\n410\n572\n749\n1053\n149\n0\n1009\n3120\n430\n211\n368\n45\n248\n151\n460\n323\n3117\n711\n3635\n274\n22\n486\n1601\n59\n644\n1804\n327\n1743\n324\n444\n265\n245\n668\n1304\n4144\n631\n4253\n462\n73789"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#exporting-cleaned-and-aggregated-datasets",
    "href": "technical-details/data-cleaning/main.html#exporting-cleaned-and-aggregated-datasets",
    "title": "Data Cleaning",
    "section": "Exporting Cleaned and Aggregated Datasets",
    "text": "Exporting Cleaned and Aggregated Datasets\n\n1. Exporting the Cleaned Dataset\nAfter cleaning the Illinois arrest dataset, the cleaned version was saved as a CSV file. This dataset retains the original structure, including arrest counts broken down by race, county, and year. Maintaining the year-specific information allows for detailed time-series analysis and year-over-year comparisons.\n\narrest_data.to_csv('../../data/processed-data/arrest_data_by_year.csv', index=False)\nprint(\"Data saved to 'arrest_data_by_year.csv'\")\n\nData saved to 'arrest_data_by_year.csv'\n\n\n\n\n2. Aggregating Arrest Data by Race and County\nThe arrest data was aggregated to simplify the analysis by summing totals across all years for each race and county. This step removes the Year column and groups arrests solely by race and county.\nSteps:\n- Grouped the dataset by the race column.\n- Summed arrest counts for each county across all years.\n- Reset the index for a clean, tabular structure.\n\n# Aggregate totals across years for each race:\naggregated_data = arrest_data.groupby('race').sum(numeric_only=True)\naggregated_data = aggregated_data.drop(columns=['Year'])\naggregated_data = aggregated_data.reset_index()\n\n# Preview the aggregated data:\naggregated_data.head()\n\n\n\n\n\n\n\n\nrace\nAdams\nAlexander\nBond\nBoone\nBrown\nBureau\nCalhoun\nCarroll\nCass\nChampaign\nChristian\nClark\nClay\nClinton\nColes\nCrawford\nCumberland\nDekalb\nDewitt\nDouglas\nDupage\nEdgar\nEdwards\nEffingham\nFayette\nFord\nFranklin\nFulton\nGallatin\nGreene\nGrundy\nHamilton\nHancock\nHardin\nHenderson\nHenry\nIroquois\nJackson\nJasper\nJefferson\nJersey\nJo daviess\nJohnson\nKane\nKankakee\nKendall\nKnox\nLake\nLasalle\nLawrence\nLee\nLivingston\nLogan\nMacon\nMacoupin\nMadison\nMarion\nMarshall\nMason\nMassac\nMcdonough\nMchenry\nMclean\nMenard\nMercer\nMonroe\nMontgomery\nMorgan\nMoultrie\nNon county agencies\nOgle\nPeoria\nPerry\nPiatt\nPike\nPope\nPulaski\nPutnam\nRandolph\nRichland\nRock island\nSaline\nSangamon\nSchuyler\nScott\nShelby\nSt. clair\nStark\nStephenson\nTazewell\nUnion\nVermilion\nWabash\nWarren\nWashington\nWayne\nWhite\nWhiteside\nWill\nWilliamson\nWinnebago\nWoodford\nCook\n\n\n\n\n0\nAsian\n12\n0\n0\n22\n0\n0\n0\n0\n0\n1186\n0\n0\n0\n6\n0\n0\n0\n301\n0\n0\n4509\n6\n0\n51\n6\n0\n12\n0\n0\n0\n12\n0\n0\n0\n0\n12\n0\n166\n0\n12\n0\n0\n0\n2239\n30\n66\n0\n2562\n164\n0\n0\n81\n18\n76\n0\n550\n0\n0\n0\n45\n66\n498\n564\n0\n0\n0\n18\n12\n0\n0\n6\n381\n6\n0\n0\n0\n12\n0\n0\n0\n250\n0\n500\n0\n0\n0\n382\n0\n12\n126\n0\n0\n0\n42\n0\n0\n0\n30\n1443\n36\n767\n46\n26764\n\n\n1\nBlack\n4022\n2722\n533\n2410\n92\n780\n18\n292\n235\n45547\n459\n187\n75\n877\n4029\n280\n117\n16146\n594\n499\n59262\n119\n6\n3293\n588\n275\n520\n627\n6\n110\n2249\n0\n122\n0\n207\n2131\n3283\n13120\n45\n5960\n526\n943\n281\n59806\n27437\n4308\n7545\n78841\n7344\n337\n1654\n4521\n2353\n27094\n818\n51485\n4542\n123\n218\n1731\n4214\n6581\n27867\n189\n268\n885\n1275\n7157\n165\n693\n1685\n79960\n1901\n358\n519\n18\n3384\n42\n1731\n213\n26048\n1795\n63703\n81\n0\n110\n60744\n24\n11866\n8363\n399\n17641\n153\n1024\n489\n91\n464\n2893\n58968\n4809\n55837\n2841\n1772659\n\n\n2\nHispanic\n6\n0\n6\n516\n0\n114\n0\n34\n68\n983\n0\n6\n0\n91\n120\n18\n18\n1783\n84\n147\n13828\n0\n0\n51\n35\n67\n22\n0\n0\n30\n470\n0\n6\n0\n11\n18\n104\n186\n0\n160\n24\n101\n0\n14151\n634\n264\n323\n10219\n762\n0\n136\n417\n49\n113\n0\n333\n25\n12\n10\n0\n138\n4126\n1439\n0\n0\n16\n12\n124\n0\n0\n350\n0\n43\n0\n18\n0\n104\n0\n79\n18\n1135\n0\n29\n12\n0\n0\n449\n0\n39\n192\n18\n0\n0\n142\n12\n0\n6\n436\n3822\n217\n3522\n63\n144574\n\n\n3\nNative American\n0\n0\n0\n0\n0\n0\n0\n0\n0\n297\n0\n0\n0\n0\n0\n0\n0\n30\n0\n0\n703\n0\n0\n0\n0\n0\n0\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n149\n0\n12\n0\n220\n6\n0\n0\n0\n0\n6\n0\n65\n0\n0\n0\n0\n0\n18\n24\n0\n0\n0\n0\n0\n0\n0\n0\n42\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n29\n0\n0\n6\n42\n0\n6\n6\n0\n0\n0\n0\n0\n0\n0\n0\n108\n0\n147\n0\n2351\n\n\n4\nWhite\n16743\n1340\n3964\n16580\n1812\n9593\n1844\n5530\n2826\n41996\n10983\n6714\n4818\n8858\n23779\n8663\n4228\n35622\n7121\n5676\n215397\n7031\n1019\n23978\n9115\n4630\n22754\n14545\n838\n5147\n18813\n1218\n5203\n1766\n2653\n11472\n11041\n16698\n3124\n14526\n14337\n7841\n3325\n170618\n31714\n27222\n20553\n188224\n54179\n8499\n13856\n20172\n14366\n30283\n12071\n130786\n19686\n4240\n6970\n8491\n18618\n101548\n48554\n3974\n7257\n10734\n16631\n23129\n2920\n751\n19240\n64589\n12123\n4323\n7890\n795\n4457\n1683\n12061\n6823\n53728\n12729\n82714\n2136\n585\n8671\n49390\n1442\n13615\n61546\n6165\n30772\n6491\n7459\n5220\n5146\n11318\n22690\n97863\n26871\n85292\n12676\n1174775\n\n\n\n\n\n\n\n\naggregated_data.to_csv('../../data/processed-data/aggregated_arrests_2001_to_2021.csv', index=False)\nprint(\"Data saved to 'aggregated_arrests_2001_to_2021.csv'\")\n\nData saved to 'aggregated_arrests_2001_to_2021.csv'"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#summary-and-next-steps-1",
    "href": "technical-details/data-cleaning/main.html#summary-and-next-steps-1",
    "title": "Data Cleaning",
    "section": "Summary and Next Steps",
    "text": "Summary and Next Steps\nThis data cleaning process ensures the Illinois arrest dataset is free of inconsistencies and properly structured for analysis. The cleaned data will now be used to conduct exploratory data analysis (EDA) and investigate patterns of overpolicing, racial disparities, and geographic trends in arrest data across Illinois."
  },
  {
    "objectID": "technical-details/data-balancing/main.html",
    "href": "technical-details/data-balancing/main.html",
    "title": "Counterfactual Data Balancing",
    "section": "",
    "text": "In this project, I set out to create a balanced dataset that would support supervised learning models for predicting the factors linked to exonerations. At the heart of this process is counterfactual balancing: building a dataset that includes exonerated individuals alongside a comparable group of non-exonerated individuals, drawn to reflect the broader incarcerated population in Illinois. This balance is critical—it allows the model to make fair and meaningful comparisons when identifying patterns and predictors of exoneration outcomes.\n\n\nCounterfactual data is a necessity when access to complete prison population records is unavailable. Since I don’t have access to a full dataset of all incarcerated individuals in Illinois and their exoneration statuses (e.g., ‘exonerated’, ‘not exonerate’), I relied on counterfactuals to bridge the gap and construct a balanced dataset.\nThe idea behind counterfactuals is simple: they allow us to ask “what if?” questions. For example: What if an exonerated person had not been exonerated? Would their characteristics look similar to non-exonerated individuals? Counterfactual data helps isolate these comparisons by holding everything else constant except the hypothetical condition—in this case, exoneration.\nAs explained in this primer on counterfactuals, a counterfactual statement operates on an unrealized “if” condition. The “if” portion, also known as the antecedent, frames the comparison: exonerated individuals versus those who weren’t. This approach is powerful because it reduces bias and ensures that the model is trained on data that is reliable, balanced, and representative.1\n\n\nThe implementation of this counterfactual data balancing relied heavily on expert guidance and code contributions from Professor Jeff Jacobs. His insights and support were invaluable in refining the methodology and making this process possible."
  },
  {
    "objectID": "technical-details/data-balancing/main.html#why-counterfactual-data",
    "href": "technical-details/data-balancing/main.html#why-counterfactual-data",
    "title": "Counterfactual Data Balancing",
    "section": "",
    "text": "Counterfactual data is critical when direct access to complete prison population databases is unavailable. Since I don’t have access to a comprehensive dataset of all incarcerated individuals in Illinois and their exoneration statuses, I use counterfactuals to create a synthetic, balanced dataset.\nA counterfactual statement allows us to explore “what if” scenarios, comparing two outcomes that differ in one key aspect. For example, a counterfactual asks: What if this person had not been exonerated? Would their characteristics resemble those of non-exonerated individuals?\nAs explained in this primer on counterfactuals and their applications , “This kind of statement—an ‘if’ statement in which the ‘if’ portion is untrue or unrealized—is known as a counterfactual. The ‘if’ portion of a counterfactual is called the hypothetical condition, or more often, the antecedent.” Counterfactual data allows us to compare outcomes under identical conditions, differing only in the hypothetical condition—in this case, exoneration versus non-exoneration. This approach helps mitigate biases and ensures the models have balanced, reliable inputs for training.\n\n\nThe implementation of this counterfactual data balancing relied heavily on expert guidance and code contributions from Professor Jeff Jacobs. His guidance helped shape the methodology and implementation of the counterfactual sampling process."
  },
  {
    "objectID": "technical-details/data-balancing/main.html#scrapping-cleaning-rows-for-exoneree-data",
    "href": "technical-details/data-balancing/main.html#scrapping-cleaning-rows-for-exoneree-data",
    "title": "Counterfactual Data Balancing",
    "section": "Scrapping & Cleaning Rows for Exoneree Data",
    "text": "Scrapping & Cleaning Rows for Exoneree Data\nRaw HTML from Prison Policy Initiative\n\nil_df = pd.read_csv('../../data/processed-data/representation_by_county_raw.csv')\nil_df = county_df[county_df['state'] == \"Illinois\"].copy()\nil_df.head(3)\n\n\n\n\n\n\n\n\ncounty\nstate\ntotal_population\ntotal_white_population\ntotal_black_population\ntotal_latino_population\nincarcerated_population\nincarcerated_white_population\nincarcerated_black_population\nincarcerated_latino_population\nnon-incarcerated_population\nnon-incarcerated_white_population\nnon-incarcerated_black_population\nnon-incarcerated_latino_population\nratio_of_overrepresentation_of_whites_incarcerated_compared_to_whites_non-incarcerated\nratio_of_overrepresentation_of_blacks_incarcerated_compared_to_blacks_non-incarcerated\nratio_of_overrepresentation_of_latinos_incarcerated_compared_to_latinos_non-incarcerated\n\n\n\n\n595\nAdams County\nIllinois\n67103\n62414\n2331\n776\n110\n73\n36\n0\n66993\n62341\n2295\n776\n0.71\n9.54\n0.00\n\n\n596\nAlexander County\nIllinois\n8238\n4983\n2915\n155\n411\n89\n242\n79\n7827\n4894\n2673\n76\n0.35\n1.72\n19.82\n\n\n597\nBond County\nIllinois\n17768\n15797\n1080\n547\n1542\n500\n657\n304\n16226\n15297\n423\n243\n0.34\n16.32\n13.14"
  },
  {
    "objectID": "technical-details/data-balancing/main.html#narrowing-to-incarcerated-population",
    "href": "technical-details/data-balancing/main.html#narrowing-to-incarcerated-population",
    "title": "Counterfactual Data Balancing",
    "section": "Narrowing to Incarcerated Population",
    "text": "Narrowing to Incarcerated Population\nWe’re interested specifically in simulating “draws” from the incarcerated population of Illinois. So, we select the relevant subset of columns here (renaming them to be a bit shorter while we’re at it – two birds one stone)\n\nrename_map = {\n    'county': 'county',\n    'state': 'state',\n    'incarcerated_population': 'Total',\n    'incarcerated_white_population': 'White',\n    'incarcerated_black_population': 'Black',\n    'incarcerated_latino_population': 'Latino',\n}\n# Keep only the cols in the rename_map\ncols_to_keep = list(rename_map.keys())\nil_df = il_df[cols_to_keep].copy()\n# And do the renaming\nil_df.rename(columns=rename_map, inplace=True)\nil_df.head()\n\n\n\n\n\n\n\n\ncounty\nstate\nTotal\nWhite\nBlack\nLatino\n\n\n\n\n595\nAdams County\nIllinois\n110\n73\n36\n0\n\n\n596\nAlexander County\nIllinois\n411\n89\n242\n79\n\n\n597\nBond County\nIllinois\n1542\n500\n657\n304\n\n\n598\nBoone County\nIllinois\n71\n38\n12\n21\n\n\n599\nBrown County\nIllinois\n2059\n419\n1267\n367\n\n\n\n\n\n\n\n\n# Snce the Exoneree project uses just the county name (like \"Cook\"), we'll remove the trailing \" County\" (so, e.g., \"Cook County\" will turn into just \"Cook\"):\nil_df['County'] = il_df['County'].str.replace(\" County\",\"\")\n\n# And, we can compute a state_prop column representing the % of all Illinois inmates contained in each county:\nil_df['state_prop'] = il_df['Total'] / il_df['Total'].sum()\nil_df.sort_values(by='state_prop', ascending=False).head()\n\n\n\n\n\n\n\n\nCounty\nState\nTotal\nWhite\nBlack\nLatino\nstate_prop\n\n\n\n\n610\nCook\nIllinois\n11649\n1769\n8369\n1468\n0.164469\n\n\n693\nWill\nIllinois\n3902\n811\n2528\n538\n0.055091\n\n\n673\nRandolph\nIllinois\n3571\n934\n2250\n377\n0.050418\n\n\n648\nLogan\nIllinois\n3060\n963\n1705\n389\n0.043203\n\n\n647\nLivingston\nIllinois\n2798\n905\n1577\n294\n0.039504\n\n\n\n\n\n\n\nSo we see that our sample should be about 16.4% from Cook County, 5.5% from Will, 5% from Randolph, and so on.\nTo avoid confusing the state_prop value with the sampled proportion that we compute below, we can drop state_prop now:\n\n# Since they're only tracking three racial groups, the total of the three race counts should not equal the total incarcerated population. But let's check:\nil_df['three_cat_total'] = il_df['Black'] + il_df['White'] + il_df['Latino']\nil_df.head()\n\n\n\n\n\n\n\n\nCounty\nState\nTotal\nWhite\nBlack\nLatino\nstate_prop\nthree_cat_total\n\n\n\n\n595\nAdams\nIllinois\n110\n73\n36\n0\n0.001553\n109\n\n\n596\nAlexander\nIllinois\n411\n89\n242\n79\n0.005803\n410\n\n\n597\nBond\nIllinois\n1542\n500\n657\n304\n0.021771\n1461\n\n\n598\nBoone\nIllinois\n71\n38\n12\n21\n0.001002\n71\n\n\n599\nBrown\nIllinois\n2059\n419\n1267\n367\n0.029070\n2053\n\n\n\n\n\n\n\nSince we need our sample to be fully representative of the county-by-county distributions, we need to use the difference (between three_cat_total and total) to construct an other category:\n\nil_df['Other'] = il_df['Total'] - il_df['three_cat_total']\nil_df.head()\n\n\n\n\n\n\n\n\nCounty\nState\nTotal\nWhite\nBlack\nLatino\nstate_prop\nthree_cat_total\nOther\n\n\n\n\n595\nAdams\nIllinois\n110\n73\n36\n0\n0.001553\n109\n1\n\n\n596\nAlexander\nIllinois\n411\n89\n242\n79\n0.005803\n410\n1\n\n\n597\nBond\nIllinois\n1542\n500\n657\n304\n0.021771\n1461\n81\n\n\n598\nBoone\nIllinois\n71\n38\n12\n21\n0.001002\n71\n0\n\n\n599\nBrown\nIllinois\n2059\n419\n1267\n367\n0.029070\n2053\n6\n\n\n\n\n\n\n\nIt’s not all that well-documented in the source for this data, but I think there might be some counties that report people more than once if they put more than one race down? I say that because, some of the three_cat_total values are actually higher than the overall county totals:\n\nil_df[il_df['three_cat_total'] &gt; il_df['Total']]\n\n\n\n\n\n\n\n\nCounty\nState\nTotal\nWhite\nBlack\nLatino\nstate_prop\nthree_cat_total\nOther\n\n\n\n\n608\nClinton\nIllinois\n1599\n486\n917\n199\n0.022576\n1602\n-3\n\n\n611\nCrawford\nIllinois\n1230\n310\n782\n141\n0.017366\n1233\n-3\n\n\n620\nFayette\nIllinois\n1527\n467\n933\n129\n0.021559\n1529\n-2\n\n\n635\nJefferson\nIllinois\n1857\n827\n812\n224\n0.026218\n1863\n-6\n\n\n645\nLawrence\nIllinois\n2358\n486\n1490\n393\n0.033292\n2369\n-11\n\n\n654\nMadison\nIllinois\n14\n0\n11\n14\n0.000198\n25\n-11\n\n\n655\nMarion\nIllinois\n114\n69\n37\n10\n0.001610\n116\n-2\n\n\n686\nVermilion\nIllinois\n2084\n536\n1236\n319\n0.029423\n2091\n-7\n\n\n690\nWayne\nIllinois\n2\n0\n2\n2\n0.000028\n4\n-2\n\n\n691\nWhite\nIllinois\n72\n35\n19\n36\n0.001017\n90\n-18\n\n\n\n\n\n\n\nHowever, since most of these are low numbers (Madison County and White County are obviously exceptions, sketchy af but… not much we could do besides contacting the correctional facilities in those counties 😵), we will set the other value to 0 in these cases:\n\nil_df['Other'] = il_df['Other'].apply(lambda x: 0 if x &lt; 0 else x)\n# And now we can drop three_cat_total, since we only needed that in order to form the other count:\nil_df.drop(columns=['three_cat_total'], inplace=True, errors='ignore')\n\n#  since we've now arrived at consistent names for the four race categories used by PPI, we store these names in a list for future use (to ensure consistency in naming throughout):\nrace_category_names = ['White', 'Black', 'Latino', 'Other']\nil_df.head()\n\n\n\n\n\n\n\n\nCounty\nState\nTotal\nWhite\nBlack\nLatino\nstate_prop\nOther\n\n\n\n\n595\nAdams\nIllinois\n110\n73\n36\n0\n0.001553\n1\n\n\n596\nAlexander\nIllinois\n411\n89\n242\n79\n0.005803\n1\n\n\n597\nBond\nIllinois\n1542\n500\n657\n304\n0.021771\n81\n\n\n598\nBoone\nIllinois\n71\n38\n12\n21\n0.001002\n0\n\n\n599\nBrown\nIllinois\n2059\n419\n1267\n367\n0.029070\n6"
  },
  {
    "objectID": "technical-details/data-balancing/main.html#illinois-exoneree-countsdemographics",
    "href": "technical-details/data-balancing/main.html#illinois-exoneree-countsdemographics",
    "title": "Counterfactual Data Balancing",
    "section": "Illinois Exoneree Counts/Demographics",
    "text": "Illinois Exoneree Counts/Demographics\n\nexon_il_df = pd.read_csv('../../data/processed-data/illinois_exoneration_data.csv')\nexon_il_df.head(3)\n\n\n\n\n\n\n\n\nlast_name\nfirst_name\nage\nrace\nsex\nstate\ncounty\nlatitude\nlongitude\nworst_crime_display\n...\nchild_welfare_worker_misconduct\nwithheld_exculpatory_evidence\nmisconduct_that_is_not_withholding_evidence\nknowingly_permitting_perjury\nwitness_tampering_or_misconduct_interrogating_co_defendant\nmisconduct_in_interrogation_of_exoneree\nperjury_by_official\nprosecutor_lied_in_court\ntag_sum\ngeocode_address\n\n\n\n\n0\nAbbott\nCinque\n19.0\nBlack\nmale\nIllinois\nCook\n41.819738\n-87.756525\nDrug Possession or Sale\n...\n0\n1\n1\n0\n0\n0\n0\n0\n7\nCook County, Illinois, United States\n\n\n1\nAbernathy\nChristopher\n17.0\nWhite\nmale\nIllinois\nCook\n41.819738\n-87.756525\nMurder\n...\n0\n1\n1\n0\n0\n1\n0\n0\n10\nCook County, Illinois, United States\n\n\n2\nAbrego\nEruby\n20.0\nHispanic\nmale\nIllinois\nCook\n41.819738\n-87.756525\nMurder\n...\n0\n1\n1\n0\n1\n1\n1\n0\n9\nCook County, Illinois, United States\n\n\n\n\n3 rows × 48 columns\n\n\n\n\nnum_il = len(exon_il_df)\nnum_il\n\n548\n\n\n\nexon_il_df['race'].value_counts(normalize=True)\n\nrace\nBlack              0.762774\nHispanic           0.147810\nWhite              0.085766\nAsian              0.001825\nNative American    0.001825\nName: proportion, dtype: float64\n\n\nSince the PPI data only has black, white, latino, and other, we need to rename Hispanic and then combine Asian and Native American into “other” (since we’re going to combine these at the end, based on the race_categories list we created above). We still keep the original Race variable, just in a new column named Race_orig:\n\nrecode_map = {\n    'Black': 'Black',\n    'Hispanic': 'Latino',\n    'White': 'White',\n    'Asian': 'Other',\n    'Native American': 'Other',\n}\nexon_il_df['Race_orig'] = exon_il_df['race']\nexon_il_df['race'] = exon_il_df['race'].apply(lambda x: recode_map[x])\nexon_il_df['race'].value_counts(normalize=True)\n\nrace\nBlack     0.762774\nLatino    0.147810\nWhite     0.085766\nOther     0.003650\nName: proportion, dtype: float64"
  },
  {
    "objectID": "technical-details/data-balancing/main.html#samplng-from-incarcerated-population",
    "href": "technical-details/data-balancing/main.html#samplng-from-incarcerated-population",
    "title": "Counterfactual Data Balancing",
    "section": "Samplng from Incarcerated Population",
    "text": "Samplng from Incarcerated Population\nNow we can conduct the simulated “sample”. The first step is to:\nSample 548 “people” from among the prison population in Illinois, by first taking a weighted replacement sample of size 548 from il_df, which will give us a valid population-weighted sample of 548 “people” where all we know about these people is their county. Here we also make sure to set the seed for Pandas’ random number generator, so that our sample is replicable:\n\nil_sample_df = il_df.sample(\n    num_il,\n    replace = True,\n    weights = il_df['Total'],\n    random_state = 5000,\n).copy()\nil_sample_df.head()\n\n\n\n\n\n\n\n\nCounty\nState\nTotal\nWhite\nBlack\nLatino\nstate_prop\nOther\n\n\n\n\n610\nCook\nIllinois\n11649\n1769\n8369\n1468\n0.164469\n43\n\n\n631\nHenry\nIllinois\n301\n172\n108\n21\n0.004250\n0\n\n\n667\nPerry\nIllinois\n2323\n561\n1398\n352\n0.032798\n12\n\n\n610\nCook\nIllinois\n11649\n1769\n8369\n1468\n0.164469\n43\n\n\n648\nLogan\nIllinois\n3060\n963\n1705\n389\n0.043203\n3\n\n\n\n\n\n\n\n\nil_sample_df['County'].value_counts(normalize=True).head()\n\nCounty\nCook        0.142336\nWill        0.060219\nRandolph    0.056569\nPerry       0.040146\nLogan       0.040146\nName: proportion, dtype: float64\n\n\nNow, all that’s left is the second step:\nFor each row in il_sample_df, use the counts for each race to form a distribution, then draw from it to replicate that county’s racial distribution of inmates in our sample. This time, since we’re working row-by-row, we use NumPy, still making sure to seed the RNG so that our results are replicable:\n\nrng = np.random.default_rng(seed = 5000)\ndef draw_race_sample(row):\n  race_counts = [row[cur_val] for cur_val in race_category_names]\n  total_count = sum(race_counts)\n  race_probs = [cur_count / total_count for cur_count in race_counts]\n  # And now we have a probability distribution! We can use rng.choice() to\n  # sample from it\n  sampled_vals = rng.choice(race_category_names, size=1, p=race_probs)\n  # We only sampled 1 value here, so we use [0] to extract it\n  sampled_val = list(sampled_vals)[0]\n  return sampled_val\n\nBefore using it to sample, it’s helpful to check that our function works by using it to draw a bunch of samples for a specific county (Cook, in this case). First, let’s compute what we would expect in terms of proportions if we sampled N inmates from Cook:\n\ncook_row = il_df[il_df['County'] == \"Cook\"].iloc[0]\nfor cname in race_category_names:\n  cook_row[f'{cname}_prop'] = cook_row[cname] / cook_row['Total']\ncook_row\n\nCounty             Cook\nState          Illinois\nTotal             11649\nWhite              1769\nBlack              8369\nLatino             1468\nstate_prop     0.164469\nOther                43\nWhite_prop     0.151859\nBlack_prop     0.718431\nLatino_prop    0.126019\nOther_prop     0.003691\nName: 610, dtype: object\n\n\nThis means that, if our draw_race_sample() function is working correctly, we’d expect it to generate “white” 15.2% of the time, “black” 71.8% of the time, and so on. So, let’s check that by using it to generate an N=5000 sample from Cook:\n\nN = 5000\ncook_samples = [draw_race_sample(cook_row) for _ in range(N)]\ncook_sample_df = pd.DataFrame(cook_samples, columns = ['Race'])\ncook_sample_df['Race'].value_counts(normalize=True)\n\nRace\nBlack     0.7186\nWhite     0.1518\nLatino    0.1260\nOther     0.0036\nName: proportion, dtype: float64\n\n\nLooks good, very close to the expected proportions! So, now that we trust our draw_race_sample() function a bit more, we can use it to sample a race value for each row in il_sample_df. This also gives me a chance to show how the tqdm library works, which is helpful when doing things like this to check the progress (and thus to make sure that your simulation code isn’t taking too long per row):\n\nil_sample_df['Race'] = il_sample_df.progress_apply(draw_race_sample, axis=1)\n\n100%|██████████| 548/548 [00:00&lt;00:00, 22719.42it/s]\n\n\n\nsample_cols_to_keep = [\n    'County',\n    'State',\n    'Race'\n]\nil_sample_df = il_sample_df[sample_cols_to_keep].copy()\nil_sample_df\n\n\n\n\n\n\n\n\nCounty\nState\nRace\n\n\n\n\n610\nCook\nIllinois\nBlack\n\n\n631\nHenry\nIllinois\nWhite\n\n\n667\nPerry\nIllinois\nBlack\n\n\n610\nCook\nIllinois\nBlack\n\n\n648\nLogan\nIllinois\nBlack\n\n\n...\n...\n...\n...\n\n\n646\nLee\nIllinois\nWhite\n\n\n605\nChristian\nIllinois\nBlack\n\n\n620\nFayette\nIllinois\nBlack\n\n\n639\nKane\nIllinois\nWhite\n\n\n647\nLivingston\nIllinois\nBlack\n\n\n\n\n548 rows × 3 columns\n\n\n\nLet’s see what the racial distribution of the Cook County subset of our sample ended up looking like:\n\ncook_sample_df = il_sample_df[il_sample_df['County'] == \"Cook\"].copy()\ncook_sample_df['Race'].value_counts(normalize=True)\n\nRace\nBlack     0.743590\nLatino    0.166667\nWhite     0.089744\nName: proportion, dtype: float64\n\n\nSo, we ended up with a slight oversample of Latinos relative to the population expectation, and an undersample of Whites, but that’s exactly (as weird as it might feel) a “feature” of this mode of sampling: it’s what we want since we’re trying to simulate our simplified model of the Exoneration Project: that their sample of exonerees represents some subset of 548 inmates from Cook, so we want to compare them with another size-548 subset of those still incarcerated in Cook.\nSo, with all that completed, we can combine the 548 rows in il_sample_df with exon_il_df to create a balanced DataFrame with (548 * 2) = 1096 rows, half of which are exonerated inmates from Illinois and half of which are non-exonerated inmates from Illinois (where the non-exonerated group is statistically representative of the incarcerated population of Illinois as a whole):\n\n# Construct our new label: exonerated vs. non-exonerated\nexon_il_df['Label'] = \"Exonerated\"\nil_sample_df['Label'] = \"Non-Exonerated\"\n# And combine!\nbalanced_df = pd.concat([exon_il_df, il_sample_df], axis=0)\n# Define the mapping for 'race'\nrace_mapping = {\n    'Asian': 'Other',\n    'Native American': 'Other',\n    'Black': 'Black',\n    'White': 'White',\n    'Hispanic': 'Hispanic'\n}\n\n# Map the 'race' column\nbalanced_df['race'] = balanced_df['race'].map(race_mapping)\n\n# Combine 'race' and 'Race' columns, prioritizing non-NaN values\nbalanced_df['Race'] = balanced_df['race'].combine_first(balanced_df['Race'])\n\n# Drop the old 'race' column\nbalanced_df.drop(columns=['race'], inplace=True)\n\n# Combine 'county' and 'County' columns, prioritizing non-NaN values\nbalanced_df['County'] = balanced_df['county'].combine_first(balanced_df['County'])\n\n# Drop the old 'county' column\nbalanced_df.drop(columns=['county'], inplace=True)\n\n# Verify the final Race column\nprint(balanced_df['Race'].value_counts())\nprint(balanced_df['County'].value_counts())\nbalanced_df.head()\n\nRace\nBlack     709\nWhite     207\nLatino     92\nOther       5\nName: count, dtype: int64\nCounty\nCook           552\nWill            37\nRandolph        31\nJefferson       23\nPerry           22\nLivingston      22\nLogan           22\nJohnson         21\nFulton          21\nTazewell        19\nLawrence        18\nMontgomery      17\nBond            17\nVermilion       16\nWinnebago       15\nLa Salle        14\nClinton         14\nSt. Clair       14\nLake            14\nFayette         13\nLee             13\nKane            12\nBrown           12\nKnox            11\nDupage          10\nMorgan          10\nRock Island     10\nPeoria          10\nCrawford         9\nMacon            9\nDuPage           6\nWilliamson       6\nChristian        6\nChampaign        5\nSangamon         4\nMclean           4\nMcHenry          4\nMchenry          3\nHenry            3\nKankakee         3\nIroquois         2\nStephenson       2\nEffingham        2\nEdgar            2\nWoodford         2\nAdams            2\nMenard           1\nBoone            1\nPope             1\nJackson          1\nCumberland       1\nMadison          1\nWashington       1\nDekalb           1\nMoultrie         1\nLasalle          1\nRichland         1\nDe Witt          1\nName: count, dtype: int64\n\n\n\n\n\n\n\n\n\nlast_name\nfirst_name\nage\nsex\nstate\nlatitude\nlongitude\nworst_crime_display\nsentence\nsentence_in_years\n...\nmisconduct_in_interrogation_of_exoneree\nperjury_by_official\nprosecutor_lied_in_court\ntag_sum\ngeocode_address\nRace_orig\nLabel\nCounty\nState\nRace\n\n\n\n\n0\nAbbott\nCinque\n19.0\nmale\nIllinois\n41.819738\n-87.756525\nDrug Possession or Sale\nProbation\n0.0\n...\n0.0\n0.0\n0.0\n7.0\nCook County, Illinois, United States\nBlack\nExonerated\nCook\nNaN\nBlack\n\n\n1\nAbernathy\nChristopher\n17.0\nmale\nIllinois\n41.819738\n-87.756525\nMurder\nLife without parole\n100.0\n...\n1.0\n0.0\n0.0\n10.0\nCook County, Illinois, United States\nWhite\nExonerated\nCook\nNaN\nWhite\n\n\n2\nAbrego\nEruby\n20.0\nmale\nIllinois\n41.819738\n-87.756525\nMurder\n90 years\n90.0\n...\n1.0\n1.0\n0.0\n9.0\nCook County, Illinois, United States\nHispanic\nExonerated\nCook\nNaN\nNaN\n\n\n3\nAdams\nDemetris\n22.0\nmale\nIllinois\n41.819738\n-87.756525\nDrug Possession or Sale\n1 year\n1.0\n...\n0.0\n0.0\n0.0\n7.0\nCook County, Illinois, United States\nBlack\nExonerated\nCook\nNaN\nBlack\n\n\n4\nAdams\nKenneth\n22.0\nmale\nIllinois\n41.819738\n-87.756525\nMurder\n75 years\n75.0\n...\n0.0\n0.0\n0.0\n11.0\nCook County, Illinois, United States\nBlack\nExonerated\nCook\nNaN\nBlack\n\n\n\n\n5 rows × 51 columns\n\n\n\n\nbalanced_df.to_csv(\"../../data/processed-data/exonerees_balanced.csv\", index=False)"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#preprocessing",
    "href": "technical-details/supervised-learning/main.html#preprocessing",
    "title": "Supervised Learning",
    "section": "Preprocessing",
    "text": "Preprocessing\n\n# Map 'Exonerated' to 1 and 'Non-Exonerated' to 0\nexonerees_balanced['Label'] = exonerees_balanced['Label'].map({'Exonerated': 1, 'Non-Exonerated': 0})\n\n# Rename the column to 'exonerated'\nexonerees_balanced.rename(columns={'Label': 'Exonerated'}, inplace=True)\n# Keep relevant features\ndata = exonerees_balanced[['Race', 'County', 'Exonerated']].copy()\n\n# Preview the transformed data\nprint(data.head())\n\n\n    Race County  Exonerated\n0  Black   Cook           1\n1  White   Cook           1\n2    NaN   Cook           1\n3  Black   Cook           1\n4  Black   Cook           1"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#one-hot-encoding",
    "href": "technical-details/supervised-learning/main.html#one-hot-encoding",
    "title": "Supervised Learning",
    "section": "One Hot encoding",
    "text": "One Hot encoding\n\n# Perform one-hot encoding without dropping any category\nencoder = OneHotEncoder(sparse_output=False)  # Use sparse_output instead of sparse\nencoded_features = encoder.fit_transform(data[['Race', 'County']])\n\n# Create a DataFrame for the encoded features\nencoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(['Race', 'County']))\n\n# Concatenate encoded features with the original dataset\ndata = pd.concat([data.drop(columns=['Race', 'County']), encoded_df], axis=1)\n\n# Preview the data\ndata.head()\n\n\n\n\n\n\n\n\n\nExonerated\nRace_Black\nRace_Latino\nRace_Other\nRace_White\nRace_nan\nCounty_Adams\nCounty_Bond\nCounty_Boone\nCounty_Brown\n...\nCounty_Sangamon\nCounty_St. Clair\nCounty_Stephenson\nCounty_Tazewell\nCounty_Vermilion\nCounty_Washington\nCounty_Will\nCounty_Williamson\nCounty_Winnebago\nCounty_Woodford\n\n\n\n\n0\n1\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n1\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n1\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n1\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 64 columns"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#splitting-data",
    "href": "technical-details/supervised-learning/main.html#splitting-data",
    "title": "Supervised Learning",
    "section": "splitting data",
    "text": "splitting data\n\nfrom sklearn.model_selection import train_test_split\n\n# Define features (X) and target (y)\nX = data.drop(columns=['Exonerated'])  # Drop the target column\ny = data['Exonerated']  # Target column\n\n# Split into train and test sets (80-20 split)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Check the shapes of the splits\nprint(\"Training set shape:\", X_train.shape)\nprint(\"Test set shape:\", X_test.shape)\n\nTraining set shape: (876, 63)\nTest set shape: (220, 63)"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-logisitic-regression",
    "href": "technical-details/supervised-learning/main.html#model-logisitic-regression",
    "title": "Supervised Learning",
    "section": "model: logisitic regression",
    "text": "model: logisitic regression\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\nimport matplotlib.pyplot as plt\n\n# Logistic Regression Model\nlog_model = LogisticRegression(random_state=42)\nlog_model.fit(X_train, y_train)\n\n# Predictions and Probabilities\nlog_y_pred = log_model.predict(X_test)\nlog_y_pred_proba = log_model.predict_proba(X_test)[:, 1]\n\n# Evaluation Metrics\nlog_accuracy = accuracy_score(y_test, log_y_pred)\nlog_precision = precision_score(y_test, log_y_pred)\nlog_recall = recall_score(y_test, log_y_pred)\nlog_f1 = f1_score(y_test, log_y_pred)\nlog_roc_auc = roc_auc_score(y_test, log_y_pred_proba)\n\nprint(f\"Logistic Regression - Accuracy: {log_accuracy:.2f}, Precision: {log_precision:.2f}, Recall: {log_recall:.2f}, F1 Score: {log_f1:.2f}, ROC-AUC: {log_roc_auc:.2f}\")\n\n# ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, log_y_pred_proba)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f\"Logistic Regression ROC Curve (AUC = {log_roc_auc:.2f})\")\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Logistic Regression ROC Curve\")\nplt.legend(loc=\"lower right\")\nplt.show()\n\n## Confusion Matrix\nConfusionMatrixDisplay.from_estimator(log_model, X_test, y_test)\nplt.title(\"Confusion Matrix - Logistic Regression\")\nplt.show()\n\n## Calibration Curve\nprob_true, prob_pred = calibration_curve(y_test, log_y_pred_proba, n_bins=10)\nplt.plot(prob_pred, prob_true, marker='o', label=\"Logistic Regression\")\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray', label=\"Perfect Calibration\")\nplt.xlabel(\"Mean Predicted Probability\")\nplt.ylabel(\"Fraction of Positives\")\nplt.title(\"Calibration Curve - Logistic Regression\")\nplt.legend()\nplt.show()\n\nLogistic Regression - Accuracy: 0.90, Precision: 0.90, Recall: 0.89, F1 Score: 0.90, ROC-AUC: 0.95"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#naive-bayes",
    "href": "technical-details/supervised-learning/main.html#naive-bayes",
    "title": "Supervised Learning",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nfrom sklearn.naive_bayes import GaussianNB\n\n# Naive Bayes Model\nnb_model = GaussianNB()\nnb_model.fit(X_train, y_train)\n\n# Predictions and Probabilities\nnb_y_pred = nb_model.predict(X_test)\nnb_y_pred_proba = nb_model.predict_proba(X_test)[:, 1]\n\n# Evaluation Metrics\nnb_accuracy = accuracy_score(y_test, nb_y_pred)\nnb_precision = precision_score(y_test, nb_y_pred)\nnb_recall = recall_score(y_test, nb_y_pred)\nnb_f1 = f1_score(y_test, nb_y_pred)\nnb_roc_auc = roc_auc_score(y_test, nb_y_pred_proba)\n\nprint(f\"Naive Bayes - Accuracy: {nb_accuracy:.2f}, Precision: {nb_precision:.2f}, Recall: {nb_recall:.2f}, F1 Score: {nb_f1:.2f}, ROC-AUC: {nb_roc_auc:.2f}\")\n\n# ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, nb_y_pred_proba)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f\"Naive Bayes ROC Curve (AUC = {nb_roc_auc:.2f})\")\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Naive Bayes ROC Curve\")\nplt.legend(loc=\"lower right\")\nplt.show()\n\n## Confusion Matrix\nConfusionMatrixDisplay.from_estimator(nb_model, X_test, y_test)\nplt.title(\"Confusion Matrix - Logistic Regression\")\nplt.show()\n\n## Calibration Curve\nprob_true, prob_pred = calibration_curve(y_test, nb_y_pred_proba, n_bins=10)\nplt.plot(prob_pred, prob_true, marker='o', label=\"Logistic Regression\")\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray', label=\"Perfect Calibration\")\nplt.xlabel(\"Mean Predicted Probability\")\nplt.ylabel(\"Fraction of Positives\")\nplt.title(\"Calibration Curve - Naive Bayes\")\nplt.legend()\nplt.show()\n\nNaive Bayes - Accuracy: 0.83, Precision: 0.76, Recall: 0.99, F1 Score: 0.86, ROC-AUC: 0.86"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#random-forest",
    "href": "technical-details/supervised-learning/main.html#random-forest",
    "title": "Supervised Learning",
    "section": "Random Forest",
    "text": "Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Random Forest Model\nrf_model = RandomForestClassifier(random_state=42)\nrf_model.fit(X_train, y_train)\n\n# Predictions and Probabilities\nrf_y_pred = rf_model.predict(X_test)\nrf_y_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n\n# Evaluation Metrics\nrf_accuracy = accuracy_score(y_test, rf_y_pred)\nrf_precision = precision_score(y_test, rf_y_pred)\nrf_recall = recall_score(y_test, rf_y_pred)\nrf_f1 = f1_score(y_test, rf_y_pred)\nrf_roc_auc = roc_auc_score(y_test, rf_y_pred_proba)\n\nprint(f\"Random Forest - Accuracy: {rf_accuracy:.2f}, Precision: {rf_precision:.2f}, Recall: {rf_recall:.2f}, F1 Score: {rf_f1:.2f}, ROC-AUC: {rf_roc_auc:.2f}\")\n\n# ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, rf_y_pred_proba)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f\"Random Forest ROC Curve (AUC = {rf_roc_auc:.2f})\")\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Random Forest ROC Curve\")\nplt.legend(loc=\"lower right\")\nplt.show()\n\n# Feature Importance\nfeature_importances = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False)\nfeature_importances.plot(kind='bar', figsize=(10, 6), title=\"Feature Importance\")\nplt.show()\n\n## Confusion Matrix\nConfusionMatrixDisplay.from_estimator(rf_model, X_test, y_test)\nplt.title(\"Confusion Matrix - Logistic Regression\")\nplt.show()\n\n## Calibration Curve\nprob_true, prob_pred = calibration_curve(y_test, rf_y_pred_proba, n_bins=10)\nplt.plot(prob_pred, prob_true, marker='o', label=\"Logistic Regression\")\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray', label=\"Perfect Calibration\")\nplt.xlabel(\"Mean Predicted Probability\")\nplt.ylabel(\"Fraction of Positives\")\nplt.title(\"Calibration Curve - Logistic Regression\")\nplt.legend()\nplt.show()\n\n\nRandom Forest - Accuracy: 0.91, Precision: 0.90, Recall: 0.92, F1 Score: 0.91, ROC-AUC: 0.95"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-comparisson",
    "href": "technical-details/supervised-learning/main.html#model-comparisson",
    "title": "Supervised Learning",
    "section": "Model Comparisson",
    "text": "Model Comparisson\n\n# Define metrics for each model\ncomparison_data = {\n    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC-AUC\"],\n    \"Logistic Regression\": [log_accuracy, log_precision, log_recall, log_f1, log_roc_auc],\n    \"Naive Bayes\": [nb_accuracy, nb_precision, nb_recall, nb_f1, nb_roc_auc ],\n    \"Random Forest\": [rf_accuracy, rf_precision, rf_recall, rf_f1, rf_roc_auc],\n    \"K-Nearest Neighbors\": [knn_accuracy, knn_precision, knn_recall, knn_f1, knn_roc_auc]\n}\n\n# Create the DataFrame\ncomparison_df = pd.DataFrame(comparison_data)\n# Style the DataFrame for better readability\ncomparison_df.style.set_caption(\"Model Comparison\").highlight_max(axis=1, color=\"lightgreen\")\n\n# Display the table\ncomparison_df.head()\n\n\n\n\n\n\n\n\nMetric\nLogistic Regression\nNaive Bayes\nRandom Forest\nK-Nearest Neighbors\n\n\n\n\n0\nAccuracy\n0.895455\n0.831818\n0.909091\n0.909091\n\n\n1\nPrecision\n0.901786\n0.756757\n0.904348\n0.872000\n\n\n2\nRecall\n0.893805\n0.991150\n0.920354\n0.964602\n\n\n3\nF1 Score\n0.897778\n0.858238\n0.912281\n0.915966\n\n\n4\nROC-AUC\n0.952651\n0.864403\n0.954553\n0.907535"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#visualizing-predicted-probability-with-most-accurate-model",
    "href": "technical-details/supervised-learning/main.html#visualizing-predicted-probability-with-most-accurate-model",
    "title": "Supervised Learning",
    "section": "Visualizing Predicted Probability with Most Accurate Model",
    "text": "Visualizing Predicted Probability with Most Accurate Model\n\n# Predict probabilities for the test set\nprobabilities = rf_model.predict_proba(X_test)\n# The second column contains the probabilities for the positive class (Exonerated)\nexoneration_probabilities = probabilities[:, 1]\n\n# Preview the first few probabilities\nprint(exoneration_probabilities[:10])\n\n[1.         0.         0.86253375 0.         1.         0.86253375\n 0.         0.7507617  0.86253375 0.        ]\n\n\n\n# Ensure X_test retains the one-hot encoded race columns\nX_test_with_probabilities = X_test.copy()  # Create a copy of X_test\nX_test_with_probabilities['exoneration_probability'] = exoneration_probabilities  # Add predicted probabilities\n\n# Decode one-hot encoded race into a single column\nrace_columns = ['Race_Black', 'Race_Latino', 'Race_Other', 'Race_White']\nX_test_with_probabilities['Race'] = X_test_with_probabilities[race_columns].idxmax(axis=1).str.replace('Race_', '')\n\n# Group by race and calculate average probabilities\nrace_probabilities = X_test_with_probabilities.groupby('Race')['exoneration_probability'].mean()\n\n# Plot the average probabilities for each race\nrace_probabilities.sort_values().plot(kind='bar', color='skyblue', figsize=(10, 6))\nplt.title('Average Predicted Probability of Illinois Exoneration by Race')\nplt.xlabel('Race')\nplt.ylabel('Average Predicted Probability')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n# Decode one-hot encoded county into a single column\ncounty_columns = [col for col in X_test.columns if col.startswith('County_')]\nfor col in county_columns:\n    county_name = col.replace('County_', '')\n    X_test_with_probabilities.loc[X_test_with_probabilities[col] == 1, 'County'] = county_name\n\n# Group by county and calculate average probabilities\ncounty_probabilities = X_test_with_probabilities.groupby('County')['exoneration_probability'].mean()\n\n# Plot the average probabilities for each county\ncounty_probabilities.sort_values().plot(kind='bar', color='skyblue', figsize=(15, 8))\nplt.title('Average Predicted Probability of Illinois Exoneration by County')\nplt.xlabel('County')\nplt.ylabel('Average Predicted Probability')\nplt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Group by race and county, then calculate average probabilities\nrace_county_probabilities = X_test_with_probabilities.groupby(['Race', 'County'])['exoneration_probability'].mean().unstack()\n\n# Plot a heatmap for race vs. county\nimport seaborn as sns\nplt.figure(figsize=(18, 8))  # Increase width for better visibility\n\n# Create the heatmap\nsns.heatmap(\n    race_county_probabilities,  # Assuming this is your DataFrame\n    annot=True,  # Display numerical values\n    fmt=\".2f\",  # Format the numbers to 2 decimal places\n    cmap=\"coolwarm\",  # Use a diverging colormap\n    cbar=True,  # Display color bar\n    linewidths=0.5  # Add spacing between cells\n)\n\n# Customize titles and labels\nplt.title(\"Average Predicted Probability of Exoneration by Race and County\", fontsize=16)\nplt.xlabel(\"County\", fontsize=14)\nplt.ylabel(\"Race\", fontsize=14)\n\n# Rotate x-axis ticks for better readability\nplt.xticks(rotation=30, ha=\"right\", fontsize=10)\nplt.yticks(fontsize=12)  # Increase font size for y-axis labels\n\n# Display the heatmap\nplt.tight_layout()  # Ensure everything fits within the figure\nplt.show()"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#merge-with-geocoded-counties",
    "href": "technical-details/data-cleaning/main.html#merge-with-geocoded-counties",
    "title": "Data Cleaning",
    "section": "",
    "text": "The geocoded Illinois counties from Data Collection were merged into the main dataset:\n\nLoad and Standardize Data: Geocoded data was loaded, and column names were standardized to lowercase for consistency.\nFilter Relevant Counties: The geocoded data was filtered to include only counties present in the main dataset.\nMerge Data: Using a left join on county and state, geographic details (geocode_address, latitude, and longitude) were added to the dataset.\n\n\n# Read the geocoded population data from Data Collection\ngeocode_unique = pd.read_csv(\"../../data/raw-data/geocoded_population_counties.csv\")\n\n# Rename columns to lowercase for consistency\ngeocode_unique.rename(columns={\"County\": \"county\", \"State\": \"state\"}, inplace=True)\n\n# Filter geocode_unique to only include counties present in df\ngeocode_unique_filtered = geocode_unique[\n    geocode_unique[['county', 'state']].apply(tuple, axis=1).isin(df[['county', 'state']].apply(tuple, axis=1))\n]\n\n# Merge the filtered geocoding data into df\ndf = df.merge(geocode_unique_filtered, on=['county', 'state'], how='left')\n\n# Display the relevant columns to verify the merge\nprint(df[['state', 'county', 'geocode_address', 'latitude', 'longitude']].head())\n\n      state county                       geocode_address   latitude  longitude\n0  Illinois   Cook  Cook County, Illinois, United States  41.819738 -87.756525\n1  Illinois   Cook  Cook County, Illinois, United States  41.819738 -87.756525\n2  Illinois   Cook  Cook County, Illinois, United States  41.819738 -87.756525\n3  Illinois   Cook  Cook County, Illinois, United States  41.819738 -87.756525\n4  Illinois   Cook  Cook County, Illinois, United States  41.819738 -87.756525"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#introduction-and-motivation-2",
    "href": "technical-details/data-cleaning/main.html#introduction-and-motivation-2",
    "title": "Data Cleaning",
    "section": "Introduction and Motivation",
    "text": "Introduction and Motivation\nThis section documents the minimal cleaning performed on the racial geography dataset which provides population and incarceration statistics by race, making it an essential input for balancing counterfactual data for supervised learning.\nUnlike other datasets, this required minimal preprocessing due to its clean structure and consistent formatting. The primary cleaning steps involved standardizing column names for uniformity.\nBy the end of this step, the dataset is ready for use in data balancing and then machine learning."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#initial-overview-of-the-dataset-2",
    "href": "technical-details/data-cleaning/main.html#initial-overview-of-the-dataset-2",
    "title": "Data Cleaning",
    "section": "Initial Overview of the Dataset",
    "text": "Initial Overview of the Dataset\nThe dataset is first loaded to inspect its structure and contents.\n\nrepresentation_df = pd.read_csv('../../data/raw-data/representation_by_county_raw.csv')\nprint(\"Initial Dataset: \")\npd.set_option('display.max_columns', None) \nrepresentation_df.head()\n\nInitial Dataset: \n\n\n\n\n\n\n\n\n\nCounty\nState\nTotal Population\nTotal White Population\nTotal Black Population\nTotal Latino Population\nIncarcerated Population\nIncarcerated White Population\nIncarcerated Black Population\nIncarcerated Latino Population\nNon-incarcerated Population\nNon-incarcerated White Population\nNon-Incarcerated Black Population\nNon-Incarcerated Latino Population\nRatio of Overrepresentation of Whites Incarcerated Compared to Whites Non-Incarcerated\nRatio of Overrepresentation of Blacks Incarcerated Compared to Blacks Non-Incarcerated\nRatio of Overrepresentation of Latinos Incarcerated Compared to Latinos Non-Incarcerated\n\n\n\n\n0\nAdams County\nIllinois\n67103\n62414\n2331\n776\n110\n73\n36\n0\n66993\n62341\n2295\n776\n0.71\n9.54\n0.00\n\n\n1\nAlexander County\nIllinois\n8238\n4983\n2915\n155\n411\n89\n242\n79\n7827\n4894\n2673\n76\n0.35\n1.72\n19.82\n\n\n2\nBond County\nIllinois\n17768\n15797\n1080\n547\n1542\n500\n657\n304\n16226\n15297\n423\n243\n0.34\n16.32\n13.14\n\n\n3\nBoone County\nIllinois\n54165\n40757\n1064\n10967\n71\n38\n12\n21\n54094\n40719\n1052\n10946\n0.71\n8.71\n1.46\n\n\n4\nBrown County\nIllinois\n6937\n5191\n1280\n402\n2059\n419\n1267\n367\n4878\n4772\n13\n35\n0.21\n227.91\n24.76"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#standardizing-column-names",
    "href": "technical-details/data-cleaning/main.html#standardizing-column-names",
    "title": "Data Cleaning",
    "section": "Standardizing Column Names",
    "text": "Standardizing Column Names\nTo improve readability and simplifyfurther processing, the column names in the dataset were converted to lowercase and spaces were replaced with underscores.\n\nrepresentation_df.columns = representation_df.columns.str.lower().str.replace(' ', '_')\nrepresentation_df.head()\n\n# Remove county from county name \nrepresentation_df['county'] = representation_df['county'].str.replace(' County', '', regex=False)\nrepresentation_df.head()\n\n\n\n\n\n\n\n\ncounty\nstate\ntotal_population\ntotal_white_population\ntotal_black_population\ntotal_latino_population\nincarcerated_population\nincarcerated_white_population\nincarcerated_black_population\nincarcerated_latino_population\nnon-incarcerated_population\nnon-incarcerated_white_population\nnon-incarcerated_black_population\nnon-incarcerated_latino_population\nratio_of_overrepresentation_of_whites_incarcerated_compared_to_whites_non-incarcerated\nratio_of_overrepresentation_of_blacks_incarcerated_compared_to_blacks_non-incarcerated\nratio_of_overrepresentation_of_latinos_incarcerated_compared_to_latinos_non-incarcerated\n\n\n\n\n0\nAdams\nIllinois\n67103\n62414\n2331\n776\n110\n73\n36\n0\n66993\n62341\n2295\n776\n0.71\n9.54\n0.00\n\n\n1\nAlexander\nIllinois\n8238\n4983\n2915\n155\n411\n89\n242\n79\n7827\n4894\n2673\n76\n0.35\n1.72\n19.82\n\n\n2\nBond\nIllinois\n17768\n15797\n1080\n547\n1542\n500\n657\n304\n16226\n15297\n423\n243\n0.34\n16.32\n13.14\n\n\n3\nBoone\nIllinois\n54165\n40757\n1064\n10967\n71\n38\n12\n21\n54094\n40719\n1052\n10946\n0.71\n8.71\n1.46\n\n\n4\nBrown\nIllinois\n6937\n5191\n1280\n402\n2059\n419\n1267\n367\n4878\n4772\n13\n35\n0.21\n227.91\n24.76"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#exporting-the-cleaned-dataset-2",
    "href": "technical-details/data-cleaning/main.html#exporting-the-cleaned-dataset-2",
    "title": "Data Cleaning",
    "section": "Exporting the Cleaned Dataset",
    "text": "Exporting the Cleaned Dataset\nThe final cleaned dataset is saved as representation_by_county.csv.\n\nrepresentation_df.to_csv('../../data/processed-data/representation_by_county.csv', index=False)\nprint(\"Data saved to 'representation_by_county_raw.csv'\")\n\nData saved to 'representation_by_county_raw.csv'"
  },
  {
    "objectID": "technical-details/data-collection/main.html#counties-and-ratos-of-overrepresentation",
    "href": "technical-details/data-collection/main.html#counties-and-ratos-of-overrepresentation",
    "title": "Data Collection",
    "section": "Counties and Ratos of Overrepresentation",
    "text": "Counties and Ratos of Overrepresentation\nScrapping & Cleaning Rows for Exoneree Data Raw HTML from Prison Policy Initiative\n\nimport requests\nfrom bs4 import BeautifulSoup\nhtml_url = \"https://www.prisonpolicy.org/racialgeography/counties.html\"\n\nresult = requests.get(html_url)\nsoup = BeautifulSoup(result.text)\n\ntable_elt = soup.find(\"table\")\n\ntable_sio = StringIO(str(table_elt))\ncounty_df = pd.read_html(table_sio)[0]\n\ncounty_df.columns = [c.replace(\"\",\"\").replace(\"\",\"\").strip() for c in county_df.columns]\n\nil_df = county_df[county_df['State'] == \"Illinois\"].copy()\nil_df.head()\n\n\n\n\n\n\n\n\nCounty\nState\nTotal Population\nTotal White Population\nTotal Black Population\nTotal Latino Population\nIncarcerated Population\nIncarcerated White Population\nIncarcerated Black Population\nIncarcerated Latino Population\nNon-incarcerated Population\nNon-incarcerated White Population\nNon-Incarcerated Black Population\nNon-Incarcerated Latino Population\nRatio of Overrepresentation of Whites Incarcerated Compared to Whites Non-Incarcerated\nRatio of Overrepresentation of Blacks Incarcerated Compared to Blacks Non-Incarcerated\nRatio of Overrepresentation of Latinos Incarcerated Compared to Latinos Non-Incarcerated\n\n\n\n\n595\nAdams County\nIllinois\n67103\n62414\n2331\n776\n110\n73\n36\n0\n66993\n62341\n2295\n776\n0.71\n9.54\n0.00\n\n\n596\nAlexander County\nIllinois\n8238\n4983\n2915\n155\n411\n89\n242\n79\n7827\n4894\n2673\n76\n0.35\n1.72\n19.82\n\n\n597\nBond County\nIllinois\n17768\n15797\n1080\n547\n1542\n500\n657\n304\n16226\n15297\n423\n243\n0.34\n16.32\n13.14\n\n\n598\nBoone County\nIllinois\n54165\n40757\n1064\n10967\n71\n38\n12\n21\n54094\n40719\n1052\n10946\n0.71\n8.71\n1.46\n\n\n599\nBrown County\nIllinois\n6937\n5191\n1280\n402\n2059\n419\n1267\n367\n4878\n4772\n13\n35\n0.21\n227.91\n24.76\n\n\n\n\n\n\n\n\nil_df.to_csv('../../data/raw-data/representation_by_county_raw.csv', index=False)\n\nprint(\"Data saved to 'representation_by_county_raw.csv'\")\n\nData saved to 'representation_by_county_raw.csv'"
  },
  {
    "objectID": "technical-details/data-collection/main.html#county-geocoding",
    "href": "technical-details/data-collection/main.html#county-geocoding",
    "title": "Data Collection",
    "section": "County Geocoding",
    "text": "County Geocoding\nTo enrich the dataset and enable more advanced geographic analysis, I incorporated geocoded data, including latitude, longitude, and full address (geocode). This addition allows for deeper exploratory data analysis (EDA) of geographic patterns within Illinois. By including location data, I can examine trends and disparities across counties, evaluate geographic clustering of exoneration cases, and explore how systemic factors vary by region. The geocoded data adds a crucial spatial dimension to the analysis, providing the foundation for mapping, visualizations, and further geographic exploration.\nTo perform the geocoding, I used GeoPy, a Python client for geocoding web services . GeoPy provides an easy-to-use interface for accessing geocoding services, including the ability to retrieve latitude, longitude, and full address information from place names.\n\nOptimization of the Geocoding Process\nTo minimize redundant API calls and enhance efficiency, I focused on unique combinations of county and state. Instead of geocoding every record in the dataset (which would result in repeated calls for the same county, such as Cook County), I first extracted unique county-state pairs. These unique combinations were geocoded, capturing the address, latitude, and longitude for each pair with the results then mapped back to the full dataset based on county and state.\nThis approach not only reduced the number of API calls, saving both time and resources, but also ensured consistency in the geocoded results. By geocoding only unique counties,unnecessary repetition was avoided and a streamlined process was maintained while enriching the dataset with geographic context for further EDA.\n\nfrom geopy.geocoders import Nominatim  # Used for geocoding (converting addresses to locations)\n\n# Initialize the geolocator:\ngeolocator = Nominatim(user_agent=\"illinois_exoneration_geocode\") \n\n# Preprocess the County column to drop \"County\" and keep the first name only:\n# Remove \"County\" from the 'County' column and strip any extra spaces:\nil_df['County'] = il_df['County'].str.replace(\"County\", \"\").str.strip()\n\n# Get unique combinations of county and state to avoid redundant geocoding:\nunique_counties = il_df[['County', 'State']].drop_duplicates()\n\n# Function to geocode unique counties:\ndef geocode_unique(row):\n    \"\"\"\n    Geocodes a given row with 'County' and 'State' columns.\n    Returns a dictionary with address, latitude, and longitude.\n    \"\"\"\n    try:\n        # Combine 'County' and 'State' into a geocoding query:\n        location = geolocator.geocode(f\"{row['County']}, {row['State']}, USA\")\n        \n        if location:  # If geocoding is successful:\n            return {\n                'address': location.address,    # Full geocoded address\n                'latitude': location.latitude,  # Latitude coordinate\n                'longitude': location.longitude # Longitude coordinate\n            }\n        else:  # If no result is returned:\n            print(f\"Failed: No result for {row['County']}, {row['State']}\")\n            return None\n    except Exception as e:  # Handle errors \n        print(f\"Error geocoding {row['County']}, {row['State']}: {e}\")\n        return None\n\n# Apply the geocoding function to unique counties:\ngeocoded_results = unique_counties.apply(geocode_unique, axis=1)\n\n# Split the geocoding results into separate columns:\nunique_counties['geocode_address'] = geocoded_results.apply(lambda x: x['address'] if isinstance(x, dict) and 'address' in x else None)\nunique_counties['latitude'] = geocoded_results.apply(lambda x: x['latitude'] if isinstance(x, dict) and 'latitude' in x else None)\nunique_counties['longitude'] = geocoded_results.apply(lambda x: x['longitude'] if isinstance(x, dict) and 'longitude' in x else None)\n\n# Display the resulting DataFrame:\nunique_counties.head()\n\n\n\n\n\n\n\n\nCounty\nState\ngeocode_address\nlatitude\nlongitude\n\n\n\n\n595\nAdams\nIllinois\nAdams County, Illinois, United States\n39.978779\n-91.211006\n\n\n596\nAlexander\nIllinois\nAlexander County, Illinois, United States\n37.180153\n-89.350283\n\n\n597\nBond\nIllinois\nBond County, Illinois, United States\n38.863033\n-89.439142\n\n\n598\nBoone\nIllinois\nBoone County, Illinois, United States\n42.321246\n-88.823551\n\n\n599\nBrown\nIllinois\nBrown County, Illinois, United States\n39.949821\n-90.748566\n\n\n\n\n\n\n\n\n# Save the geocoded results to a CSV file \nunique_counties.to_csv(\"../../data/raw-data/geocoded_population_counties.csv\", index=False)\nprint(\"Geocoded county data saved to 'geocoded_population_counties.csv'\")\n\nGeocoded county data saved to 'geocoded_population_counties.csv'"
  },
  {
    "objectID": "technical-details/data-collection/methods.html#illinois-arrest-data",
    "href": "technical-details/data-collection/methods.html#illinois-arrest-data",
    "title": "Methods",
    "section": "",
    "text": "The Illinois arrest dataset was sourced from the Illinois Criminal Justice Information Authority’s (ICJIA) Arrest Explorer, a platform providing aggregate arrest data from the Criminal History Record Information (CHRI) system—a statewide resource for demographic and offense-related variables.1\nTo ensure privacy and confidentiality, ICJIA applied the following modifications:\n- Counts under 10 are approximated (e.g., 1 for counts 0–4, 6 for counts 5–9),\n- Subtotals, such as arrests by race or county, are accurate within +1/-1, and\n- Statewide totals align exactly with the CHRI database at the time of retrieval, which occurs twice annually.\nFurther, the dataset excludes juvenile arrests, class C misdemeanors, and cases with missing demographic details. For this project, the data was first filtered by race, county, and year, and then downloaded directly to examine patterns relevant to my analysis.2"
  },
  {
    "objectID": "technical-details/data-collection/methods.html#exoneration-data",
    "href": "technical-details/data-collection/methods.html#exoneration-data",
    "title": "Methods",
    "section": "",
    "text": "The exoneration dataset was downloaded directly from the National Registry of Exonerations, which collects and publishes searchable, online statistical data and case details for known exonerations of innocent criminal defendants in the United States from 1989 to the present.3\nThe Registry defines exonerations as cases where a person, following new evidence of innocence, is officially cleared through actions like factual declarations of innocence, pardons, or the dismissal/acquittal of charges.4\nTo access the data, a spreadsheet request form had to be submitted, and the dataset was provided under specific conditions to ensure its proper use:\n\nNo retransmission: The spreadsheet or any substantial portion of it cannot be shared with anyone who has not agreed to the conditions.\n\nAdvance notice: The National Registry must be informed in advance of any publication or distribution of data derived from the spreadsheet.\n\nCorrections and additions: Recipients agree to report any errors or missing data they identify to the Registry."
  },
  {
    "objectID": "technical-details/data-collection/methods.html#mass-incarceration-racial-geography",
    "href": "technical-details/data-collection/methods.html#mass-incarceration-racial-geography",
    "title": "Methods",
    "section": "",
    "text": "The population and incarceration data for Illinois counties were obtained by scraping Prison Policy Initiative’s website which provides information on total population and incarcerated populations broken down by race for counties across the United States5.\nTo extract the data, the requests library was used to retrieve the webpage’s HTML content, and BeautifulSoup was employed to parse the HTML and locate the relevant table which was then converted into a Pandas DataFrame for cleaning and analysis. Here is the code used:"
  },
  {
    "objectID": "technical-details/data-collection/main.html#illinois-arrest-data",
    "href": "technical-details/data-collection/main.html#illinois-arrest-data",
    "title": "Data Collection",
    "section": "Illinois Arrest Data",
    "text": "Illinois Arrest Data\nThe Illinois arrest dataset was sourced from the Illinois Criminal Justice Information Authority’s (ICJIA) Arrest Explorer, a platform providing aggregate arrest data from the Criminal History Record Information (CHRI) system—a statewide resource for demographic and offense-related variables.1\nTo ensure privacy and confidentiality, ICJIA applied the following modifications:\n\nCounts under 10 are approximated (e.g., 1 for counts 0–4, 6 for counts 5–9),\n\nSubtotals, such as arrests by race or county, are accurate within +1/-1, and\n\nStatewide totals align exactly with the CHRI database at the time of retrieval, which occurs twice annually.\n\nFurther, the dataset excludes juvenile arrests, class C misdemeanors, and cases with missing demographic details. For this project, the data was first filtered by race, county, and year, and then downloaded directly to examine patterns relevant to my analysis.2\n\nimport pandas as pd\narrest_df = pd.read_csv('../../data/raw-data/illinois_arrest_explorer_data.csv')\narrest_df.head(3)\n\n\n\n\n\n\n\n\nYear\nrace\ncounty_Adams\ncounty_Alexander\ncounty_Bond\ncounty_Boone\ncounty_Brown\ncounty_Bureau\ncounty_Calhoun\ncounty_Carroll\n...\ncounty_Wabash\ncounty_Warren\ncounty_Washington\ncounty_Wayne\ncounty_White\ncounty_Whiteside\ncounty_Will\ncounty_Williamson\ncounty_Winnebago\ncounty_Woodford\n\n\n\n\n0\n2001\nAfrican American\n226\n147\n25\n18\n18\n48\n6\n12\n...\n6\n46\n25\n6\n16\n128\n3000\n75\n2509\n42\n\n\n1\n2001\nAsian\n1\n1\n1\n1\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n1\n14\n1\n28\n1\n\n\n2\n2001\nHispanic\n1\n1\n1\n1\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n3 rows × 106 columns"
  },
  {
    "objectID": "technical-details/data-collection/main.html#exoneration-data",
    "href": "technical-details/data-collection/main.html#exoneration-data",
    "title": "Data Collection",
    "section": "Exoneration Data",
    "text": "Exoneration Data\nThe exoneration dataset was downloaded directly from the National Registry of Exonerations, a collaborative initiative by the Newkirk Center for Science and Society at the University of California (Irvine), the University of Michigan Law School, and Michigan State University College of Law. Established in 2012 by Rob Warden, then Executive Director of Northwestern University’s Pritzker School of Law’s Center on Wrongful Convictions, and Samuel R. Gross, a Law Professor at the University of Michigan, the Registry collects and publishes comprehensive, searchable statistical data and detailed case records for exonerations of innocent criminal defendants in the United States dating back to 19893.\nThe Registry defines exonerations as cases where a person, following new evidence of innocence, is officially cleared through actions like factual declarations of innocence, pardons, or the dismissal/acquittal of charges.4\nTo access the exoneration dataset, a spreadsheet request form was submitted, and the dataset was provided under conditions ensuring its proper use. These conditions include restrictions on retransmission, a requirement for advance notice of publication, and the obligation to report any identified errors or missing data.\n\nexoneration_df = pd.read_csv('../../data/raw-data/US_exoneration_data.csv')\nexoneration_df.head(3)\n\n\n\n\n\n\n\n\nLast Name\nFirst Name\nAge\nRace\nSex\nState\nCounty\nTags\nWorst Crime Display\nSentence\n...\nF/MFE\nFC\nILD\nP/FA\nDNA\nMWID\nOM\nDate of Exoneration\nDate of 1st Conviction\nDate of Release\n\n\n\n\n0\nAbbitt\nJoseph\n31.0\nBlack\nMale\nNorth Carolina\nForsyth\nCV;#IO;#SA\nChild Sex Abuse\nLife\n...\nNaN\nNaN\nNaN\nNaN\nDNA\nMWID\nNaN\n9/2/09\n6/22/95\n9/2/09\n\n\n1\nAbbott\nCinque\n19.0\nBlack\nMale\nIllinois\nCook\nCIU;#IO;#NC;#P\nDrug Possession or Sale\nProbation\n...\nNaN\nNaN\nNaN\nP/FA\nNaN\nNaN\nOM\n2/1/22\n3/25/08\n3/25/08\n\n\n2\nAbdal\nWarith Habib\n43.0\nBlack\nMale\nNew York\nErie\nIO;#SA\nSexual Assault\n20 to Life\n...\nF/MFE\nNaN\nNaN\nNaN\nDNA\nMWID\nOM\n9/1/99\n6/6/83\n9/1/99\n\n\n\n\n3 rows × 22 columns"
  },
  {
    "objectID": "technical-details/data-collection/main.html#mass-incarceration-racial-geography",
    "href": "technical-details/data-collection/main.html#mass-incarceration-racial-geography",
    "title": "Data Collection",
    "section": "Mass Incarceration Racial Geography",
    "text": "Mass Incarceration Racial Geography\nThe population and incarceration data for Illinois counties were obtained by scraping Prison Policy Initiative’s website which provides information on total population and incarcerated populations broken down by race for counties across the United States5.\nTo extract the data, the requests library was used to retrieve the webpage’s HTML content, and BeautifulSoup was employed to parse the HTML and locate the relevant table which was then converted into a Pandas DataFrame for cleaning and analysis. Here is the code used:\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Retrieve the HTML content of the target webpage\nhtml_url = \"https://www.prisonpolicy.org/racialgeography/counties.html\"\nresult = requests.get(html_url)\n\n# Parse the HTML content with BeautifulSoup\nsoup = BeautifulSoup(result.text)\n\n# Locate the table element containing the data\ntable_elt = soup.find(\"table\")\n\n# Convert the HTML table into a Pandas DataFrame\ntable_sio = StringIO(str(table_elt))\ncounty_df = pd.read_html(table_sio)[0]\n\n# Clean the column names by removing unnecessary characters\ncounty_df.columns = [c.replace(\"\",\"\").replace(\"\",\"\").strip() for c in county_df.columns]\n\n# Filter for Illinois\nil_df = county_df[county_df['State'] == \"Illinois\"].copy()\n\n# Export to CSV\nil_df.to_csv('../../data/raw-data/representation_by_county_raw.csv', index=False)\nprint(\"Data saved to 'representation_by_county_raw.csv'\")\nil_df.head()\n\nData saved to 'representation_by_county_raw.csv'\n\n\n\n\n\n\n\n\n\nCounty\nState\nTotal Population\nTotal White Population\nTotal Black Population\nTotal Latino Population\nIncarcerated Population\nIncarcerated White Population\nIncarcerated Black Population\nIncarcerated Latino Population\nNon-incarcerated Population\nNon-incarcerated White Population\nNon-Incarcerated Black Population\nNon-Incarcerated Latino Population\nRatio of Overrepresentation of Whites Incarcerated Compared to Whites Non-Incarcerated\nRatio of Overrepresentation of Blacks Incarcerated Compared to Blacks Non-Incarcerated\nRatio of Overrepresentation of Latinos Incarcerated Compared to Latinos Non-Incarcerated\n\n\n\n\n595\nAdams County\nIllinois\n67103\n62414\n2331\n776\n110\n73\n36\n0\n66993\n62341\n2295\n776\n0.71\n9.54\n0.00\n\n\n596\nAlexander County\nIllinois\n8238\n4983\n2915\n155\n411\n89\n242\n79\n7827\n4894\n2673\n76\n0.35\n1.72\n19.82\n\n\n597\nBond County\nIllinois\n17768\n15797\n1080\n547\n1542\n500\n657\n304\n16226\n15297\n423\n243\n0.34\n16.32\n13.14\n\n\n598\nBoone County\nIllinois\n54165\n40757\n1064\n10967\n71\n38\n12\n21\n54094\n40719\n1052\n10946\n0.71\n8.71\n1.46\n\n\n599\nBrown County\nIllinois\n6937\n5191\n1280\n402\n2059\n419\n1267\n367\n4878\n4772\n13\n35\n0.21\n227.91\n24.76"
  },
  {
    "objectID": "technical-details/data-collection/main.html#geographical-data",
    "href": "technical-details/data-collection/main.html#geographical-data",
    "title": "Data Collection",
    "section": "Geographical Data",
    "text": "Geographical Data\nTo enable advanced geographic analysis, I incorporated geocoded data, including latitude, longitude, and full address. This addition will hopefully allow for deeper exploratory data analysis (EDA) of geographic patterns within Illinois including location data which will allow me to examine trends and disparities across counties, evaluate geographic clustering of exoneration cases, and explore how systemic factors vary by region. This will ultimatley add a crucial spatial dimension to the overall analysis that provides the foundation for mapping, visualizations, and further geographic exploration.\nThe geocoding process was conducted using GeoPy, a Python library that serves as an interface to geocoding APIs, specifically the Nominatim API from OpenStreetMap. GeoPy simplifies the retrieval of geographic details like latitude, longitude, and full address from place names by sending requests to the Nominatim API and processing the responses.\n\n# Import the Nominatim geocoder for converting location names into geographic coordinates:\nfrom geopy.geocoders import Nominatim  \n\n# Initialize the geolocator with a user-defined agent to avoid request limits:\ngeolocator = Nominatim(user_agent=\"illinois_exoneration_geocode\") \n\n# Clean the 'County' column by removing the word \"County\" and any extra spaces:\nil_df['County'] = il_df['County'].str.replace(\"County\", \"\").str.strip()\n\n# Rename the DataFrame to clarify it contains Illinois counties:\nillinois_counties = il_df[['County', 'State']].copy()\n\n# Define a function to geocode counties and return geographic details:\ndef geocode_county(row):\n    \"\"\"\n    Takes a row containing 'County' and 'State' columns.\n    Uses the geolocator to find the full address, latitude, and longitude.\n    Returns a dictionary with the geocoded data or None if geocoding fails.\n    \"\"\"\n    try:\n        # Combine county and state into a query string for geocoding:\n        location = geolocator.geocode(f\"{row['County']}, {row['State']}, USA\")\n        \n        # If a location is found, return the geocoded details:\n        if location:\n            return {\n                'address': location.address,    # The full geocoded address\n                'latitude': location.latitude,  # The latitude coordinate\n                'longitude': location.longitude # The longitude coordinate\n            }\n        else:  # Print a message if no geocoding result is found:\n            print(f\"Failed: No result for {row['County']}, {row['State']}\")\n            return None\n    except Exception as e:  # Handle errors during geocoding and print them:\n        print(f\"Error geocoding {row['County']}, {row['State']}: {e}\")\n        return None\n\n# Apply the geocoding function to each Illinois county:\ngeocoded_results = illinois_counties.apply(geocode_county, axis=1)\n\n# Extract the geocoding results into separate columns for address, latitude, and longitude:\nillinois_counties['geocode_address'] = geocoded_results.apply(lambda x: x['address'] if isinstance(x, dict) and 'address' in x else None)\nillinois_counties['latitude'] = geocoded_results.apply(lambda x: x['latitude'] if isinstance(x, dict) and 'latitude' in x else None)\nillinois_counties['longitude'] = geocoded_results.apply(lambda x: x['longitude'] if isinstance(x, dict) and 'longitude' in x else None)\n\n# Display the first few rows of the DataFrame with geocoded data:\nillinois_counties.head()\n\n\n\n\n\n\n\n\nCounty\nState\ngeocode_address\nlatitude\nlongitude\n\n\n\n\n595\nAdams\nIllinois\nAdams County, Illinois, United States\n39.978779\n-91.211006\n\n\n596\nAlexander\nIllinois\nAlexander County, Illinois, United States\n37.180153\n-89.350283\n\n\n597\nBond\nIllinois\nBond County, Illinois, United States\n38.863033\n-89.439142\n\n\n598\nBoone\nIllinois\nBoone County, Illinois, United States\n42.321246\n-88.823551\n\n\n599\nBrown\nIllinois\nBrown County, Illinois, United States\n39.949821\n-90.748566\n\n\n\n\n\n\n\n\n# Save the geocoded results to a CSV file \nillinois_counties.to_csv(\"../../data/raw-data/geocoded_population_counties.csv\", index=False)\nprint(\"Geocoded county data saved to 'geocoded_population_counties.csv'\")\n\nGeocoded county data saved to 'geocoded_population_counties.csv'"
  },
  {
    "objectID": "technical-details/data-collection/main.html#illinois-shapefile",
    "href": "technical-details/data-collection/main.html#illinois-shapefile",
    "title": "Data Collection",
    "section": "Illinois Shapefile",
    "text": "Illinois Shapefile\nTo visualize geocoded data and perform geographic exploratory data analysis (EDA), I required a shapefile for Illinois county boundaries. While the Census Bureau provides Illinois shapefiles, these files did not work as expected for my Exploratory Data Analysis (EDA) purposes due to compatibility issues.\nInstead, I sourced a shapefile directly from the Illinois State Geological Survey (ISGS) which included county boundaries in a format compatible with the GIS tools I used. ISGS’s shapefiles provided the geographic foundation for mapping and visualizing trends across Illinois counties for EDA which will allow me to adda crucial layer of context to the datasets and support meaningful EDA.6"
  },
  {
    "objectID": "technical-details/data-collection/overview.html#goals",
    "href": "technical-details/data-collection/overview.html#goals",
    "title": "Overview",
    "section": "",
    "text": "The primary goal of the data collection process is to gather diverse datasets necessary for investigating systemic racial disparities in policing and exonerations. This includes sourcing reliable and representative arrest records, exoneration data, incarceration demographics, and geospatial information. The objective is to compile datasets that can independently and collectively provide insight into over-policing trends, racial patterns in exonerations, and the geographic context of these phenomena."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#objectives",
    "href": "technical-details/data-collection/overview.html#objectives",
    "title": "Overview",
    "section": "",
    "text": "Identify and collect datasets needed to analyze racial disparities in over-policing and wrongful convictions in Illinois, including arrest records, exoneration data, incarceration demographics, and geospatial information.\n\nEnsure datasets are accurate, up-to-date, and aligned with the project’s goals by prioritizing reliable sources like official registries and well-documented repositories.\n\nStructure the data to support analysis of racial patterns in policing and exonerations, allowing for both independent and interconnected evaluations.\n\nDocument all steps in the data collection process, including sourcing methods and preprocessing workflows, to ensure transparency and reproducibility.\n\nCollect and preprocess geocoded data and Illinois county shapefiles to enable geographic exploratory data analysis and visualizations."
  },
  {
    "objectID": "technical-details/data-collection/main.html#goals",
    "href": "technical-details/data-collection/main.html#goals",
    "title": "Data Collection",
    "section": "",
    "text": "The primary goal of the data collection process is to gather diverse datasets necessary for investigating systemic racial disparities in policing and exonerations. This includes sourcing reliable and representative arrest records, exoneration data, incarceration demographics, and geospatial information. The objective is to compile datasets that can independently and collectively provide insight into over-policing trends, racial patterns in exonerations, and the geographic context of these phenomena."
  },
  {
    "objectID": "technical-details/data-collection/main.html#objectives",
    "href": "technical-details/data-collection/main.html#objectives",
    "title": "Data Collection",
    "section": "",
    "text": "Identify and collect datasets needed to analyze racial disparities in over-policing and wrongful convictions in Illinois, including arrest records, exoneration data, incarceration demographics, and geospatial information.\n\nEnsure datasets are accurate, up-to-date, and aligned with the project’s goals by prioritizing reliable sources like official registries and well-documented repositories.\n\nStructure the data to support analysis of racial patterns in policing and exonerations, allowing for both independent and interconnected evaluations.\n\nDocument all steps in the data collection process, including sourcing methods and preprocessing workflows, to ensure transparency and reproducibility.\n\nCollect and preprocess geocoded data and Illinois county shapefiles to enable geographic exploratory data analysis and visualizations."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#challenges-1",
    "href": "technical-details/data-collection/closing.html#challenges-1",
    "title": "Summary",
    "section": "",
    "text": "Accurate and reliable crime data are the foundation for meaningful analysis, yet longstanding challenges in criminal history record systems undermine their precision and completenesswoodard_use_1993?. As highlighted in the Use and Management of Criminal History Record Information Report (1993), significant issues stem from incomplete data reporting, delays in recording arrests and case dispositions, and inconsistent fingerprint submissions. Criminal History Record Information (CHRI), which serves as the backbone for arrest datasets like the one from the Illinois Criminal Justice Information Authority (ICJIA), is particularly vulnerable to these challenges. Dispositions—outcomes of arrests—are frequently missing or delayed, leaving gaps in the data that complicate any attempt to analyze systemic trends. While technological advancements such as digital fingerprinting have improved certain processes, data fragmentation and insufficient oversight remain persistent barriers to comprehensive and accurate reporting.\nFast forward three decades, and many of these challenges remain unresolved, now exacerbated by modern systems’ uneven implementation. The Marshall Project highlights a critical example: in 2022, the FBI’s shift to the National Incident-Based Reporting System (NIBRS) created a significant data gap in national crime statisticsli_many_2023?. Over 6,000 police agencies failed to submit their data, representing one-third of all agencies and leaving large segments of the U.S. population unaccounted for. Major departments, including the NYPD and LAPD, were absent from federal crime datasets, alongside many smaller agenciesli_many_2023?. These failures reflect broader systemic issues—inconsistent adoption of modern systems, lack of funding, and inadequate oversight—that mirror the challenges identified decades earlier.\nThat is to say, crime data, even when sourced from official systems, is inherently imperfect and incomplete. Whether due to outdated processes, inconsistent reporting, privacy-driven modifications, or gaps in modern data collection systems, crime data often fails to provide a fully accurate or comprehensive picture of reality. These limitations are not new—they have persisted for decades—and continue to challenge researchers and policymakers alike. As a result, any analysis relying on such data must account for these uncertainties, recognizing that while the data can reveal significant trends and systemic issues, it is rarely a flawless representation of the truth.\n\n\nThe Illinois arrest dataset obtained from the ICJIA Arrest Explorer uses approximations for counts under 10 (e.g., replacing values 0–4 with 1 and 5–9 with 6) to ensure privacy. While this practice protects confidentiality, it also introduces inaccuracies, particularly for smaller counties or demographic groups, where such approximations can distort trends and analytical precision. Moreover, the lack of oversight in reporting raises questions about data consistency and accuracy, a limitation inherent in many criminal justice datasets."
  },
  {
    "objectID": "technical-details/data-collection/main.html#challenges-1",
    "href": "technical-details/data-collection/main.html#challenges-1",
    "title": "Data Collection",
    "section": "Challenges",
    "text": "Challenges\nAccurate and reliable crime data are the foundation for meaningful analysis, yet longstanding challenges in criminal history record systems undermine their precision and completeness6. As highlighted in the Use and Management of Criminal History Record Information Report (1993), significant issues stem from incomplete data reporting, delays in recording arrests and case dispositions, and inconsistent fingerprint submissions. Criminal History Record Information (CHRI), which serves as the backbone for arrest datasets like the one from the Illinois Criminal Justice Information Authority (ICJIA), is particularly vulnerable to these challenges. Dispositions—outcomes of arrests—are frequently missing or delayed, leaving gaps in the data that complicate any attempt to analyze systemic trends. While technological advancements such as digital fingerprinting have improved certain processes, data fragmentation and insufficient oversight remain persistent barriers to comprehensive and accurate reporting.\nFast forward three decades, and many of these challenges remain unresolved, now exacerbated by modern systems’ uneven implementation. The Marshall Project highlights a critical example: in 2022, the FBI’s shift to the National Incident-Based Reporting System (NIBRS) created a significant data gap in national crime statistics7. Over 6,000 police agencies failed to submit their data, representing one-third of all agencies and leaving large segments of the U.S. population unaccounted for. Major departments, including the NYPD and LAPD, were absent from federal crime datasets, alongside many smaller agencies7. These failures reflect broader systemic issues—inconsistent adoption of modern systems, lack of funding, and inadequate oversight—that mirror the challenges identified decades earlier.\nThat is to say, crime data, even when sourced from official systems, is inherently imperfect and incomplete. Whether due to outdated processes, inconsistent reporting, privacy-driven modifications, or gaps in modern data collection systems, crime data often fails to provide a fully accurate or comprehensive picture of reality. These limitations are not new—they have persisted for decades—and continue to challenge researchers and policymakers alike. As a result, any analysis relying on such data must account for these uncertainties, recognizing that while the data can reveal significant trends and systemic issues, it is rarely a flawless representation of the truth.\n\nArrest Data Precision\nThe Illinois arrest dataset obtained from the ICJIA Arrest Explorer uses approximations for counts under 10 (e.g., replacing values 0–4 with 1 and 5–9 with 6) to ensure privacy. While this practice protects confidentiality, it also introduces inaccuracies, particularly for smaller counties or demographic groups, where such approximations can distort trends and analytical precision. Moreover, the lack of oversight in reporting raises questions about data consistency and accuracy, a limitation inherent in many criminal justice datasets."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#motivation",
    "href": "technical-details/data-collection/overview.html#motivation",
    "title": "Overview",
    "section": "",
    "text": "The motivation behind the data collection process stems from the need to uncover data that supports a detailed analysis of racial bias in the criminal justice system. By sourcing arrest records, exoneration data, and geospatial information, the project seeks to ensure the data reflects the systemic inequities affecting marginalized communities, particularly Black populations. The aim is to establish a solid foundation for further exploration of the relationships between policing, exonerations, and structural injustices, while maintaining transparency and reproducibility in the data collection process."
  },
  {
    "objectID": "technical-details/data-collection/main.html#motivation",
    "href": "technical-details/data-collection/main.html#motivation",
    "title": "Data Collection",
    "section": "",
    "text": "The motivation behind the data collection process stems from the need to uncover data that supports a detailed analysis of racial bias in the criminal justice system. By sourcing arrest records, exoneration data, and geospatial information, the project seeks to ensure the data reflects the systemic inequities affecting marginalized communities, particularly Black populations. The aim is to establish a solid foundation for further exploration of the relationships between policing, exonerations, and structural injustices, while maintaining transparency and reproducibility in the data collection process."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#goals-motivation",
    "href": "technical-details/data-collection/overview.html#goals-motivation",
    "title": "Overview",
    "section": "",
    "text": "My goal in the data collection process is to gather the datasets needed to examine systemic racial disparities in policing and exonerations. I’m focused on sourcing reliable arrest records, exoneration data, incarceration demographics, and geospatial information. Ultimately, I aim to compile data that can stand alone or work together to analyze over-policing trends, uncover racial patterns in exonerations, and explore the geographic dynamics connected to these issues. I’m particularly driven by the need to ground these systemic inequities in hard numbers and statistical analysis. Too often, people refuse to acknowledge the reality of racism without concrete evidence. By putting real data behind these patterns, I hope to demonstrate the scope and impact of these injustices in a way that cannot be ignored."
  },
  {
    "objectID": "technical-details/data-collection/main.html#goals-motivation",
    "href": "technical-details/data-collection/main.html#goals-motivation",
    "title": "Data Collection",
    "section": "",
    "text": "My goal in the data collection process is to gather the datasets needed to examine systemic racial disparities in policing and exonerations. I’m focused on sourcing reliable arrest records, exoneration data, incarceration demographics, and geospatial information. Ultimately, I aim to compile data that can stand alone or work together to analyze over-policing trends, uncover racial patterns in exonerations, and explore the geographic dynamics connected to these issues. I’m particularly driven by the need to ground these systemic inequities in hard numbers and statistical analysis. Too often, people refuse to acknowledge the reality of racism without concrete evidence. By putting real data behind these patterns, I hope to demonstrate the scope and impact of these injustices in a way that cannot be ignored."
  },
  {
    "objectID": "technical-details/data-collection/main.html#foundation-exoneration-and-arrest-datasets",
    "href": "technical-details/data-collection/main.html#foundation-exoneration-and-arrest-datasets",
    "title": "Data Collection",
    "section": "Foundation: Exoneration and Arrest Datasets",
    "text": "Foundation: Exoneration and Arrest Datasets\nThe exoneration and arrest datasets formed the backbone of the project, each bringing a unique perspective to the analysis of racial disparities in the criminal justice system.\n\nIllinois Arrest Data\nThe Illinois arrest dataset was sourced from the Illinois Criminal Justice Information Authority’s (ICJIA) Arrest Explorer, a platform providing aggregate arrest data from the Criminal History Record Information (CHRI) system—a statewide resource for demographic and offense-related variables.1\nTo ensure privacy and confidentiality, ICJIA applied the following modifications:\n\nCounts under 10 are approximated (e.g., 1 for counts 0–4, 6 for counts 5–9),\n\nSubtotals, such as arrests by race or county, are accurate within +1/-1, and\n\nStatewide totals align exactly with the CHRI database at the time of retrieval, which occurs twice annually.\n\nFurther, the dataset excludes juvenile arrests, class C misdemeanors, and cases with missing demographic details. For this project, the data was first filtered by race, county, and year, and then downloaded directly to examine patterns relevant to my analysis.2\n\nimport pandas as pd\narrest_df = pd.read_csv('../../data/raw-data/illinois_arrest_explorer_data.csv')\narrest_df.head(3)\n\n\n\n\n\n\n\n\nYear\nrace\ncounty_Adams\ncounty_Alexander\ncounty_Bond\ncounty_Boone\ncounty_Brown\ncounty_Bureau\ncounty_Calhoun\ncounty_Carroll\n...\ncounty_Wabash\ncounty_Warren\ncounty_Washington\ncounty_Wayne\ncounty_White\ncounty_Whiteside\ncounty_Will\ncounty_Williamson\ncounty_Winnebago\ncounty_Woodford\n\n\n\n\n0\n2001\nAfrican American\n226\n147\n25\n18\n18\n48\n6\n12\n...\n6\n46\n25\n6\n16\n128\n3000\n75\n2509\n42\n\n\n1\n2001\nAsian\n1\n1\n1\n1\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n1\n14\n1\n28\n1\n\n\n2\n2001\nHispanic\n1\n1\n1\n1\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n3 rows × 106 columns\n\n\n\n\n\nExoneration Data\nThe exoneration dataset was downloaded directly from the National Registry of Exonerations, a collaborative initiative by the Newkirk Center for Science and Society at the University of California (Irvine), the University of Michigan Law School, and Michigan State University College of Law. Established in 2012 by Rob Warden, then Executive Director of Northwestern University’s Pritzker School of Law’s Center on Wrongful Convictions, and Samuel R. Gross, a Law Professor at the University of Michigan, the Registry collects and publishes comprehensive, searchable statistical data and detailed case records for exonerations of innocent criminal defendants in the United States dating back to 19893.\nThe Registry defines exonerations as cases where a person, following new evidence of innocence, is officially cleared through actions like factual declarations of innocence, pardons, or the dismissal/acquittal of charges.4\nTo access the exoneration dataset, a spreadsheet request form was submitted, and the dataset was provided under conditions ensuring its proper use. These conditions include restrictions on retransmission, a requirement for advance notice of publication, and the obligation to report any identified errors or missing data.\n\nexoneration_df = pd.read_csv('../../data/raw-data/US_exoneration_data.csv')\nexoneration_df.head(3)\n\n\n\n\n\n\n\n\nLast Name\nFirst Name\nAge\nRace\nSex\nState\nCounty\nTags\nWorst Crime Display\nSentence\n...\nF/MFE\nFC\nILD\nP/FA\nDNA\nMWID\nOM\nDate of Exoneration\nDate of 1st Conviction\nDate of Release\n\n\n\n\n0\nAbbitt\nJoseph\n31.0\nBlack\nMale\nNorth Carolina\nForsyth\nCV;#IO;#SA\nChild Sex Abuse\nLife\n...\nNaN\nNaN\nNaN\nNaN\nDNA\nMWID\nNaN\n9/2/09\n6/22/95\n9/2/09\n\n\n1\nAbbott\nCinque\n19.0\nBlack\nMale\nIllinois\nCook\nCIU;#IO;#NC;#P\nDrug Possession or Sale\nProbation\n...\nNaN\nNaN\nNaN\nP/FA\nNaN\nNaN\nOM\n2/1/22\n3/25/08\n3/25/08\n\n\n2\nAbdal\nWarith Habib\n43.0\nBlack\nMale\nNew York\nErie\nIO;#SA\nSexual Assault\n20 to Life\n...\nF/MFE\nNaN\nNaN\nNaN\nDNA\nMWID\nOM\n9/1/99\n6/6/83\n9/1/99\n\n\n\n\n3 rows × 22 columns"
  },
  {
    "objectID": "technical-details/data-collection/main.html#adding-context-web-scraping-and-demographic-data",
    "href": "technical-details/data-collection/main.html#adding-context-web-scraping-and-demographic-data",
    "title": "Data Collection",
    "section": "Adding Context: Web Scraping and Demographic Data",
    "text": "Adding Context: Web Scraping and Demographic Data\nTo contextualize these datasets within broader racial and geographic dynamics, I scraped incarceration demographics for Illinois counties from the Prison Policy Initiative’s. Using Python libraries like requests and BeautifulSoup, I extracted and processed incarceration rates by race for geographic analysis. This provided insight into racial overrepresentation within incarceration systems and illuminated trends at the county level.5\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Retrieve the HTML content of the target webpage\nhtml_url = \"https://www.prisonpolicy.org/racialgeography/counties.html\"\nresult = requests.get(html_url)\n\n# Parse the HTML content with BeautifulSoup\nsoup = BeautifulSoup(result.text)\n\n# Locate the table element containing the data\ntable_elt = soup.find(\"table\")\n\n# Convert the HTML table into a Pandas DataFrame\ntable_sio = StringIO(str(table_elt))\ncounty_df = pd.read_html(table_sio)[0]\n\n# Clean the column names by removing unnecessary characters\ncounty_df.columns = [c.replace(\"\",\"\").replace(\"\",\"\").strip() for c in county_df.columns]\n\n# Filter for Illinois\nil_df = county_df[county_df['State'] == \"Illinois\"].copy()\n\n# Export to CSV\nil_df.to_csv('../../data/raw-data/representation_by_county_raw.csv', index=False)\nprint(\"Data saved to 'representation_by_county_raw.csv'\")\nil_df.head()\n\nData saved to 'representation_by_county_raw.csv'\n\n\n\n\n\n\n\n\n\nCounty\nState\nTotal Population\nTotal White Population\nTotal Black Population\nTotal Latino Population\nIncarcerated Population\nIncarcerated White Population\nIncarcerated Black Population\nIncarcerated Latino Population\nNon-incarcerated Population\nNon-incarcerated White Population\nNon-Incarcerated Black Population\nNon-Incarcerated Latino Population\nRatio of Overrepresentation of Whites Incarcerated Compared to Whites Non-Incarcerated\nRatio of Overrepresentation of Blacks Incarcerated Compared to Blacks Non-Incarcerated\nRatio of Overrepresentation of Latinos Incarcerated Compared to Latinos Non-Incarcerated\n\n\n\n\n595\nAdams County\nIllinois\n67103\n62414\n2331\n776\n110\n73\n36\n0\n66993\n62341\n2295\n776\n0.71\n9.54\n0.00\n\n\n596\nAlexander County\nIllinois\n8238\n4983\n2915\n155\n411\n89\n242\n79\n7827\n4894\n2673\n76\n0.35\n1.72\n19.82\n\n\n597\nBond County\nIllinois\n17768\n15797\n1080\n547\n1542\n500\n657\n304\n16226\n15297\n423\n243\n0.34\n16.32\n13.14\n\n\n598\nBoone County\nIllinois\n54165\n40757\n1064\n10967\n71\n38\n12\n21\n54094\n40719\n1052\n10946\n0.71\n8.71\n1.46\n\n\n599\nBrown County\nIllinois\n6937\n5191\n1280\n402\n2059\n419\n1267\n367\n4878\n4772\n13\n35\n0.21\n227.91\n24.76"
  },
  {
    "objectID": "technical-details/data-collection/main.html#geographical-insights-api-based-geocoding",
    "href": "technical-details/data-collection/main.html#geographical-insights-api-based-geocoding",
    "title": "Data Collection",
    "section": "Geographical Insights: API-Based Geocoding",
    "text": "Geographical Insights: API-Based Geocoding\nTo strengthen the exploratory data analysis (EDA), I utilized geocoded data, adding latitude, longitude, and full addresses for Illinois counties. This addition enabled the visualization of systemic racial disparities across geographic areas, revealing trends and disparities among counties, identifying geographic clusters of exoneration cases, and examining regional variations in systemic factors.\nThe geocoding process was conducted using GeoPy, a Python library that serves as an interface to geocoding APIs, specifically the Nominatim API from OpenStreetMap. GeoPy simplifies the retrieval of geographic details like latitude, longitude, and full address from place names by sending requests to the Nominatim API and processing the responses.\n\n# Import the Nominatim geocoder for converting location names into geographic coordinates:\nfrom geopy.geocoders import Nominatim  \n\n# Initialize the geolocator with a user-defined agent to avoid request limits:\ngeolocator = Nominatim(user_agent=\"illinois_exoneration_geocode\") \n\n# Clean the 'County' column by removing the word \"County\" and any extra spaces:\nil_df['County'] = il_df['County'].str.replace(\"County\", \"\").str.strip()\n\n# Rename the DataFrame to clarify it contains Illinois counties:\nillinois_counties = il_df[['County', 'State']].copy()\n\n# Define a function to geocode counties and return geographic details:\ndef geocode_county(row):\n    \"\"\"\n    Takes a row containing 'County' and 'State' columns.\n    Uses the geolocator to find the full address, latitude, and longitude.\n    Returns a dictionary with the geocoded data or None if geocoding fails.\n    \"\"\"\n    try:\n        # Combine county and state into a query string for geocoding:\n        location = geolocator.geocode(f\"{row['County']}, {row['State']}, USA\")\n        \n        # If a location is found, return the geocoded details:\n        if location:\n            return {\n                'address': location.address,    # The full geocoded address\n                'latitude': location.latitude,  # The latitude coordinate\n                'longitude': location.longitude # The longitude coordinate\n            }\n        else:  # Print a message if no geocoding result is found:\n            print(f\"Failed: No result for {row['County']}, {row['State']}\")\n            return None\n    except Exception as e:  # Handle errors during geocoding and print them:\n        print(f\"Error geocoding {row['County']}, {row['State']}: {e}\")\n        return None\n\n# Apply the geocoding function to each Illinois county:\ngeocoded_results = illinois_counties.apply(geocode_county, axis=1)\n\n# Extract the geocoding results into separate columns for address, latitude, and longitude:\nillinois_counties['geocode_address'] = geocoded_results.apply(lambda x: x['address'] if isinstance(x, dict) and 'address' in x else None)\nillinois_counties['latitude'] = geocoded_results.apply(lambda x: x['latitude'] if isinstance(x, dict) and 'latitude' in x else None)\nillinois_counties['longitude'] = geocoded_results.apply(lambda x: x['longitude'] if isinstance(x, dict) and 'longitude' in x else None)\n\n# Display the first few rows of the DataFrame with geocoded data:\nillinois_counties.head()\n\n\n\n\n\n\n\n\nCounty\nState\ngeocode_address\nlatitude\nlongitude\n\n\n\n\n595\nAdams\nIllinois\nAdams County, Illinois, United States\n39.978779\n-91.211006\n\n\n596\nAlexander\nIllinois\nAlexander County, Illinois, United States\n37.180153\n-89.350283\n\n\n597\nBond\nIllinois\nBond County, Illinois, United States\n38.863033\n-89.439142\n\n\n598\nBoone\nIllinois\nBoone County, Illinois, United States\n42.321246\n-88.823551\n\n\n599\nBrown\nIllinois\nBrown County, Illinois, United States\n39.949821\n-90.748566\n\n\n\n\n\n\n\n\n# Save the geocoded results to a CSV file \nillinois_counties.to_csv(\"../../data/raw-data/geocoded_population_counties.csv\", index=False)\nprint(\"Geocoded county data saved to 'geocoded_population_counties.csv'\")\n\nGeocoded county data saved to 'geocoded_population_counties.csv'"
  },
  {
    "objectID": "technical-details/data-collection/main.html#mapping-illinois-county",
    "href": "technical-details/data-collection/main.html#mapping-illinois-county",
    "title": "Data Collection",
    "section": "Mapping: Illinois County",
    "text": "Mapping: Illinois County\nTo visualize geocoded data and perform geographic exploratory data analysis (EDA), I required a shapefile for Illinois county boundaries. While the Census Bureau provides Illinois shapefiles, these files did not work as expected for my Exploratory Data Analysis (EDA) purposes due to compatibility issues.\nInstead, I sourced a shapefile directly from the Illinois State Geological Survey (ISGS) which included county boundaries in a format compatible with the GIS tools I used. ISGS’s shapefiles provided the geographic foundation for mapping and visualizing trends across Illinois counties for EDA which will allow me to adda crucial layer of context to the datasets and support meaningful EDA.6"
  },
  {
    "objectID": "technical-details/data-balancing/main.html#why-use-counterfactual-data",
    "href": "technical-details/data-balancing/main.html#why-use-counterfactual-data",
    "title": "Counterfactual Data Balancing",
    "section": "",
    "text": "Counterfactual data is a necessity when access to complete prison population records is unavailable. Since I don’t have access to a full dataset of all incarcerated individuals in Illinois and their exoneration statuses (e.g., ‘exonerated’, ‘not exonerate’), I relied on counterfactuals to bridge the gap and construct a balanced dataset.\nThe idea behind counterfactuals is simple: they allow us to ask “what if?” questions. For example: What if an exonerated person had not been exonerated? Would their characteristics look similar to non-exonerated individuals? Counterfactual data helps isolate these comparisons by holding everything else constant except the hypothetical condition—in this case, exoneration.\nAs explained in this primer on counterfactuals, a counterfactual statement operates on an unrealized “if” condition. The “if” portion, also known as the antecedent, frames the comparison: exonerated individuals versus those who weren’t. This approach is powerful because it reduces bias and ensures that the model is trained on data that is reliable, balanced, and representative.1\n\n\nThe implementation of this counterfactual data balancing relied heavily on expert guidance and code contributions from Professor Jeff Jacobs. His insights and support were invaluable in refining the methodology and making this process possible."
  },
  {
    "objectID": "technical-details/data-balancing/main.html#draw-representative-samples",
    "href": "technical-details/data-balancing/main.html#draw-representative-samples",
    "title": "Counterfactual Data Balancing",
    "section": "Draw Representative Samples",
    "text": "Draw Representative Samples\nThe first step in the simulation is to draw a representative sample of 548 “people” from the Illinois prison population. To achieve this, a weighted random sample with replacement was performed from the il_df dataset. Sampling weights were determined based on each county’s total incarcerated population, ensuring that counties with larger populations contributed proportionally more to the sample.\nA random seed (random_state=5000) was set to ensure the results are replicable. This step produces a valid population-weighted sample where the only known characteristic of each “person” is their county.\n\nil_sample_df = il_df.sample(\n    num_il,\n    replace = True,\n    weights = il_df['Total'],\n    random_state = 5000,\n).copy()\nil_sample_df.head()\n\n\n\n\n\n\n\n\ncounty\nstate\nTotal\nWhite\nBlack\nLatino\nOther\n\n\n\n\n15\nCook\nIllinois\n11649\n1769\n8369\n1468\n43\n\n\n36\nHenry\nIllinois\n301\n172\n108\n21\n0\n\n\n72\nPerry\nIllinois\n2323\n561\n1398\n352\n12\n\n\n15\nCook\nIllinois\n11649\n1769\n8369\n1468\n43\n\n\n53\nLogan\nIllinois\n3060\n963\n1705\n389\n3\n\n\n\n\n\n\n\n\nil_sample_df['county'].value_counts(normalize=True).head()\n\ncounty\nCook        0.142336\nWill        0.060219\nRandolph    0.056569\nPerry       0.040146\nLogan       0.040146\nName: proportion, dtype: float64"
  },
  {
    "objectID": "technical-details/data-balancing/main.html#simulating-racial-distribution",
    "href": "technical-details/data-balancing/main.html#simulating-racial-distribution",
    "title": "Counterfactual Data Balancing",
    "section": "Simulating Racial Distribution",
    "text": "Simulating Racial Distribution\nTo replicate the racial makeup of the incarcerated population, racial counts for each county were used to create a probability distribution for race. For each row in il_sample_df (which represents a sampled county), a distribution was formed based on the race-specific counts, and a single “person” was drawn from that distribution.\nThis process was done row-by-row using NumPy’s random.choice() function. A random seed (RNG) was also set to ensure the results remain consistent and replicable across runs.\n\nrng = np.random.default_rng(seed = 5000)\ndef draw_race_sample(row):\n  race_counts = [row[cur_val] for cur_val in race_category_names]\n  total_count = sum(race_counts)\n  race_probs = [cur_count / total_count for cur_count in race_counts]\n  # And now we have a probability distribution! We can use rng.choice() to sample from it\n  sampled_vals = rng.choice(race_category_names, size=1, p=race_probs)\n  # We only sampled 1 value here, so we use [0] to extract it\n  sampled_val = list(sampled_vals)[0]\n  return sampled_val\n\nBefore sampling, the function was tested by drawing multiple samples for a specific county—Cook County, in this case. To verify its accuracy, the expected proportions for sampling N inmates from Cook were first computed.\n\ncook_row = il_df[il_df['county'] == \"Cook\"].iloc[0]\nfor cname in race_category_names:\n  cook_row[f'{cname}_prop'] = cook_row[cname] / cook_row['Total']\ncook_row\n\ncounty             Cook\nstate          Illinois\nTotal             11649\nWhite              1769\nBlack              8369\nLatino             1468\nOther                43\nWhite_prop     0.151859\nBlack_prop     0.718431\nLatino_prop    0.126019\nOther_prop     0.003691\nName: 15, dtype: object\n\n\nThis means that if the draw_race_sample() function is working correctly, it should generate “White” 15.2% of the time, “Black” 71.8% of the time, and so on. To confirm this, a sample of size N=5000 was generated from Cook County to check whether the proportions align with the expected values.\n\nN = 5000\ncook_samples = [draw_race_sample(cook_row) for _ in range(N)]\ncook_sample_df = pd.DataFrame(cook_samples, columns = ['Race'])\ncook_sample_df['Race'].value_counts(normalize=True)\n\nRace\nBlack     0.7186\nWhite     0.1518\nLatino    0.1260\nOther     0.0036\nName: proportion, dtype: float64\n\n\nThe results look good and are very close to the expected proportions, which confirms that the draw_race_sample() function is working as intended. With this validation, the function can now be used to sample a race value for each row in il_sample_df.\nThis step also introduces the tqdm library, which is useful for tracking progress when running simulations like this. It helps monitor how long the code takes per row, ensuring the simulation remains efficient.\n\nil_sample_df['Race'] = il_sample_df.progress_apply(draw_race_sample, axis=1)\n\n100%|██████████| 548/548 [00:00&lt;00:00, 8289.38it/s]\n\n\n\nsample_cols_to_keep = [\n    'county',\n    'state',\n    'Race'\n]\nil_sample_df = il_sample_df[sample_cols_to_keep].copy()\nil_sample_df\n\n\n\n\n\n\n\n\ncounty\nstate\nRace\n\n\n\n\n15\nCook\nIllinois\nBlack\n\n\n36\nHenry\nIllinois\nWhite\n\n\n72\nPerry\nIllinois\nBlack\n\n\n15\nCook\nIllinois\nBlack\n\n\n53\nLogan\nIllinois\nBlack\n\n\n...\n...\n...\n...\n\n\n51\nLee\nIllinois\nWhite\n\n\n10\nChristian\nIllinois\nBlack\n\n\n25\nFayette\nIllinois\nBlack\n\n\n44\nKane\nIllinois\nWhite\n\n\n52\nLivingston\nIllinois\nBlack\n\n\n\n\n548 rows × 3 columns\n\n\n\nLet’s take a look at the racial distribution of the Cook County subset from our sample to see how it turned out:\n\ncook_sample_df = il_sample_df[il_sample_df['county'] == \"Cook\"].copy()\ncook_sample_df['Race'].value_counts(normalize=True)\n\nRace\nBlack     0.743590\nLatino    0.166667\nWhite     0.089744\nName: proportion, dtype: float64\n\n\nThe results show a slight oversample of Latinos compared to the population expectation and an undersample of Whites. While this might seem odd, it’s actually a feature of this sampling process. The goal here is to simulate the simplified model of the Exoneration Registry, where the sample of exonerees represents a subset of 548 inmates from Cook County. This allows for a direct comparison with another size-548 subset of those still incarcerated in Cook.\nWith this step completed, the 548 rows from il_sample_df can now be combined with the 548 rows in exon_il_df, creating a balanced DataFrame with a total of 1,096 rows. Half of these rows represent exonerated individuals from Illinois, and the other half represent non-exonerated individuals, sampled to be statistically representative of Illinois’ incarcerated population as a whole."
  },
  {
    "objectID": "technical-details/data-balancing/main.html#constructing-the-final-balanced-dataset",
    "href": "technical-details/data-balancing/main.html#constructing-the-final-balanced-dataset",
    "title": "Counterfactual Data Balancing",
    "section": "Constructing the Final Balanced Dataset",
    "text": "Constructing the Final Balanced Dataset\nTo prepare the final balanced dataset, a new label column was added to distinguish between exonerated and non-exonerated individuals. Specifically:\n- The Label column in exon_il_df was set to “Exonerated”.\n- The Label column in il_sample_df was set to “Non-Exonerated”.\nTo avoid confusion when combining datasets, the county column in il_sample_df was renamed to County. With the labels in place and columns aligned, both datasets were combined into a single DataFrame using pd.concat().\nNext, a race mapping was applied to standardize the race categories across datasets:\n- “Asian” and “Native American” were combined into the “Other” category.\n- “Black,” “White,” and “Hispanic” categories were kept as-is.\nTo clean up, the race and Race columns were combined, prioritizing non-NaN values to ensure no data was lost. The original race column was then dropped. Similarly, the county and County columns were merged, and the original county column was removed to streamline the final DataFrame.\nFinally, the resulting Race and County columns were checked to confirm the expected values, and the first few rows of the balanced dataset were displayed to verify everything was in place.\n\n# Construct our new label: exonerated vs. non-exonerated\nexon_il_df['Label'] = \"Exonerated\"\nil_sample_df['Label'] = \"Non-Exonerated\"\nil_sample_df = il_sample_df.rename(columns={'county' : 'County'}) # Rename to distinguish when combining datasets\n\n# And combine!\nbalanced_df = pd.concat([exon_il_df, il_sample_df], axis=0)\n# Define the mapping for 'race'\nrace_mapping = {\n    'Asian': 'Other',\n    'Native American': 'Other',\n    'Black': 'Black',\n    'White': 'White',\n    'Hispanic': 'Hispanic'\n}\n\n\n# Map the 'race' column\nbalanced_df['race'] = balanced_df['race'].map(race_mapping)\n\n# Combine 'race' and 'Race' columns, prioritizing non-NaN values\nbalanced_df['Race'] = balanced_df['race'].combine_first(balanced_df['Race'])\n\n# Drop the old 'race' column\nbalanced_df.drop(columns=['race'], inplace=True)\n\n# Combine 'county' and 'County' columns, prioritizing non-NaN values\nbalanced_df['County'] = balanced_df['county'].combine_first(balanced_df['County'])\n\n# Drop the old 'county' column\nbalanced_df.drop(columns=['county'], inplace=True)\n\n# Verify the final Race column\nprint(balanced_df['Race'].value_counts())\nprint(balanced_df['County'].value_counts())\nbalanced_df.head()\n\nRace\nBlack     709\nWhite     207\nLatino     92\nOther       5\nName: count, dtype: int64\nCounty\nCook           552\nWill            37\nRandolph        31\nJefferson       23\nLogan           22\nPerry           22\nLivingston      22\nFulton          21\nJohnson         21\nTazewell        19\nLawrence        18\nMontgomery      17\nBond            17\nVermilion       16\nDuPage          15\nWinnebago       15\nLake            14\nSt. Clair       14\nClinton         14\nLa Salle        14\nFayette         13\nLee             13\nBrown           12\nKane            12\nKnox            11\nPeoria          10\nMorgan          10\nRock Island     10\nMacon            9\nCrawford         9\nMcHenry          7\nChristian        6\nWilliamson       6\nChampaign        5\nMcLean           4\nSangamon         4\nHenry            3\nKankakee         3\nStephenson       2\nWoodford         2\nEdgar            2\nEffingham        2\nIroquois         2\nAdams            2\nRichland         1\nMenard           1\nPope             1\nMadison          1\nBoone            1\nJackson          1\nCumberland       1\nWashington       1\nDupage           1\nDekalb           1\nMoultrie         1\nLaSalle          1\nDe Witt          1\nName: count, dtype: int64\n\n\n\n\n\n\n\n\n\nlast_name\nfirst_name\nage\nsex\nstate\nlatitude\nlongitude\nworst_crime_display\nsentence\nsentence_in_years\n...\nwitness_tampering_or_misconduct_interrogating_co_defendant\nmisconduct_in_interrogation_of_exoneree\nperjury_by_official\nprosecutor_lied_in_court\ntag_sum\ngeocode_address\nRace_orig\nLabel\nCounty\nRace\n\n\n\n\n0\nAbbott\nCinque\n19.0\nmale\nIllinois\n41.819738\n-87.756525\nDrug Possession or Sale\nProbation\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n7.0\nCook County, Illinois, United States\nBlack\nExonerated\nCook\nBlack\n\n\n1\nAbernathy\nChristopher\n17.0\nmale\nIllinois\n41.819738\n-87.756525\nMurder\nLife without parole\n100.0\n...\n0.0\n1.0\n0.0\n0.0\n10.0\nCook County, Illinois, United States\nWhite\nExonerated\nCook\nWhite\n\n\n2\nAbrego\nEruby\n20.0\nmale\nIllinois\n41.819738\n-87.756525\nMurder\n90 years\n90.0\n...\n1.0\n1.0\n1.0\n0.0\n9.0\nCook County, Illinois, United States\nHispanic\nExonerated\nCook\nNaN\n\n\n3\nAdams\nDemetris\n22.0\nmale\nIllinois\n41.819738\n-87.756525\nDrug Possession or Sale\n1 year\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n7.0\nCook County, Illinois, United States\nBlack\nExonerated\nCook\nBlack\n\n\n4\nAdams\nKenneth\n22.0\nmale\nIllinois\n41.819738\n-87.756525\nMurder\n75 years\n75.0\n...\n1.0\n0.0\n0.0\n0.0\n11.0\nCook County, Illinois, United States\nBlack\nExonerated\nCook\nBlack\n\n\n\n\n5 rows × 51 columns\n\n\n\n\nbalanced_df.to_csv(\"../../data/processed-data/exonerees_balanced.csv\", index=False)"
  },
  {
    "objectID": "technical-details/data-balancing/main.html#next-steps",
    "href": "technical-details/data-balancing/main.html#next-steps",
    "title": "Counterfactual Data Balancing",
    "section": "Next Steps",
    "text": "Next Steps\nThis balanced dataset can now be used for supervised learning tasks, such as:\n- Predicting Exoneration Factors: Training machine learning models to identify the characteristics most associated with exoneration outcomes.\n- Comparative Analysis: Exploring differences in demographics, geographic distribution, or other variables between exonerated and non-exonerated individuals.\n- Visualization and Insights: Mapping trends or disparities across counties and racial groups to better understand systemic patterns in wrongful convictions.\nWith this dataset, models and analyses can provide deeper insights into the factors driving exonerations while ensuring fairness and balance in comparisons."
  },
  {
    "objectID": "bibliography.html",
    "href": "bibliography.html",
    "title": "Bibliography",
    "section": "",
    "text": "References\n\n1. Jones-Brown, D., & Williams, J. M. (2021). Over-policing black bodies: The need for multidimensional and transformative reforms. In Journal of Ethnicity in criminal JusticE (3-4; Vol. 19, pp. 181–187). Taylor & Francis.\n\n\n2. Boehme, H. M., Cann, D., & Isom, D. A. (2022). Citizens’ perceptions of over-and under-policing: A look at race, ethnicity, and community characteristics. Crime & Delinquency, 68(1), 123–154.\n\n\n3. Gesin, J. (2014). Socioeconomic determinants of violent crime rates in the US. Empirical Economic Bulletin, An Undergraduate Journal, 7(1), 1.\n\n\n4. Cherone, H. (2023). Chicago ranks no. 1 in exonerations for 5th year in a row, accounting for more than half of national total: report. https://news.wttw.com/2023/05/16/chicago-ranks-no-1-exonerations-5th-year-row-accounting-more-half-national-total-report.\n\n\n5. University of California Irvine Newkirk Center for Science & Society, & Michigan State University College of Law, U. of M. L. S. (2024). Exoneration Detail List. In The National Registry of Exonerations. https://www.law.umich.edu/special/exoneration/Pages/detaillist.aspx\n\n\n6. University of California Irvine Newkirk Center for Science & Society, & Michigan State University College of Law, U. of M. L. S. (2024). The National Registry of Exonerations - Exoneration Registry. In The National Registry of Exonerations. https://www.law.umich.edu/special/exoneration/Pages/about.aspx\n\n\n7. University of California Irvine Newkirk Center for Science & Society, & Michigan State University College of Law, U. of M. L. S. (2024). The National Registry of Exonerations - Glossary. In The National Registry of Exonerations. https://www.law.umich.edu/special/exoneration/Pages/glossary.aspx\n\n\n8. University of California Irvine Newkirk Center for Science & Society, & Michigan State University College of Law, U. of M. L. S. (2024). The National Registry of Exonerations - Our Mission. In The National Registry of Exonerations. https://www.law.umich.edu/special/exoneration/Pages/mission.aspx\n\n\n9. Initiative, P. P. (2024). Appendix A. Counties – Ratios of Overrepresentation. https://www.prisonpolicy.org/racialgeography/counties.html\n\n\n10. Illinois Criminal Justice Information Authority. (2024). Arrests by race, county, and year. In Arrest Explorer. https://icjia.illinois.gov/arrestexplorer/\n\n\n11. Illinois Criminal Justice Information Authority. (2024). Overview  arrest explorer. In Arrest Explorer. https://icjia.illinois.gov/arrestexplorer/docs/#what-data-is-available\n\n\n12. Survey, I. S. G. (1984). Illinois county boundaries (2.0 ed.). Illinois State Geological Survey. https://clearinghouse.isgs.illinois.edu/data/reference/illinois-county-boundaries-polygons-and-lines\n\n\n13. Li, W., & Ricard, J. (2023). Many Large U.S. Police Agencies Are Missing from FBI Crime Data. In The Marshall Project. https://www.themarshallproject.org/2023/07/13/fbi-crime-rates-data-gap-nibrs\n\n\n14. Woodard, P. L., & Belair, R. R. (1993). Use and Management of Criminal History Record Information: A Comprehensive Report  Bureau of Justice Statistics [.gov]. In Bureau of Justice Statistics. https://bjs.ojp.gov/library/publications/use-and-management-criminal-history-record-information-comprehensive-report\n\n\n15. Pearl, J. (2016). Counterfactuals and their applications. In Causal inference in statistics: A primer. John Wiley & Sons. https://bayes.cs.ucla.edu/PRIMER/ch4-preview.pdf\n\n\n16. Illinois Primary Health Care Association (IPHCA). (2020). Illinois counties by rural/urban classification. https://dph.illinois.gov/content/dam/soi/en/web/idph/files/rur-urb-2021.pdf"
  },
  {
    "objectID": "technical-details/eda/main.html#exonerations-by-county",
    "href": "technical-details/eda/main.html#exonerations-by-county",
    "title": "Exploratory Data Analysis",
    "section": "Exonerations by County",
    "text": "Exonerations by County\n\nLog-Scaled Number of Exonerations by County in Illinois\n\n\nCode\n# Creates side-by-side plots\nfig, axes = plt.subplots(1, 2, figsize=(20, 10), constrained_layout=True)\n\nmerged_counties.plot(\n    column='log_num_exonerations',  # Use the log-transformed column\n    cmap='Blues',\n    linewidth=0.5,\n    edgecolor='black',\n    ax=axes[0],\n    legend=True\n)\naxes[0].set_title(\"Log-Scaled Number of Exonerations by County in Illinois (Including Cook)\", fontsize=14)\naxes[0].set_xlabel(\"Longitude\")\naxes[0].set_ylabel(\"Latitude\")\naxes[0].grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n\n# Plot the gradient map excluding Cook County, keeping linear scale\nfiltered_counties.plot(\n    column='num_exonerations',  \n    cmap='Blues',\n    linewidth=0.5,\n    edgecolor='black',\n    ax=axes[1],\n    legend=True\n)\naxes[1].set_title(\"Number of Exonerations by County (Excluding Cook)\", fontsize=14)\naxes[1].set_xlabel(\"Longitude\")\naxes[1].set_ylabel(\"Latitude\")\naxes[1].grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nThe maps reveal critical patterns in both exonerations and arrests across Illinois counties. Cook County (Chicago), as expected, dominates the data with orders of magnitude higher counts, which skews the results and makes it challenging to identify trends in smaller counties. To address this, a logarithmic transformation (np.log1p) was applied to compress the data range, ensuring that smaller counties remain visible without flattening Cook County’s impact.\nWhen Cook County is excluded, linear scaling highlights natural variations across the remaining counties. While smaller differences become more interpretable, the visualizations still emphasize the concentration of arrests and exonerations in urban regions outside Cook County. This dual scaling approach—logarithmic for overall visibility and linear for regional comparisons—strikes a balance, allowing geographic disparities across Illinois to emerge clearly.\nBy visualizing both exonerations and arrests, the maps underscore systemic trends: wrongful convictions are disproportionately concentrated in high-population areas, particularly urban hubs. At the same time, significant activity persists in smaller counties, reinforcing the need for statewide examination of these issues."
  },
  {
    "objectID": "technical-details/eda/main.html#arrests-by-county",
    "href": "technical-details/eda/main.html#arrests-by-county",
    "title": "Exploratory Data Analysis",
    "section": "Arrests by County",
    "text": "Arrests by County\n\nLog-Scaled Total Arrests by County in Illinois\n\n\nCode\n# Create side-by-side plots\nfig, axes = plt.subplots(1, 2, figsize=(20, 10), constrained_layout=True)\n\n# Plot the gradient map for total arrests by county (including Cook County with log scale)\nmerged_total_arrests.plot(\n    column='log_total_arrests',  # Use log-scaled data\n    cmap='Reds',\n    linewidth=0.5,\n    edgecolor='black',\n    ax=axes[0],\n    legend=True\n)\naxes[0].set_title(\"Log-Scaled Total Arrests by County in Illinois (Including Cook)\", fontsize=14)\naxes[0].set_xlabel(\"Longitude\")\naxes[0].set_ylabel(\"Latitude\")\naxes[0].grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n\n# Plot the gradient map for total arrests by county (excluding Cook County with linear scale)\nfiltered_total_arrests.plot(\n    column='total_arrests',  # Use linear-scaled data\n    cmap='Reds',\n    linewidth=0.5,\n    edgecolor='black',\n    ax=axes[1],\n    legend=True\n)\naxes[1].set_title(\"Total Arrests by County (Excluding Cook) in Illinois\", fontsize=14)\naxes[1].set_xlabel(\"Longitude\")\naxes[1].set_ylabel(\"Latitude\")\naxes[1].grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n\n# Show the side-by-side maps\nplt.show()\n\n\n\n\n\n\n\n\n\nThe maps highlight clear geographic disparities in arrest patterns across Illinois. Urban counties, particularly Cook County, dominate the arrest totals, even when using a logarithmic scale to compress the data. Cook County remains a significant outlier, with arrest counts orders of magnitude higher than in rural areas, driving the statewide totals.\nWhen Cook County is excluded and linear scaling is applied, the smaller counties’ arrest patterns become more visible. While some rural counties show modest variations, their arrest totals remain consistently lower than those in urban regions.\nTo provide further context, Illinois counties’ rural and urban classifications (see Appendix A) align with these trends. Rural counties—defined by lower population densities—consistently exhibit fewer arrests, while urban counties, like Cook, DuPage, and Lake, demonstrate the systemic concentration of policing and arrests in high-population areas.\n\n\nHypothesis Testing\n\nMann-Whitney U Test\nThe Mann-Whitney U Test is a non-parametric statistical test used to compare whether two independent samples come from distributions with significantly different medians. Unlike parametric tests like the t-test, it does not assume normality, making it particularly useful for skewed data—like arrest counts, which often deviate from a normal distribution. This method is ideal for comparing Cook County (urban) to all other counties (primarily rural) to identify meaningful differences in arrest patterns. It is also robust to outliers, which are common in arrest data and can heavily skew the results of parametric tests.\nThe Mann-Whitney U test compares whether the distributions of log-transformed arrests differ significantly between Cook County and other counties.\n\nNull Hypothesis (H₀):\nThe distribution of log-transformed total arrests in Cook County is not significantly different from the distribution in other counties.\nAlternative Hypothesis (H₁): The distribution of log-transformed total arrests in Cook County is significantly different from the distribution in other counties.\n\n\n\nCode\nfrom scipy.stats import mannwhitneyu\n\n# Apply log scaling to total arrests\nmerged_total_arrests['log_total_arrests'] = np.log1p(merged_total_arrests['total_arrests'])\n\n# Split data into Cook County and other counties using log-scaled arrests\ncook_arrests = merged_total_arrests[merged_total_arrests['county'] == 'Cook']['log_total_arrests']\nother_arrests = merged_total_arrests[merged_total_arrests['county'] != 'Cook']['log_total_arrests']\n\n# Mann-Whitney U Test (non-parametric test)\nu_stat, p_val = mannwhitneyu(cook_arrests, other_arrests, alternative='two-sided')\nprint(f\"Mann-Whitney U Test (Log-Scaled): U-statistic = {u_stat:.4f}, p-value = {p_val:.4f}\")\n\n\nMann-Whitney U Test (Log-Scaled): U-statistic = 101.0000, p-value = 0.0196\n\n\n\n\nHypotheses for Cohen’s d (Effect Size)\nCohen’s d quantifies the magnitude of the difference between the means of two groups, providing a standardized measure of effect size that allows for easy comparison. While the Mann-Whitney U Test determines whether there is a significant difference, Cohen’s d measures the magnitude of that difference. A non-zero Cohen’s d indicates a meaningful difference in log-transformed total arrests between Cook County and other counties. By combining a statistical test (Mann-Whitney U) with an effect size (Cohen’s d), we achieve a comprehensive understanding of both statistical significance and the practical importance of the observed differences.\n\nNull Hypothesis (H₀):\nThe effect size (Cohen’s d) is close to zero, indicating no practical difference in the mean log-transformed total arrests between Cook County and other counties.\nAlternative Hypothesis (H₁):\nThe effect size (Cohen’s d) is non-zero, suggesting a practical and meaningful difference in the mean log-transformed total arrests between Cook County and other counties.\n\n\n\nCode\n# Debugging step: Print values\nprint(\"Mean (Cook arrests):\", np.mean(cook_arrests))\nprint(\"Mean (Other arrests):\", np.mean(other_arrests))\nprint(\"Standard Deviation (Cook arrests):\", np.std(cook_arrests, ddof=1))\nprint(\"Standard Deviation (Other arrests):\", np.std(other_arrests, ddof=1))\n\n# Check for NaN or Inf in the data\nprint(\"Are there NaN values in Cook arrests?\", np.isnan(cook_arrests).any())\nprint(\"Are there NaN values in Other arrests?\", np.isnan(other_arrests).any())\nprint(\"Are there Inf values in Cook arrests?\", np.isinf(cook_arrests).any())\nprint(\"Are there Inf values in Other arrests?\", np.isinf(other_arrests).any())\n\n\nMean (Cook arrests): 14.953703751319162\nMean (Other arrests): 9.46284614509313\nStandard Deviation (Cook arrests): nan\nStandard Deviation (Other arrests): 1.3730102891385425\nAre there NaN values in Cook arrests? False\nAre there NaN values in Other arrests? False\nAre there Inf values in Cook arrests? False\nAre there Inf values in Other arrests? False\n\n\n\n\nCode\n# Check for empty data or NaN values\nif cook_arrests.empty or other_arrests.empty:\n    print(\"One of the groups is empty. Cannot calculate Cohen's d.\")\nelse:\n    # Calculate mean and standard deviation\n    mean_cook = np.mean(cook_arrests)\n    mean_other = np.mean(other_arrests)\n    std_cook = np.std(cook_arrests, ddof=1)\n    std_other = np.std(other_arrests, ddof=1)\n\n    # Check for zero variance\n    if std_cook == 0 or std_other == 0:\n        print(\"Standard deviation is zero in one of the groups. Cannot calculate Cohen's d.\")\n    else:\n        # Pooled standard deviation\n        std_pooled = np.sqrt(((len(cook_arrests) - 1) * std_cook**2 +\n                             (len(other_arrests) - 1) * std_other**2) / \n                             (len(cook_arrests) + len(other_arrests) - 2))\n        \n        # Calculate Cohen's d\n        cohens_d = (mean_cook - mean_other) / std_pooled\n        print(f\"Cohen's d (Effect Size, Log-Scaled): {cohens_d:.4f}\")\n\n\nCohen's d (Effect Size, Log-Scaled): nan"
  },
  {
    "objectID": "technical-details/eda/main.html#total-arrests-and-total-exonerations-by-race-and-county",
    "href": "technical-details/eda/main.html#total-arrests-and-total-exonerations-by-race-and-county",
    "title": "Exploratory Data Analysis",
    "section": "Total Arrests and Total Exonerations by Race and County",
    "text": "Total Arrests and Total Exonerations by Race and County\n\nmerged_total_arrests = illinois_counties.merge(\n    aggregated_data_melted, on='county', how='left'\n)\n\nmerged_total_arrests['total_arrests'] = merged_total_arrests['total_arrests'].fillna(0)\n\n# Log-transform total arrests for maps including Cook County\nmerged_total_arrests['log_total_arrests'] = np.log10(merged_total_arrests['total_arrests'] + 1)  # Add 1 to avoid log(0)\n\n# Filter out Cook County\nmerged_data_excluding_cook = merged_total_arrests[merged_total_arrests['county'] != 'Cook']\n# Define the races to plot\nraces_to_plot = [\"Black\", \"White\", \"Hispanic\", \"Asian\"]\n\n\n# Create subplots\nfig, axes = plt.subplots(2, len(races_to_plot), figsize=(20, 20), constrained_layout=True)\n\n# Plot each race's data (including Cook County with log scale) on the first row\nfor i, race in enumerate(races_to_plot):\n    race_data = merged_total_arrests[merged_total_arrests['race'] == race]\n\n    # Ensure data is not empty\n    if race_data.empty:\n        print(f\"No data available for the selected race: {race}\")\n        continue\n\n    # Plot the gradient map for the current race\n    race_data.plot(\n        column='log_total_arrests',  # Use log-transformed data\n        cmap='Reds',\n        linewidth=0.5,\n        edgecolor='black',\n        ax=axes[0, i],\n        legend=(i == len(races_to_plot) - 1)  # Show legend only on the last subplot\n    )\n    axes[0, i].set_title(f\"Log-Scaled Total Arrests (Including Cook): {race}\")\n    axes[0, i].set_xlabel(\"Longitude\")\n    axes[0, i].set_ylabel(\"Latitude\")\n    axes[0, i].grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n\n# Plot each race's data (excluding Cook County with linear scale) on the second row\nfor i, race in enumerate(races_to_plot):\n    race_data = merged_data_excluding_cook[merged_data_excluding_cook['race'] == race]\n\n    # Ensure data is not empty\n    if race_data.empty:\n        print(f\"No data available for the selected race: {race}\")\n        continue\n\n    # Plot the gradient map for the current race\n    race_data.plot(\n        column='total_arrests',  # Use linear scale for excluding Cook\n        cmap='Reds',\n        linewidth=0.5,\n        edgecolor='black',\n        ax=axes[1, i],\n        legend=(i == len(races_to_plot) - 1)  # Show legend only on the last subplot\n    )\n    axes[1, i].set_title(f\"Total Arrests (Excluding Cook): {race}\")\n    axes[1, i].set_xlabel(\"Longitude\")\n    axes[1, i].set_ylabel(\"Latitude\")\n    axes[1, i].grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n# Merge shapefile with exoneration and arrest datasets\nmerged_exonerations = illinois_counties.merge(exoneration_counts, on='county', how='left')\nmerged_exonerations['num_exonerations'] = merged_exonerations['num_exonerations'].fillna(0)\n\nmerged_arrests = illinois_counties.merge(total_arrests_by_county, on='county', how='left')\nmerged_arrests['total_arrests'] = merged_arrests['total_arrests'].fillna(0)\n\n# Filter to exclude Cook County for both datasets\nfiltered_exonerations = merged_exonerations[merged_exonerations['county'] != 'Cook']\nfiltered_arrests = merged_arrests[merged_arrests['county'] != 'Cook']\n\n# Create side-by-side plots (Excluding Cook Only)\nfig, axes = plt.subplots(1, 2, figsize=(20, 10), constrained_layout=True)\n\n# 1. Exonerations (Excluding Cook)\nfiltered_exonerations.plot(\n    column='num_exonerations',\n    cmap='Reds',\n    linewidth=0.5,\n    edgecolor='black',\n    ax=axes[0],\n    legend=True\n)\naxes[0].set_title(\"Exonerations by County (Excluding Cook)\")\naxes[0].set_xlabel(\"Longitude\")\naxes[0].set_ylabel(\"Latitude\")\naxes[0].grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n\n# 2. Arrests (Excluding Cook)\nfiltered_arrests.plot(\n    column='total_arrests',\n    cmap='Blues',\n    linewidth=0.5,\n    edgecolor='black',\n    ax=axes[1],\n    legend=True\n)\naxes[1].set_title(\"Arrests by County (Excluding Cook)\")\naxes[1].set_xlabel(\"Longitude\")\naxes[1].set_ylabel(\"Latitude\")\naxes[1].grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n\n# Show the combined plots\nplt.show()"
  },
  {
    "objectID": "technical-details/eda/main.html#title",
    "href": "technical-details/eda/main.html#title",
    "title": "Exploratory Data Analysis",
    "section": "title",
    "text": "title\n\n# Aggregate exoneration counts by race and county\nexonerees_by_race_county = exon_df.groupby(['race', 'county']).size().reset_index(name='exonerees')\n# Merge exonerees and arrests by race and county\nmerged_exoneree_arrest = exonerees_by_race_county.merge(\n    aggregated_data_melted, on=['race', 'county'], how='left'\n)\n\n# Fill NaN values in arrests with 0 (in case no arrests are recorded for a county)\nmerged_exoneree_arrest['total_arrests'] = merged_exoneree_arrest['total_arrests'].fillna(0)\n\nThe Disproportionality Index is calculated by dividing the proportion of exonerations for each race by the proportion of arrests for that race, as seen in proportionality and disparity analyses in social science and criminology research. BASED OF CSH’S DISPARITY INDEX https://www.csh.org/csh-solutions/data/#RDDI\nhttps://www.csh.org/wp-content/uploads/2020/04/RDDI_OverviewHowTo.pdf\n\n# Calculate total arrests and exonerations by race\ntotal_arrests_by_race = merged_exoneree_arrest.groupby('race')['total_arrests'].sum()\ntotal_exonerations_by_race = merged_exoneree_arrest.groupby('race')['exonerees'].sum()\n\n# Calculate percentages\narrest_percent = total_arrests_by_race / total_arrests_by_race.sum() * 100\nexoneration_percent = total_exonerations_by_race / total_exonerations_by_race.sum() * 100\n\n# Calculate disproportionality index\ndisproportionality = (exoneration_percent / arrest_percent).fillna(0)\n\n# Convert to DataFrame for plotting\ndisproportionality_df = disproportionality.reset_index()\ndisproportionality_df.columns = ['race', 'disproportionality_index']\n\n# Plot disproportionality index\nplt.figure(figsize=(10, 6))\nsns.barplot(data=disproportionality_df, x='race', y='disproportionality_index', palette='coolwarm')\nplt.title(\"Disproportionality Index: Exonerations vs Arrests by Race\")\nplt.xlabel(\"Race\")\nplt.ylabel(\"Disproportionality Index\")\nplt.axhline(1, color='gray', linestyle='--', label=\"Parity (Exonerations = Arrests)\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Calculate raw counts for context\nrace_counts = merged_exoneree_arrest.groupby('race')[['exonerees', 'total_arrests']].sum().reset_index()\n\n# Plot raw counts for comparison\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n# Plot total arrests\nsns.barplot(data=race_counts, x='race', y='total_arrests', ax=axes[0], palette='viridis')\naxes[0].set_title(\"Total Arrests by Race\")\naxes[0].set_ylabel(\"Total Arrests\")\naxes[0].set_xlabel(\"Race\")\n\n# Plot total exonerations\nsns.barplot(data=race_counts, x='race', y='exonerees', ax=axes[1], palette='viridis')\naxes[1].set_title(\"Total Exonerations by Race\")\naxes[1].set_ylabel(\"Total Exonerations\")\naxes[1].set_xlabel(\"Race\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Summarize counts by race\nrace_summary = merged_exoneree_arrest.groupby('race').agg(\n    total_arrests=('total_arrests', 'sum'),\n    total_exonerees=('exonerees', 'sum')\n).reset_index()\n\npd.set_option('display.max_columns', None)  # Ensure all columns are visible\nrace_summary\n\n\n\n\n\n\n\n\nrace\ntotal_arrests\ntotal_exonerees\n\n\n\n\n0\nAsian\n81.0\n1\n\n\n1\nBlack\n2297196.0\n418\n\n\n2\nHispanic\n168944.0\n81\n\n\n3\nNative American\n2351.0\n1\n\n\n4\nWhite\n1815269.0\n47\n\n\n\n\n\n\n\n\ntotal_arrests_sum = race_summary[\"total_arrests\"].sum()\ntotal_arrests_sum # Total arrests to determine an approrpraite threshold wel\n\n4283841.0\n\n\n\n# Set thresholds for filtering races with low data\nthreshold =  100000\nvalid_races = total_arrests_by_race[total_arrests_by_race &gt; threshold].index  # Keep races above threshold\n\n# Filter the disproportionality index to include only valid races\nfiltered_disproportionality = disproportionality[valid_races]\n\n# Convert the filtered disproportionality index to a DataFrame for plotting\nfiltered_disproportionality_df = filtered_disproportionality.reset_index()\nfiltered_disproportionality_df.columns = ['race', 'disproportionality_index']\n\n# Plot the filtered disproportionality index\nplt.figure(figsize=(10, 6))\nsns.barplot(data=filtered_disproportionality_df, x='race', y='disproportionality_index', palette='coolwarm')\nplt.title(\"Disproportionality Index: Exonerations vs Arrests by Race (Filtered)\")\nplt.xlabel(\"Race\")\nplt.ylabel(\"Disproportionality Index\")\nplt.axhline(1, color='gray', linestyle='--', label=\"Parity (Exonerations = Arrests)\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nC:\\Users\\court\\AppData\\Local\\Temp\\ipykernel_38060\\2899622546.py:14: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data=filtered_disproportionality_df, x='race', y='disproportionality_index', palette='coolwarm')\n\n\n\n\n\n\n\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n\n# Define the specific columns to visualize\ncolumns_to_plot = ['age', 'race', 'county', 'sentence_in_years']\n\n# Create histograms for the numeric or categorical distributions\nfor col in columns_to_plot:\n    if col in exon_df.columns:  # Ensure the column exists\n        plt.figure(figsize=(8, 4))\n        \n        # Check if the column is numeric or categorical\n        if pd.api.types.is_numeric_dtype(df[col]):\n            sns.histplot(df[col], kde=True, bins=30, color='blue')\n            plt.title(f'Distribution of {col}')\n            plt.xlabel(col)\n            plt.ylabel('Frequency')\n        else:\n            sns.countplot(y=df[col], order=df[col].value_counts().index, palette='viridis')\n            plt.title(f'Distribution of {col}')\n            plt.xlabel('Count')\n            plt.ylabel(col)\n        \n        plt.show()\n    else:\n        print(f\"Column '{col}' does not exist in the dataset.\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Define the columns for counts\ncolumns_to_count = ['worst_crime_display', 'sex']\n\n# Generate count plots for these columns\nfor col in columns_to_count:\n    if col in exon_df.columns:  # Ensure the column exists\n        plt.figure(figsize=(8, 4))\n        sns.countplot(y=df[col], order=df[col].value_counts().index, palette='viridis')\n        plt.title(f'Count of {col}')\n        plt.xlabel('Count')\n        plt.ylabel(col)\n        plt.show()\n    else:\n        print(f\"Column '{col}' does not exist in the dataset.\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(12, 6))\nsns.countplot(\n    data=exon_df,\n    y='worst_crime_display',\n    hue='race',\n    order=df['worst_crime_display'].value_counts().index,\n    palette='viridis'\n)\nplt.title('Worst Crime Display by Race')\nplt.xlabel('Count')\nplt.ylabel('Worst Crime Display')\nplt.legend(title='Race', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Drop rows where sentence_in_years equals 100 -&gt; life sentence and death penalty\ndf_filtered = exon_df[exon_df['sentence_in_years'] != 100]\n\n# Plot Sentence in Years by Race\nplt.figure(figsize=(12, 6))\nsns.boxplot(data=df_filtered, x='race', y='sentence_in_years', palette='viridis')\nplt.title('Sentence Length in Years by Race (Excluding Life Sentence and Death Penalty)')\nplt.xlabel('Race')\nplt.ylabel('Sentence Length (Years)')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Calculate sample size per race\nrace_counts = df_filtered['race'].value_counts()\n\n#   Z-Score normalization\ndf_filtered['normalized_sentence_length'] = (\n    df_filtered.groupby('race')['sentence_in_years']\n    .transform(lambda x: (x - x.mean()) / x.std())\n)\n\n# Plot the normalized sentence lengths\nplt.figure(figsize=(12, 6))\nsns.boxplot(data=df_filtered, x='race', y='normalized_sentence_length', palette='viridis')\nplt.title('Normalized Sentence Length in Years by Race')\nplt.xlabel('Race')\nplt.ylabel('Normalized Sentence Length')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\nC:\\Users\\court\\AppData\\Local\\Temp\\ipykernel_38060\\1186036694.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_filtered['normalized_sentence_length'] = (\n\n\n\n\n\n\n\n\n\nMisconduct EDA\n\n# Count the frequency of each tag\ntag_columns = [\n    'arson', 'bitemark', 'co_defendant_confessed', 'conviction_integrity_unit',\n    'child_sex_abuse_hysteria_case', 'child_victim', 'female_exoneree', \n    'federal_case', 'homicide', 'innocence_organization', 'jailhouse_informant', \n    'juvenile_defendant', 'misdemeanor', 'no_crime_case', 'guilty_plea_case', \n    'posthumous_exoneration', 'sexual_assault', 'shaken_baby_syndrome_case',\n    'prosecutor_misconduct', 'police_officer_misconduct', 'forensic_analyst_misconduct', \n    'child_welfare_worker_misconduct', 'withheld_exculpatory_evidence',\n    'misconduct_that_is_not_withholding_evidence', 'knowingly_permitting_perjury', \n    'witness_tampering_or_misconduct_interrogating_co_defendant',\n    'misconduct_in_interrogation_of_exoneree', 'perjury_by_official', \n    'prosecutor_lied_in_court'\n]\n\n# Count occurrences of each tag\ntag_counts = exon_df[tag_columns].sum().sort_values(ascending=False)\ntag_counts.plot(kind='bar', figsize=(12, 6), color='skyblue')\nplt.title(\"Frequency of Tags in Cases\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Tags\")\nplt.xticks(rotation=45, ha='right')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Distribution of tag_sum\nplt.figure(figsize=(8, 4))\nsns.histplot(exon_df['tag_sum'], kde=True, bins=20, color='orange')\nplt.title(\"Distribution of Total Tags Per Case\")\nplt.xlabel(\"Total Tags (tag_sum)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n# Tag sum by county\nplt.figure(figsize=(12, 6))\nsns.boxplot(data=df, x='county', y='tag_sum', palette='coolwarm')\nplt.title(\"Distribution of Total Tags by County\")\nplt.xlabel(\"County\")\nplt.ylabel(\"Total Tags\")\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Group misconduct-related tags\nmisconduct_tags = [\n    'prosecutor_misconduct', 'police_officer_misconduct', \n    'forensic_analyst_misconduct', 'child_welfare_worker_misconduct',\n    'withheld_exculpatory_evidence', 'misconduct_that_is_not_withholding_evidence',\n    'knowingly_permitting_perjury', 'witness_tampering_or_misconduct_interrogating_co_defendant',\n    'misconduct_in_interrogation_of_exoneree', 'perjury_by_official', \n    'prosecutor_lied_in_court'\n]\n\n# Create a column for combined misconduct\ndf['total_misconduct'] = exon_df[misconduct_tags].sum(axis=1)\n\n# Aggregate total misconduct by race (using sum for total counts)\nrace_misconduct = df.groupby('race')['total_misconduct'].sum().reset_index()\n\n# Bar plot of total misconduct by race\nplt.figure(figsize=(10, 6))\nsns.barplot(data=race_misconduct, x='race', y='total_misconduct', palette='viridis')\nplt.title(\"Total Misconduct Tags by Race\")\nplt.xlabel(\"Race\")\nplt.ylabel(\"Total Number of Misconduct Tags\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n# Aggregate each misconduct tag by race\nmisconduct_by_race = exon_df.groupby('race')[misconduct_tags].sum()\n\n# Heatmap of misconduct types by race with improved layout\nplt.figure(figsize=(14, 10))  # Adjust figure size for better spacing\nsns.heatmap(\n    misconduct_by_race, \n    annot=True, \n    cmap='coolwarm', \n    fmt='d', \n    linewidths=0.5, \n    cbar_kws={'label': 'Counts'},  # Add a color bar label\n    xticklabels=True, \n    yticklabels=True\n)\nplt.title(\"Breakdown of Police Misconduct Types by Race\", fontsize=16)  # Larger title font size\nplt.xlabel(\"Misconduct Type\", fontsize=12)  # Larger x-axis label font size\nplt.ylabel(\"Race\", fontsize=12)  # Larger y-axis label font size\nplt.xticks(rotation=30, ha='right', fontsize=10)  # Rotate x-axis labels for readability\nplt.yticks(fontsize=10)  # Adjust y-axis label font size\nplt.tight_layout()  # Optimize layout to avoid overlap\nplt.show()\n\n\n\n\n\n\n\n\n\n# Aggregate misconduct by county and type\nmisconduct_county = df.groupby('county')[misconduct_tags].sum()\n\n# Heatmap of misconduct types by county\nplt.figure(figsize=(14, 10))\nsns.heatmap(\n    misconduct_county,\n    annot=True,\n    cmap='viridis',\n    fmt='.0f',  # Format numbers as integers\n    linewidths=0.5,\n    cbar_kws={'label': 'Counts'}\n)\nplt.title(\"Breakdown of Misconduct Types by County\", fontsize=16)\nplt.xlabel(\"Misconduct Type\", fontsize=12)\nplt.ylabel(\"County\", fontsize=12)\nplt.xticks(rotation=30, ha='right', fontsize=10)\nplt.yticks(fontsize=10)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Filter rows where OM equals 1\ndf_misconduct = df[df['om'] == 1]\n\n# Misconduct counts by county\ncounty_misconduct = df_misconduct.groupby('county').size().sort_values(ascending=False)\n\n# Bar plot for misconduct by county\ncounty_misconduct.plot(kind='bar', figsize=(12, 6), color='coral')\nplt.title(\"Total Misconduct Cases by County (OM = 1)\")\nplt.xlabel(\"County\")\nplt.ylabel(\"Count of Misconduct Cases\")\nplt.xticks(rotation=45, ha='right')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Aggregate misconduct by county and race\nmisconduct_race_county = df.groupby(['county', 'race'])['total_misconduct'].sum().unstack()\n\n# Heatmap of misconduct by county and race\nplt.figure(figsize=(14, 10))\nsns.heatmap(\n    misconduct_race_county,\n    annot=True,  # Add cell annotations\n    cmap='viridis', \n    fmt='.0f',  # Format annotations as integers\n    linewidths=0.5,  # Add gridlines\n    cbar_kws={'label': 'Total Misconduct'}\n)\nplt.title(\"Misconduct Tags by County and Race\", fontsize=16)\nplt.xlabel(\"Race\", fontsize=12)\nplt.ylabel(\"County\", fontsize=12)\nplt.xticks(rotation=30, ha='right', fontsize=10)\nplt.yticks(fontsize=10)\nplt.tight_layout()  # Adjust layout for readability\nplt.show()"
  },
  {
    "objectID": "technical-details/eda/main.html#total-arrests-by-race-and-county",
    "href": "technical-details/eda/main.html#total-arrests-by-race-and-county",
    "title": "Exploratory Data Analysis",
    "section": "Total Arrests by Race and County",
    "text": "Total Arrests by Race and County\n\nLog-Scaled Total Arrests by Race and County\n\n\nCode\nmerged_total_arrests = illinois_counties.merge(\n    aggregated_data_melted, on='county', how='left'\n)\n\nmerged_total_arrests['total_arrests'] = merged_total_arrests['total_arrests'].fillna(0)\n\n# Log-transform total arrests for maps including Cook County\nmerged_total_arrests['log_total_arrests'] = np.log10(merged_total_arrests['total_arrests'] + 1)  # Add 1 to avoid log(0)\n\n# Filter out Cook County\nmerged_data_excluding_cook = merged_total_arrests[merged_total_arrests['county'] != 'Cook']\n\n# Define the races to plot\nraces_to_plot = [\"Black\", \"White\", \"Hispanic\", \"Asian\"]\n\n# Create subplots\nfig, axes = plt.subplots(2, len(races_to_plot), figsize=(20, 20), constrained_layout=True)\n\n# Plot each race's data (including Cook County with log scale) on the first row\nfor i, race in enumerate(races_to_plot):\n    race_data = merged_total_arrests[merged_total_arrests['race'] == race]\n\n    # Ensure data is not empty\n    if race_data.empty:\n        print(f\"No data available for the selected race: {race}\")\n        continue\n\n    # Plot the gradient map for the current race\n    race_data.plot(\n        column='log_total_arrests',  # Use log-transformed data\n        cmap='Reds',\n        linewidth=0.5,\n        edgecolor='black',\n        ax=axes[0, i],\n        legend=(i == len(races_to_plot) - 1)  # Show legend only on the last subplot\n    )\n    axes[0, i].set_title(f\"Log-Scaled Total Arrests (Including Cook): {race}\")\n    axes[0, i].set_xlabel(\"Longitude\")\n    axes[0, i].set_ylabel(\"Latitude\")\n    axes[0, i].grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n\n# Plot each race's data (excluding Cook County with linear scale) on the second row\nfor i, race in enumerate(races_to_plot):\n    race_data = merged_data_excluding_cook[merged_data_excluding_cook['race'] == race]\n\n    # Ensure data is not empty\n    if race_data.empty:\n        print(f\"No data available for the selected race: {race}\")\n        continue\n\n    # Plot the gradient map for the current race\n    race_data.plot(\n        column='total_arrests',  # Use linear scale for excluding Cook\n        cmap='Reds',\n        linewidth=0.5,\n        edgecolor='black',\n        ax=axes[1, i],\n        legend=(i == len(races_to_plot) - 1)  # Show legend only on the last subplot\n    )\n    axes[1, i].set_title(f\"Total Arrests (Excluding Cook): {race}\")\n    axes[1, i].set_xlabel(\"Longitude\")\n    axes[1, i].set_ylabel(\"Latitude\")\n    axes[1, i].grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nHypothesis Testing\n\nKruskal-Wallis\nNull Hypothesis (H₀): There is no significant difference in total arrests across racial groups. Alternative Hypothesis (H₁): There is a significant difference in total arrests across racial groups.\n\n\nCode\nfrom scipy.stats import kruskal\n\n# Group total arrests by race\nblack_arrests = merged_total_arrests[merged_total_arrests['race'] == 'Black']['total_arrests']\nwhite_arrests = merged_total_arrests[merged_total_arrests['race'] == 'White']['total_arrests']\nhispanic_arrests = merged_total_arrests[merged_total_arrests['race'] == 'Hispanic']['total_arrests']\n\n# Kruskal-Wallis Test\nh_stat, p_val = kruskal(black_arrests, white_arrests, hispanic_arrests)\nprint(f\"Kruskal-Wallis Test: H-statistic = {h_stat:.4f}, p-value = {p_val:.4f}\")\n\n\nKruskal-Wallis Test: H-statistic = 161.1528, p-value = 0.0000\n\n\n\n\nMann-Whitney U Test\nNull Hypothesis (H₀): There is no significant difference in total arrests for a racial group between Cook County and other counties. Alternative Hypothesis (H₁): There is a significant difference.\n\n\nCode\nraces = ['Black', 'White', 'Hispanic']\nfor race in races:\n    cook_arrests = merged_total_arrests[\n        (merged_total_arrests['county'] == 'Cook') & (merged_total_arrests['race'] == race)\n    ]['total_arrests']\n    other_arrests = merged_total_arrests[\n        (merged_total_arrests['county'] != 'Cook') & (merged_total_arrests['race'] == race)\n    ]['total_arrests']\n    \n    if len(cook_arrests) &gt; 0 and len(other_arrests) &gt; 0:\n        u_stat, p_val = mannwhitneyu(cook_arrests, other_arrests, alternative='two-sided')\n        print(f\"{race} Arrests - Mann-Whitney U Test: U-statistic = {u_stat:.4f}, p-value = {p_val:.4f}\")\n\n\nBlack Arrests - Mann-Whitney U Test: U-statistic = 98.0000, p-value = 0.0897\nWhite Arrests - Mann-Whitney U Test: U-statistic = 98.0000, p-value = 0.0202\nHispanic Arrests - Mann-Whitney U Test: U-statistic = 98.0000, p-value = 0.0851"
  },
  {
    "objectID": "technical-details/eda/main.html#total-exonerations-by-race",
    "href": "technical-details/eda/main.html#total-exonerations-by-race",
    "title": "Exploratory Data Analysis",
    "section": "Total Exonerations by Race",
    "text": "Total Exonerations by Race"
  },
  {
    "objectID": "technical-details/eda/main.html#log-scaled-overrepresentation-ratios-by-county",
    "href": "technical-details/eda/main.html#log-scaled-overrepresentation-ratios-by-county",
    "title": "Exploratory Data Analysis",
    "section": "Log-Scaled Overrepresentation Ratios by County",
    "text": "Log-Scaled Overrepresentation Ratios by County\nThe raw values for the overrepresentation ratios across races clearly demonstrate the need for scaling. Given the wide disparity in values, log scaling was implemented to compress large values while preserving their relative differences. To ensure consistency across maps, a global color scale was applied using vmin and vmax. This guarantees that the same color represents the same value across all maps, making comparisons more meaningful:\n\nRaw Values for Overrepresentation Ratios\nThe raw values for the overrepresentation ratios across races clearly demonstrate the need for scaling:\n\ndemographic_df = pd.read_csv('../../data/processed-data/representation_by_county.csv')\n\n# Merge demographic data with Illinois shapefile\nmerged_demographics = illinois_counties.merge(\n    demographic_df, on='county', how='left'\n)\n\n# Define the ratios to plot\nratios_to_plot = [\n    'ratio_of_overrepresentation_of_whites_incarcerated_compared_to_whites_non-incarcerated',\n    'ratio_of_overrepresentation_of_blacks_incarcerated_compared_to_blacks_non-incarcerated',\n    'ratio_of_overrepresentation_of_latinos_incarcerated_compared_to_latinos_non-incarcerated'\n]\n\n# Display min, mean, max, and std for each overrepresentation ratio\nfor ratio in ratios_to_plot:\n    stats = merged_demographics[ratio].agg(['min', 'mean', 'max', 'std'])\n    print(f\"{ratio}:\\n{stats}\\n\")\n\nratio_of_overrepresentation_of_whites_incarcerated_compared_to_whites_non-incarcerated:\nmin     0.000000\nmean    0.517474\nmax     1.070000\nstd     0.319997\nName: ratio_of_overrepresentation_of_whites_incarcerated_compared_to_whites_non-incarcerated, dtype: float64\n\nratio_of_overrepresentation_of_blacks_incarcerated_compared_to_blacks_non-incarcerated:\nmin       0.000000\nmean     40.916211\nmax     418.720000\nstd      78.475567\nName: ratio_of_overrepresentation_of_blacks_incarcerated_compared_to_blacks_non-incarcerated, dtype: float64\n\nratio_of_overrepresentation_of_latinos_incarcerated_compared_to_latinos_non-incarcerated:\nmin      0.000000\nmean     6.393895\nmax     96.150000\nstd     12.830208\nName: ratio_of_overrepresentation_of_latinos_incarcerated_compared_to_latinos_non-incarcerated, dtype: float64\n\n\n\nEvidently, the scales are wildly disproportionate:\n\nThe mean for Blacks (~41) is orders of magnitude higher than for Whites (~0.5) or Latinos (~6).\nThe standard deviation (std) further emphasizes the disparity, particularly for Black incarceration rates.\n\nEven with log scaling, the imbalance remains striking but becomes much easier to visualize and compare.\n\n# Titles for the plots\ntitles = [\n    \"Overrepresentation of White Population Incarcerated\",\n    \"Overrepresentation of Black Population Incarcerated\",\n    \"Overrepresentation of Latino Population Incarcerated\"\n]\n\n# Apply log scaling to ratios and store in new columns\nfor ratio in ratios_to_plot:\n    merged_demographics[f'log_{ratio}'] = np.log1p(merged_demographics[ratio])  # log1p avoids log(0)\n\n# Combine all log-scaled ratios to get global color scale\nall_log_ratios = pd.concat([\n    merged_demographics[f'log_{ratios_to_plot[0]}'],\n    merged_demographics[f'log_{ratios_to_plot[1]}'],\n    merged_demographics[f'log_{ratios_to_plot[2]}']\n], axis=0)\n\n# Get global color scale for log values\nvmin, vmax = all_log_ratios.min(), all_log_ratios.max()\n\n# Create subplots\nfig, axes = plt.subplots(1, 3, figsize=(20, 8), constrained_layout=True)\n\n# Plot each log-scaled ratio\nfor i, ratio in enumerate(ratios_to_plot):\n    # BASE LAYER: Plot county boundaries\n    illinois_counties.boundary.plot(\n        ax=axes[i],\n        linewidth=0.5,\n        color=\"black\"\n    )\n\n    # OVERLAY: Plot the log-transformed ratio data\n    merged_demographics.plot(\n        column=f'log_{ratio}',\n        cmap='Purples',\n        linewidth=0.5,\n        edgecolor='black',\n        ax=axes[i],\n        legend=True,\n        vmin=vmin,  # Global log scale min\n        vmax=vmax   # Global log scale max\n    )\n    \n    # Centered titles\n    axes[i].set_title(titles[i], fontsize=12, loc='center', pad=20, x=0.4)\n\n    axes[i].set_axis_off()  # Hide axes for clean appearance\n\nplt.suptitle(\"\\n Log-Scaled Overrepresentation Ratios by County \\n\", fontsize=16)\n\nplt.show()"
  },
  {
    "objectID": "technical-details/eda/main.html#hypothesis-testing",
    "href": "technical-details/eda/main.html#hypothesis-testing",
    "title": "Exploratory Data Analysis",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nMann-Whitney U Test\nThe Mann-Whitney U test compares whether the distributions of log-transformed arrests differ significantly between Cook County and other counties.\n\nNull Hypothesis (H₀):\nThe distribution of log-transformed total arrests in Cook County is not significantly different from the distribution in other counties.\nAlternative Hypothesis (H₁):\n\n\n\nHypotheses for Cohen’s d (Effect Size)\nCohen’s d measures the magnitude of the difference between the means of the two groups.\n\nNull Hypothesis (H₀):\nThe effect size (Cohen’s d) is close to zero, indicating no practical difference in the mean log-transformed total arrests between Cook County and other counties.\nAlternative Hypothesis (H₁):\nThe effect size (Cohen’s d) is non-zero, suggesting a practical and meaningful difference in the mean log-transformed total arrests between Cook County and other counties.\n\n\nfrom scipy.stats import mannwhitneyu\nimport numpy as np\n\n# Apply log scaling to total arrests\nmerged_total_arrests['log_total_arrests'] = np.log1p(merged_total_arrests['total_arrests'])\n\n# Split data into Cook County and other counties using log-scaled arrests\ncook_arrests = merged_total_arrests[merged_total_arrests['county'] == 'Cook']['log_total_arrests']\nother_arrests = merged_total_arrests[merged_total_arrests['county'] != 'Cook']['log_total_arrests']\n\n# Mann-Whitney U Test (non-parametric test)\nu_stat, p_val = mannwhitneyu(cook_arrests, other_arrests, alternative='two-sided')\nprint(f\"Mann-Whitney U Test (Log-Scaled): U-statistic = {u_stat:.4f}, p-value = {p_val:.4f}\")\n\n# Calculate Cohen's d for effect size using log-scaled data\nmean_cook = np.mean(cook_arrests)\nmean_other = np.mean(other_arrests)\nstd_pooled = np.sqrt(((len(cook_arrests) - 1) * np.std(cook_arrests, ddof=1)**2 +\n                     (len(other_arrests) - 1) * np.std(other_arrests, ddof=1)**2) / \n                     (len(cook_arrests) + len(other_arrests) - 2))\n\ncohens_d = (mean_cook - mean_other) / std_pooled\nprint(f\"Cohen's d (Effect Size, Log-Scaled): {cohens_d:.4f}\")\n\nMann-Whitney U Test (Log-Scaled): U-statistic = 2300.0000, p-value = 0.0007\nCohen's d (Effect Size, Log-Scaled): 1.8953"
  },
  {
    "objectID": "technical-details/eda/main.html#hypothesis-testing-1",
    "href": "technical-details/eda/main.html#hypothesis-testing-1",
    "title": "Exploratory Data Analysis",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nKruskal-Wallis\nNull Hypothesis (H₀): There is no significant difference in total arrests across racial groups. Alternative Hypothesis (H₁): There is a significant difference in total arrests across racial groups.\n\nfrom scipy.stats import kruskal\n\n# Group total arrests by race\nblack_arrests = merged_total_arrests[merged_total_arrests['race'] == 'Black']['total_arrests']\nwhite_arrests = merged_total_arrests[merged_total_arrests['race'] == 'White']['total_arrests']\nhispanic_arrests = merged_total_arrests[merged_total_arrests['race'] == 'Hispanic']['total_arrests']\n\n# Kruskal-Wallis Test\nh_stat, p_val = kruskal(black_arrests, white_arrests, hispanic_arrests)\nprint(f\"Kruskal-Wallis Test: H-statistic = {h_stat:.4f}, p-value = {p_val:.4f}\")\n\nKruskal-Wallis Test: H-statistic = 161.1528, p-value = 0.0000\n\n\n\n\nMann-Whitney U Test\nNull Hypothesis (H₀): There is no significant difference in total arrests for a racial group between Cook County and other counties. Alternative Hypothesis (H₁): There is a significant difference.\n\nfrom scipy.stats import mannwhitneyu\n\n# Loop through each race and perform Mann-Whitney U test\nraces = ['Black', 'White', 'Hispanic']\nfor race in races:\n    cook_arrests = merged_total_arrests[\n        (merged_total_arrests['county'] == 'Cook') & (merged_total_arrests['race'] == race)\n    ]['total_arrests']\n    other_arrests = merged_total_arrests[\n        (merged_total_arrests['county'] != 'Cook') & (merged_total_arrests['race'] == race)\n    ]['total_arrests']\n    \n    if len(cook_arrests) &gt; 0 and len(other_arrests) &gt; 0:\n        u_stat, p_val = mannwhitneyu(cook_arrests, other_arrests, alternative='two-sided')\n        print(f\"{race} Arrests - Mann-Whitney U Test: U-statistic = {u_stat:.4f}, p-value = {p_val:.4f}\")\n\nBlack Arrests - Mann-Whitney U Test: U-statistic = 98.0000, p-value = 0.0897\nWhite Arrests - Mann-Whitney U Test: U-statistic = 98.0000, p-value = 0.0202\nHispanic Arrests - Mann-Whitney U Test: U-statistic = 98.0000, p-value = 0.0851"
  },
  {
    "objectID": "technical-details/eda/main.html#hypothesis-testing-2",
    "href": "technical-details/eda/main.html#hypothesis-testing-2",
    "title": "Exploratory Data Analysis",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nKruskal-Wallis Test:\nNull Hypothesis (H₀): The log-scaled ratios have no significant differences across the three racial groups. If p-value &lt; 0.05, reject H₀ and conclude there are differences.\n\n#Kruskal - Wallis\n# Extract log-scaled ratios for each group\nwhite_ratios = merged_demographics[f'log_{ratios_to_plot[0]}'].dropna()\nblack_ratios = merged_demographics[f'log_{ratios_to_plot[1]}'].dropna()\nlatino_ratios = merged_demographics[f'log_{ratios_to_plot[2]}'].dropna()\n\n# Kruskal-Wallis Test to check for overall differences\nh_stat, p_val = kruskal(white_ratios, black_ratios, latino_ratios)\nprint(f\"Kruskal-Wallis Test: H-statistic = {h_stat:.4f}, p-value = {p_val:.4f}\")\n\nKruskal-Wallis Test: H-statistic = 62.1876, p-value = 0.0000\n\n\n\n\nPairwise Mann-Whitney U Tests:\nCompares each pair (White vs. Black, White vs. Latino, Black vs. Latino) to pinpoint where the differences lie. p-values &lt; 0.05 indicate significant differences between the pair\n\n# Pairwise Mann-Whitney U Tests\nif p_val &lt; 0.05:  # Only run if Kruskal-Wallis indicates significance\n    print(\"\\nPerforming pairwise Mann-Whitney U Tests:\")\n    pairs = [('White', white_ratios), ('Black', black_ratios), ('Latino', latino_ratios)]\n    \n    for i in range(len(pairs)):\n        for j in range(i + 1, len(pairs)):\n            group1_name, group1 = pairs[i]\n            group2_name, group2 = pairs[j]\n            u_stat, p_val_pair = mannwhitneyu(group1, group2, alternative='two-sided')\n            print(f\"{group1_name} vs {group2_name}: U-statistic = {u_stat:.4f}, p-value = {p_val_pair:.4f}\")\n\n\nPerforming pairwise Mann-Whitney U Tests:\nWhite vs Black: U-statistic = 1848.0000, p-value = 0.0000\nWhite vs Latino: U-statistic = 2591.5000, p-value = 0.0000\nBlack vs Latino: U-statistic = 6162.5000, p-value = 0.0000"
  },
  {
    "objectID": "technical-details/eda/main.html#geospatial-workflow",
    "href": "technical-details/eda/main.html#geospatial-workflow",
    "title": "Exploratory Data Analysis",
    "section": "Geospatial Workflow",
    "text": "Geospatial Workflow\nThe following analyses involved a sequence of steps to prepare and visualize the exoneration and arrest data:\n\nGeometry Creation\nUsing shapely.geometry, latitude and longitude values were converted into geometric points to map exoneration cases spatially. This allowed for the dataset to be transformed into a GeoDataFrame using GeoPandas.\n\n\nCode\n# Create geometry column for points based on latitude and longitude\ngeometry = [Point(xy) for xy in zip(exon_df['longitude'], exon_df['latitude'])]\ngeo_df = gpd.GeoDataFrame(exon_df, geometry=geometry)\n\n\n\n\nLoading Shapefiles\nThe Illinois county boundary shapefile was loaded using GeoPandas (gpd.read_file). This file provided the geographic outlines needed to visualize data at the county level.\n\n\nCode\n# Load Illinois county shapefile \nillinois_counties = gpd.read_file('../../data/geospatial/IL_BNDY_COUNTY_Py.shp')\n\n\n\n\nAggregating Exoneration Data\nExoneration counts were grouped by county and race to produce a summarized table of exonerations (groupby in pandas). This data was then merged with the Illinois shapefile to align geographic boundaries with exoneration counts.\n\n\nCode\n# Aggregate exoneration counts by county and race\nexoneration_counts = exon_df.groupby(['county', 'race'], as_index=False).size()\nexoneration_counts.rename(columns={'size': 'num_exonerations'}, inplace=True)\n\nillinois_counties['county'] = illinois_counties['COUNTY_NAM'].str.title()  # Match casing\n\n# Merge exoneration data with shapefile\nmerged_counties = illinois_counties.merge(exoneration_counts, on='county', how='left')\n\n\n\n\nAggregating Arrest Data\nArrest counts were grouped by county and race to produce a summarized table of arrests (groupby in pandas). This data was then merged with the Illinois shapefile to align geographic boundaries with arrest totals.\n\n\nCode\n# Load aggregated arrests data\naggregated_data = pd.read_csv('../../data/processed-data/aggregated_arrests_2001_to_2021.csv')\naggregated_data_melted = aggregated_data.melt(id_vars='race', var_name='county', value_name='total_arrests')\n\n# Aggregate total arrests by county for all races\ntotal_arrests_by_county = aggregated_data_melted.groupby('county')['total_arrests'].sum().reset_index()\n\n# Ensure county names are consistent\ntotal_arrests_by_county['county'] = total_arrests_by_county['county'].str.strip().str.title()\n\n# Merge total arrests data with the shapefile\nmerged_total_arrests = illinois_counties.merge(total_arrests_by_county, on='county', how='left')\n\n\n\n\nHandling Missing Data\nCounties without recorded exonerations or arrests were assigned a count of 0 to ensure completeness in the visualizations.\n\n\nCode\n# Replace NaN values with 0 for counties without exonerations\nmerged_counties['num_exonerations'] = merged_counties['num_exonerations'].fillna(0)\n\n# Fill NaN values with 0 for counties without data\nmerged_total_arrests['total_arrests'] = merged_total_arrests['total_arrests'].fillna(0)\n\n\n\n\nLogarithmic Transformation\nThe geospatial analysis visualizes exoneration and arrest patterns across Illinois counties. Cook County’s numbers are orders of magnitude higher than those in other counties, which skews the results and makes it difficult to identify trends in smaller counties on a linear scale. To address this, two scaling approaches were used:\n\nLogarithmic Scaling (Including Cook County): Log transformations (np.log1p) compress the data range, making patterns in smaller counties clearer while still preserving Cook County’s contribution.\n\nLinear Scaling (Excluding Cook County): Removing Cook County allows linear scaling to highlight smaller variations across the remaining counties, ensuring their natural distribution is preserved.\n\nWith log scaling, patterns across smaller counties become visible without Cook County completely dominating the visualization. Even on a linear scale, exonerations and arrests remain heavily concentrated in urban areas like Cook County and its surrounding regions.\nThis dual approach highlights the systemic concentration of wrongful convictions and arrests in high-population areas while ensuring patterns in smaller counties are not obscured. By combining log scaling and linear scaling, the visualizations strike a balance between emphasizing Cook County’s impact and uncovering broader geographic trends.\n\n\nCode\n# Exonerations Log transformation to handle the exponential scale of Cook County data\nmerged_counties['log_num_exonerations'] = np.log1p(merged_counties['num_exonerations'])\n\n# Arrests Log transformation to handle the exponential scale of Cook County data\nmerged_total_arrests['log_total_arrests'] = np.log10(merged_total_arrests['total_arrests'] + 1)  # Add 1 to avoid log(0)\n\n# Filter out Cook County \nfiltered_counties = merged_counties[merged_counties['county'] != 'Cook']\nfiltered_total_arrests = merged_total_arrests[merged_total_arrests['county'] != 'Cook']"
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Apendix",
    "section": "",
    "text": "Appendix A: Illinois Rural/Urban Classification Map\nThe following map provides the rural and urban classification for counties in Illinois. This classification is referenced in the Geographic Analysis section for contextual understanding of urban vs rural dynamics.1\n\n\n\nIllinois Counties by Rural/Urban Classification\n\n\n\n\n\n\n\nReferences\n\n1. Illinois Primary Health Care Association (IPHCA). (2020). Illinois counties by rural/urban classification. https://dph.illinois.gov/content/dam/soi/en/web/idph/files/rur-urb-2021.pdf"
  },
  {
    "objectID": "technical-details/eda/main.html#overrepresentation-by-county",
    "href": "technical-details/eda/main.html#overrepresentation-by-county",
    "title": "Exploratory Data Analysis",
    "section": "Overrepresentation by County",
    "text": "Overrepresentation by County\n\nLog-Scaled Overrepresentation Ratios by County\nThe raw values for the overrepresentation ratios across races clearly demonstrate the need for scaling. Given the wide disparity in values, log scaling was implemented to compress large values while preserving their relative differences. To ensure consistency across maps, a global color scale was applied using vmin and vmax. This guarantees that the same color represents the same value across all maps, making comparisons more meaningful:\n\nRaw Values for Overrepresentation Ratios\nThe raw values for the overrepresentation ratios across races clearly demonstrate the need for scaling:\n\n\nCode\ndemographic_df = pd.read_csv('../../data/processed-data/representation_by_county.csv')\n\n# Merge demographic data with Illinois shapefile\nmerged_demographics = illinois_counties.merge(\n    demographic_df, on='county', how='left'\n)\n\n# Define the ratios to plot\nratios_to_plot = [\n    'ratio_of_overrepresentation_of_whites_incarcerated_compared_to_whites_non-incarcerated',\n    'ratio_of_overrepresentation_of_blacks_incarcerated_compared_to_blacks_non-incarcerated',\n    'ratio_of_overrepresentation_of_latinos_incarcerated_compared_to_latinos_non-incarcerated'\n]\n\n# Display min, mean, max, and std for each overrepresentation ratio\nfor ratio in ratios_to_plot:\n    stats = merged_demographics[ratio].agg(['min', 'mean', 'max', 'std'])\n    print(f\"{ratio}:\\n{stats}\\n\")\n\n\nratio_of_overrepresentation_of_whites_incarcerated_compared_to_whites_non-incarcerated:\nmin     0.000000\nmean    0.517474\nmax     1.070000\nstd     0.319997\nName: ratio_of_overrepresentation_of_whites_incarcerated_compared_to_whites_non-incarcerated, dtype: float64\n\nratio_of_overrepresentation_of_blacks_incarcerated_compared_to_blacks_non-incarcerated:\nmin       0.000000\nmean     40.916211\nmax     418.720000\nstd      78.475567\nName: ratio_of_overrepresentation_of_blacks_incarcerated_compared_to_blacks_non-incarcerated, dtype: float64\n\nratio_of_overrepresentation_of_latinos_incarcerated_compared_to_latinos_non-incarcerated:\nmin      0.000000\nmean     6.393895\nmax     96.150000\nstd     12.830208\nName: ratio_of_overrepresentation_of_latinos_incarcerated_compared_to_latinos_non-incarcerated, dtype: float64\n\n\n\nEvidently, the scales are wildly disproportionate:\n\nThe mean for Blacks (~41) is orders of magnitude higher than for Whites (~0.5) or Latinos (~6).\nThe standard deviation (std) further emphasizes the disparity, particularly for Black incarceration rates.\n\nEven with log scaling, the imbalance remains striking but becomes much easier to visualize and compare.\n\n\nCode\n# Titles for the plots\ntitles = [\n    \"Overrepresentation of White Population Incarcerated\",\n    \"Overrepresentation of Black Population Incarcerated\",\n    \"Overrepresentation of Latino Population Incarcerated\"\n]\n\n# Apply log scaling to ratios and store in new columns\nfor ratio in ratios_to_plot:\n    merged_demographics[f'log_{ratio}'] = np.log1p(merged_demographics[ratio])  # log1p avoids log(0)\n\n# Combine all log-scaled ratios to get global color scale\nall_log_ratios = pd.concat([\n    merged_demographics[f'log_{ratios_to_plot[0]}'],\n    merged_demographics[f'log_{ratios_to_plot[1]}'],\n    merged_demographics[f'log_{ratios_to_plot[2]}']\n], axis=0)\n\n# Get global color scale for log values\nvmin, vmax = all_log_ratios.min(), all_log_ratios.max()\n\n# Create subplots\nfig, axes = plt.subplots(1, 3, figsize=(20, 8), constrained_layout=True)\n\n# Plot each log-scaled ratio\nfor i, ratio in enumerate(ratios_to_plot):\n    # BASE LAYER: Plot county boundaries\n    illinois_counties.boundary.plot(\n        ax=axes[i],\n        linewidth=0.5,\n        color=\"black\"\n    )\n\n    # OVERLAY: Plot the log-transformed ratio data\n    merged_demographics.plot(\n        column=f'log_{ratio}',\n        cmap='Purples',\n        linewidth=0.5,\n        edgecolor='black',\n        ax=axes[i],\n        legend=True,\n        vmin=vmin,  # Global log scale min\n        vmax=vmax   # Global log scale max\n    )\n    \n    # Centered titles\n    axes[i].set_title(titles[i], fontsize=12, loc='center', pad=20, x=0.4)\n\n    axes[i].set_axis_off()  # Hide axes for clean appearance\n\nplt.suptitle(\"\\n Log-Scaled Overrepresentation Ratios by County \\n\", fontsize=16)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesis Testing\n\nKruskal-Wallis Test:\nNull Hypothesis (H₀): The log-scaled ratios have no significant differences across the three racial groups. If p-value &lt; 0.05, reject H₀ and conclude there are differences.\n\n\nCode\n#Kruskal - Wallis\n# Extract log-scaled ratios for each group\nwhite_ratios = merged_demographics[f'log_{ratios_to_plot[0]}'].dropna()\nblack_ratios = merged_demographics[f'log_{ratios_to_plot[1]}'].dropna()\nlatino_ratios = merged_demographics[f'log_{ratios_to_plot[2]}'].dropna()\n\n# Kruskal-Wallis Test to check for overall differences\nh_stat, p_val = kruskal(white_ratios, black_ratios, latino_ratios)\nprint(f\"Kruskal-Wallis Test: H-statistic = {h_stat:.4f}, p-value = {p_val:.4f}\")\n\n\nKruskal-Wallis Test: H-statistic = 62.1876, p-value = 0.0000\n\n\n\n\nPairwise Mann-Whitney U Tests:\nCompares each pair (White vs. Black, White vs. Latino, Black vs. Latino) to pinpoint where the differences lie. p-values &lt; 0.05 indicate significant differences between the pair\n\n\nCode\n# Pairwise Mann-Whitney U Tests\nif p_val &lt; 0.05:  # Only run if Kruskal-Wallis indicates significance\n    print(\"\\nPerforming pairwise Mann-Whitney U Tests:\")\n    pairs = [('White', white_ratios), ('Black', black_ratios), ('Latino', latino_ratios)]\n    \n    for i in range(len(pairs)):\n        for j in range(i + 1, len(pairs)):\n            group1_name, group1 = pairs[i]\n            group2_name, group2 = pairs[j]\n            u_stat, p_val_pair = mannwhitneyu(group1, group2, alternative='two-sided')\n            print(f\"{group1_name} vs {group2_name}: U-statistic = {u_stat:.4f}, p-value = {p_val_pair:.4f}\")\n\n\n\nPerforming pairwise Mann-Whitney U Tests:\nWhite vs Black: U-statistic = 1848.0000, p-value = 0.0000\nWhite vs Latino: U-statistic = 2591.5000, p-value = 0.0000\nBlack vs Latino: U-statistic = 6162.5000, p-value = 0.0000"
  }
]