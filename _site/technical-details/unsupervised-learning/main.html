<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.555">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>DSAN-5000: Project - Unsupervised Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../assets/gu-logo.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">DSAN-5000: Project</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../report/report.html"> 
<span class="menu-text">Report</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-technical-details" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Technical details</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-technical-details">    
        <li>
    <a class="dropdown-item" href="../../technical-details/data-collection/main.html">
 <span class="dropdown-text">Data-collection</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../technical-details/data-cleaning/main.html">
 <span class="dropdown-text">Data-cleaning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../technical-details/data-balancing/main.html">
 <span class="dropdown-text">Counterfactual Data Balancing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../technical-details/eda/main.html">
 <span class="dropdown-text">Exploratory Data Analysis</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../technical-details/unsupervised-learning/main.html">
 <span class="dropdown-text">Unsupervised Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../technical-details/supervised-learning/main.html">
 <span class="dropdown-text">Supervised Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../technical-details/progress-log.html">
 <span class="dropdown-text">Progress Log</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../technical-details/llm-usage-log.html">
 <span class="dropdown-text">LLM usage Log</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../bibliography.html"> 
<span class="menu-text">Bibliography</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../appendix.html"> 
<span class="menu-text">Appendix</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/dsan-5000/project-courtneyrgreen.git"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://crg123.georgetown.domains/"> 
<span class="menu-text">Back to Portfolio</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-and-motivation" id="toc-introduction-and-motivation" class="nav-link active" data-scroll-target="#introduction-and-motivation">Introduction and Motivation</a></li>
  <li><a href="#data-preprocessing" id="toc-data-preprocessing" class="nav-link" data-scroll-target="#data-preprocessing">Data Preprocessing</a></li>
  <li><a href="#dimensionality-reduction" id="toc-dimensionality-reduction" class="nav-link" data-scroll-target="#dimensionality-reduction">Dimensionality Reduction</a>
  <ul class="collapse">
  <li><a href="#pca-principal-component-analysis" id="toc-pca-principal-component-analysis" class="nav-link" data-scroll-target="#pca-principal-component-analysis">PCA (Principal Component Analysis)</a>
  <ul class="collapse">
  <li><a href="#explained-variance-ratio" id="toc-explained-variance-ratio" class="nav-link" data-scroll-target="#explained-variance-ratio">Explained Variance Ratio</a></li>
  </ul></li>
  <li><a href="#t-sne-t-distributed-stochastic-neighbor-embedding" id="toc-t-sne-t-distributed-stochastic-neighbor-embedding" class="nav-link" data-scroll-target="#t-sne-t-distributed-stochastic-neighbor-embedding">t-SNE (t-distributed Stochastic Neighbor Embedding)</a>
  <ul class="collapse">
  <li><a href="#comparison-of-pca-and-t-sne-results" id="toc-comparison-of-pca-and-t-sne-results" class="nav-link" data-scroll-target="#comparison-of-pca-and-t-sne-results">Comparison of PCA and t-SNE Results</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#clustering-methods" id="toc-clustering-methods" class="nav-link" data-scroll-target="#clustering-methods">Clustering Methods</a>
  <ul class="collapse">
  <li><a href="#k-means" id="toc-k-means" class="nav-link" data-scroll-target="#k-means">K-Means</a>
  <ul class="collapse">
  <li><a href="#elbow-method" id="toc-elbow-method" class="nav-link" data-scroll-target="#elbow-method">Elbow Method</a></li>
  <li><a href="#visualize-clusters-for-optimal-k" id="toc-visualize-clusters-for-optimal-k" class="nav-link" data-scroll-target="#visualize-clusters-for-optimal-k">Visualize Clusters for Optimal K</a></li>
  </ul></li>
  <li><a href="#dbscan-density-based-spatial-clustering-of-applications-with-noise" id="toc-dbscan-density-based-spatial-clustering-of-applications-with-noise" class="nav-link" data-scroll-target="#dbscan-density-based-spatial-clustering-of-applications-with-noise">DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</a>
  <ul class="collapse">
  <li><a href="#eps-hyper-parameter-tuning" id="toc-eps-hyper-parameter-tuning" class="nav-link" data-scroll-target="#eps-hyper-parameter-tuning">Eps Hyper-Parameter Tuning</a></li>
  </ul></li>
  <li><a href="#hierarchical-clustering" id="toc-hierarchical-clustering" class="nav-link" data-scroll-target="#hierarchical-clustering">Hierarchical Clustering</a>
  <ul class="collapse">
  <li><a href="#hierarchical-clustering-results-interpretation" id="toc-hierarchical-clustering-results-interpretation" class="nav-link" data-scroll-target="#hierarchical-clustering-results-interpretation">Hierarchical Clustering Results Interpretation</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Unsupervised Learning</h1>
</div>



<div class="quarto-title-meta column-page-left">

    
  
    
  </div>
  


</header>


<section id="introduction-and-motivation" class="level1">
<h1>Introduction and Motivation</h1>
<p>This analysis employs <strong>unsupervised learning</strong> techniques—including <strong>Principal Component Analysis (PCA)</strong>, <strong>t-Distributed Stochastic Neighbor Embedding (t-SNE)</strong>, <strong>K-Means</strong>, <strong>DBSCAN</strong>, and <strong>Hierarchical Clustering</strong>—to examine the <strong>Illinois exoneration dataset</strong>. The primary objective is to identify patterns and hidden structures within the data, particularly focusing on how case characteristics, demographic variables (such as race and county), and the number of years lost to wrongful convictions intersect.</p>
<p>The analysis is structured as follows:<br>
1. <strong>Dimensionality Reduction</strong>: Methods such as PCA and t-SNE are utilized to project high-dimensional data into lower-dimensional spaces, simplifying the visualization of complex relationships while preserving key structural and variance-based insights.</p>
<ol start="2" type="1">
<li><p><strong>Clustering</strong>: Clustering techniques—<strong>K-Means</strong>, <strong>DBSCAN</strong>, and <strong>Hierarchical Clustering</strong>—are applied to uncover natural groupings within the dataset and assess whether these clusters align with demographic features like race or case-related factors.</p></li>
<li><p><strong>Evaluation and Interpretation</strong>: The performance of each method is evaluated, and clustering results are compared to draw meaningful interpretations. Visualizations are integrated throughout the analysis to enhance clarity and support findings.</p></li>
</ol>
<p>The motivation for this analysis stems from the critical need to uncover systemic patterns in wrongful conviction data. By applying unsupervised learning methods, the investigation aims to reveal relationships and disparities between demographic factors and case outcomes that are not immediately apparent. These insights contribute to a deeper understanding of biases and inequities within exoneration cases and support broader efforts for justice system reform.</p>
</section>
<section id="data-preprocessing" class="level1">
<h1>Data Preprocessing</h1>
<p>The data preprocessing stage prepares the <strong>Illinois exoneration dataset</strong> for dimensionality reduction and clustering. This step involves selecting relevant features, encoding categorical variables, and standardizing numerical features to ensure compatibility with unsupervised learning algorithms.</p>
<section id="feature-selection" class="level4">
<h4 class="anchored" data-anchor-id="feature-selection">Feature Selection</h4>
<p>The features chosen for this analysis include a combination of <strong>numerical</strong> and <strong>categorical</strong> variables. The numerical variables—<strong>age</strong>, <strong>sentence in years</strong>, and <strong>years lost</strong>—were selected to provide quantitative insights into exoneration cases, such as the age at conviction, the length of imprisonment, and the total number of years lost. The categorical variable <strong>race</strong> was included to capture demographic patterns and be used as a <strong>color vector</strong> during visualization to assess how the identified clusters align with racial groupings.</p>
<p>The selected features are as follows:<br>
- <strong>Numerical</strong>: <code>age</code>, <code>sentence_in_years</code>, <code>years_lost</code><br>
- <strong>Categorical</strong>: <code>race</code></p>
</section>
<section id="standardization-and-encoding" class="level4">
<h4 class="anchored" data-anchor-id="standardization-and-encoding">Standardization and Encoding</h4>
<p>To ensure that numerical features contribute equally to the analysis, they were standardized using <strong>StandardScaler</strong>. Standardization adjusts each numerical feature to have a mean of 0 and a standard deviation of 1, preventing variables like sentence lengths from dominating the clustering process. The categorical variable <strong>race</strong> was encoded using <strong>Label Encoding</strong>, which assigns a numerical value to each category (e.g., race). This transformation ensures compatibility with algorithms such as <strong>K-Means</strong>, <strong>DBSCAN</strong>, and <strong>Hierarchical Clustering</strong> that require numerical inputs for processing.</p>
<div id="cell-3" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler, LabelEncoder</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'../../data/processed-data/illinois_exoneration_data.csv'</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Select features of interest</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">'age'</span>, <span class="st">'sentence_in_years'</span>, <span class="st">'years_lost'</span>, <span class="st">'race'</span>, <span class="st">'county'</span>]</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df[features].dropna()</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Encode categorical variables</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>le_race <span class="op">=</span> LabelEncoder()</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>le_county <span class="op">=</span> LabelEncoder()</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'race_encoded'</span>] <span class="op">=</span> le_race.fit_transform(df[<span class="st">'race'</span>])</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardize numerical variables</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>scaled_features <span class="op">=</span> scaler.fit_transform(df[[<span class="st">'age'</span>, <span class="st">'sentence_in_years'</span>, <span class="st">'years_lost'</span>]])</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> scaled_features  <span class="co"># Already standardized</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="dimensionality-reduction" class="level1">
<h1>Dimensionality Reduction</h1>
<p>The objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.</p>
<section id="pca-principal-component-analysis" class="level2">
<h2 class="anchored" data-anchor-id="pca-principal-component-analysis">PCA (Principal Component Analysis)</h2>
<p><strong>Principal Component Analysis (PCA)</strong> is a statistical technique used to reduce the dimensionality of high-dimensional datasets. It achieves this by identifying the most significant features, known as principal components, through linear transformations. These components capture the maximum variance in the data, allowing for a simplified yet informative representation of complex datasets.</p>
<div id="cell-5" class="cell" data-execution_count="102">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------------------------------------------</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Unsupervised Learning Implementation</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Principal Component Analysis (PCA) and clustering methods used</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># in this code are based on demos and labs provided by:</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Hickman, J. (2024.). Principal Components Analysis and clustering.</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Georgetown University Centralized Lecture Content.</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Retrieved from: https://jfh.georgetown.domains/centralized-lecture-content/content/machine-learning/unsupervised-learning/dimensionality-reduction/PCA/notes.html#pca-vs-clustering</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------------------------------------------</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Define utility plotting function</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_2D(X,color_vector, plot_title):</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    ax.scatter(X[:,<span class="dv">0</span>], X[:,<span class="dv">1</span>],c<span class="op">=</span>color_vector, alpha<span class="op">=</span><span class="fl">0.5</span>) <span class="co">#, c=y</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    ax.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">'PC-1 '</span>, ylabel<span class="op">=</span><span class="st">'PC-2'</span>,</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span> plot_title)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    ax.grid()</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># fig.savefig("test.png")</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Define variance plot function to visualize variance explained by PCA components</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_variance_explained(pca):</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    explained_variance_ratio <span class="op">=</span> pca.explained_variance_ratio_</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot explained variance ratio</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    plt.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(explained_variance_ratio) <span class="op">+</span> <span class="dv">1</span>), explained_variance_ratio, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Number of components'</span>)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Explained variance ratio'</span>)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Explained Variance Ratio by Component'</span>)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot cumulative explained variance</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    cumulative_variance <span class="op">=</span> np.cumsum(explained_variance_ratio)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>    plt.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(cumulative_variance) <span class="op">+</span> <span class="dv">1</span>), cumulative_variance, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Number of components'</span>)</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Cumulative explained variance'</span>)</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Cumulative Explained Variance by Component'</span>)</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="explained-variance-ratio" class="level3">
<h3 class="anchored" data-anchor-id="explained-variance-ratio">Explained Variance Ratio</h3>
<p>The explained variance ratio in PCA indicates the proportion of the total variance in the dataset that is captured by each principal component (PC). This metric is essential for determining the optimal number of components to retain during dimensionality reduction. Each principal component captures a fraction of the total variance, with the first principal component (PC-1) explaining the largest share, followed by the second component (PC-2), and so forth. By analyzing the distribution of variance across the components, it becomes possible to identify which components contribute the most meaningful information to the dataset. The cumulative variance** is calculated by summing the explained variance ratios of successive components. This cumulative measure helps determine how many components are necessary to retain a significant portion of the total variance, such as 90% or 95%. Retaining fewer components simplifies the data representation, making it more computationally efficient, while still preserving most of the underlying structure and variability of the original dataset.</p>
<div id="cell-7" class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply PCA</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA()</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>X_pca <span class="op">=</span> pca.fit_transform(X)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Print variance explained and cumulative variance by each principal component</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Variance explained by each principal component:"</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pca.explained_variance_ratio_[:<span class="dv">10</span>])</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Cumulative variance explained by each principal component:"</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.cumsum(pca.explained_variance_ratio_)[:<span class="dv">10</span>])</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the variance explained</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>plot_variance_explained(pca)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Variance explained by each principal component:
[0.59465697 0.29997716 0.10536587]

Cumulative variance explained by each principal component:
[0.59465697 0.89463413 1.        ]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-4-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-4-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The first plot, <strong>“Explained Variance Ratio by Component,”</strong> shows the proportion of variance captured by each principal component. The steep decline in this plot indicates that the <strong>first principal component (PC-1)</strong> explains the largest portion of the variance, followed by the <strong>second component (PC-2)</strong>. After these first two components, the additional variance explained by subsequent components decreases significantly. This behavior suggests that the majority of the dataset’s structure can be captured by the first two components.</p>
<p>The second plot, <strong>“Cumulative Explained Variance by Component,”</strong> illustrates the total variance explained as additional components are added. The curve rises sharply at the start, with the <strong>first two components</strong> capturing approximately <strong>90% of the total variance</strong>. Beyond the second component, the curve begins to flatten, indicating diminishing returns. This flattening demonstrates that including more components contributes little new information to the overall representation of the data.</p>
<p>Together, these plots emphasize the significance of the first two principal components. By focusing on these components, the dimensionality of the data can be effectively reduced while retaining most of its variance. This reduction simplifies computations, decreases model complexity, and enhances the interpretability of visualizations, all without sacrificing critical information.</p>
<div id="cell-9" class="cell" data-execution_count="101">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>X_pca_2 <span class="op">=</span> pca.fit_transform(X)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5: Plot 2D results</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>plot_2D(X_pca_2, df[<span class="st">'race_encoded'</span>], <span class="st">'Principal Component Analysis Results'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-5-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The plot, <strong>“Principal Component Analysis Results,”</strong> presents the dataset reduced to two dimensions—<strong>PC-1</strong> and <strong>PC-2</strong>—the two principal components that capture the most variance. Each point represents an individual data observation, with colors corresponding to the <code>race_encoded</code> variable.</p>
<ul>
<li><strong>PC-1 (x-axis)</strong> captures the largest portion of the variance, approximately <strong>60%</strong>.<br>
</li>
<li><strong>PC-2 (y-axis)</strong> explains the next largest portion, around <strong>30%</strong>.</li>
</ul>
<p>The distribution of points shows that much of the variance is concentrated along the <strong>PC-1 axis</strong>, suggesting that this direction captures the most meaningful structure in the dataset. The spread along the <strong>PC-2 axis</strong> provides additional separation, though to a lesser extent. The presence of vertical “striping” and overlapping points indicates that the <code>race_encoded</code> variable (color-coded) does not perfectly align with the variance explained by the first two components. This observation suggests that the numerical features—<strong><code>age</code></strong>, <strong><code>sentence_in_years</code></strong>, and <strong><code>years_lost</code></strong>—alone may not fully differentiate racial categories.</p>
<p>In sum, PCA effectively reduces the dataset to two dimensions, capturing a significant portion of the variance. However, the clustering patterns observed suggest that race-based groupings may not be strongly linear within the numerical features.</p>
</section>
</section>
<section id="t-sne-t-distributed-stochastic-neighbor-embedding" class="level2">
<h2 class="anchored" data-anchor-id="t-sne-t-distributed-stochastic-neighbor-embedding">t-SNE (t-distributed Stochastic Neighbor Embedding)</h2>
<p><strong>T-distributed Stochastic Neighbor Embedding (t-SNE)</strong> is a non-linear dimensionality reduction technique designed for visualizing high-dimensional data in a low-dimensional space. It preserves local relationships within the data, making it particularly effective for identifying clusters and patterns that may not be visible in higher dimensions.</p>
<div id="cell-12" class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test different perplexity values</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>perplexity_values <span class="op">=</span> [<span class="dv">5</span>, <span class="dv">30</span>, <span class="dv">50</span>, <span class="dv">100</span>]</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> perplexity <span class="kw">in</span> perplexity_values:</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Running t-SNE with perplexity=</span><span class="sc">{</span>perplexity<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    tsne <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>, perplexity<span class="op">=</span>perplexity, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    X_tsne <span class="op">=</span> tsne.fit_transform(X)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    plot_2D(X_tsne, df[<span class="st">'race_encoded'</span>], <span class="ss">f't-SNE Results (Perplexity=</span><span class="sc">{</span>perplexity<span class="sc">}</span><span class="ss">)'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Running t-SNE with perplexity=5
Running t-SNE with perplexity=30
Running t-SNE with perplexity=50
Running t-SNE with perplexity=100</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-6-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-6-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-6-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-6-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The <strong>t-SNE visualizations</strong> were generated using perplexity values of <strong>5, 30, 50, and 100</strong> to examine how this parameter influences the clustering structure. Perplexity determines the balance between local and global relationships within the data, where lower values emphasize small neighborhoods and higher values capture broader patterns.</p>
<p>At <strong>perplexity = 5</strong>, the plot reveals fragmented and overly localized clusters. While small neighborhoods are highlighted, the data appears disjointed, making it difficult to identify coherent global groupings. This behavior suggests that a perplexity of 5 is too low to capture meaningful structure.</p>
<p>At <strong>perplexity = 30</strong>, the visualization becomes more organized, striking a balance between local and global structure. Clear regional groupings emerge, with smaller clusters visible alongside broader trends. This representation provides an interpretable and balanced view of the data.</p>
<p>At <strong>perplexity = 50</strong>, the clustering appears more cohesive and distinct. The structure of the data is well-defined, and groupings are clearer compared to perplexity = 30. This value maintains a strong balance between fine-grained patterns and global structure, making it ideal for visualizing race-based patterns in the data.</p>
<p>At <strong>perplexity = 100</strong>, the plot emphasizes global structure but sacrifices local details. Clusters become stretched horizontally, and smaller, fine-grained groupings are smoothed out. While broad relationships are highlighted, important insights from localized clusters are diminished.</p>
<p>In conclusion, a perplexity value of <strong>50</strong> was selected as it produces the clearest and most cohesive clusters. This value preserves both local details and global structure, providing the optimal balance for identifying meaningful groupings and visualizing patterns related to race.</p>
<section id="comparison-of-pca-and-t-sne-results" class="level3">
<h3 class="anchored" data-anchor-id="comparison-of-pca-and-t-sne-results">Comparison of PCA and t-SNE Results</h3>
<div id="cell-15" class="cell" data-execution_count="89">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply t-SNE with perplexity=50</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>tsne <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>, perplexity<span class="op">=</span><span class="dv">50</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>X_tsne <span class="op">=</span> tsne.fit_transform(X)  <span class="co"># X is your standardized input data</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Side-by-side comparison: PCA vs t-SNE</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">6</span>))</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co"># PCA Plot</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].scatter(X_pca[:, <span class="dv">0</span>], X_pca[:, <span class="dv">1</span>], c<span class="op">=</span>df[<span class="st">'race_encoded'</span>], alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'PCA Results'</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'PC-1'</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'PC-2'</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="co"># t-SNE Plot</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].scatter(X_tsne[:, <span class="dv">0</span>], X_tsne[:, <span class="dv">1</span>], c<span class="op">=</span>df[<span class="st">'race_encoded'</span>], alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'t-SNE Results'</span>)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'t-SNE Component 1'</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'t-SNE Component 2'</span>)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The two plots above compare the results of <strong>Principal Component Analysis (PCA)</strong> and <strong>t-SNE (perplexity = 50)</strong> when applied to the same dataset. In the <strong>PCA Results</strong>, the data is reduced to two principal components that capture the directions of maximum variance. The points appear somewhat aligned along the <strong>vertical axis (PC-1)</strong>, indicating that the majority of the variance lies in that direction. However, the plot does not reveal clear or well-defined clusters, suggesting that PCA effectively captures <strong>global variance patterns</strong> but struggles to preserve <strong>local neighborhood structures</strong>. The visual separation by race (color-coded) is not particularly distinct in the PCA output. Alternatively, <strong>t-SNE Results</strong> provide a more nuanced and detailed visualization. By balancing local and global relationships, t-SNE produces <strong>more distinct clusters</strong> and patterns with greater cohesion. The clusters are clearer and better separated, indicating that t-SNE excels at preserving the <strong>local structure</strong> of the data. Although some overlap remains, the t-SNE output reveals a structure that is significantly clearer compared to PCA, particularly when visualized using race-based color encoding. Overall, t-SNE (with perplexity = 50) outperforms PCA in uncovering patterns and potential clusters within the dataset. While PCA captures the global variance effectively, it fails to separate groups as clearly. This comparison highlights the advantage of <strong>t-SNE</strong> for visualizing <strong>complex, high-dimensional data</strong> where local relationships are particularly important.</p>
</section>
</section>
</section>
<section id="clustering-methods" class="level1">
<h1>Clustering Methods</h1>
<p>Clustering is an unsupervised learning technique used to identify natural groupings, or clusters, within a dataset. In clustering, the data is unlabeled, meaning there are no predefined classes or categories. The primary goal is to <strong>discover groups of data points</strong> that are similar to each other based on a defined measure of similarity or distance.</p>
<p>In this analysis, <strong>K-Means</strong>, <strong>DBSCAN</strong>, and <strong>Hierarchical Clustering</strong> are applied to the dataset to:<br>
- Explore the structure of the data and identify meaningful clusters.<br>
- Compare the performance and outcomes of each clustering technique.<br>
- Interpret the results to gain insights into groupings and relationships within the data.</p>
<p>By leveraging these methods, the analysis seeks to uncover patterns and hidden structures that may not be immediately apparent, providing a deeper understanding of the data.</p>
<section id="k-means" class="level2">
<h2 class="anchored" data-anchor-id="k-means">K-Means</h2>
<p>K-Means is a foundational unsupervised clustering algorithm that partitions data into a predefined number of clusters, denoted as <strong>K</strong>. K-Means begins by randomly selecting <code>K</code> initial cluster centroids and assigns each data point to the closest centroid based on the Euclidean distance. The centroids are then recalculated as the mean of all data points within their respective clusters. This process of assignment and centroid adjustment repeats until the centroids stabilize or a convergence criterion is reached. The goal of K-Means is to group data into clusters that are both cohesive and well-separated—where distances within each cluster are minimized, and distances between clusters are maximized. While K-Means performs effectively when clusters are spherical and well-defined, it requires specifying the number of clusters (<code>K</code>) in advance, which introduces the need for hyperparameter tuning.</p>
<section id="elbow-method" class="level3">
<h3 class="anchored" data-anchor-id="elbow-method">Elbow Method</h3>
<p>To identify the optimal number of clusters, the <strong>Elbow Method</strong> and <strong>Silhouette Score</strong> are employed. The <strong>Elbow Method</strong> involves plotting the point where the rate of inertia reduction slows, indicating the optimal number of clusters, as adding more clusters beyond it provides diminishing returns in variance reduction. The <strong>Silhouette Score</strong> offers an additional measure to assess clustering quality by evaluating how similar each data point is to its own cluster compared to other clusters. Higher Silhouette Scores indicate clusters that are well-defined and more cohesive, providing a clearer structure within the data.</p>
<div id="cell-19" class="cell" data-execution_count="97">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> silhouette_score</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>, category<span class="op">=</span><span class="pp">UserWarning</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Range for number of clusters</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>k_values <span class="op">=</span> <span class="bu">range</span>(<span class="dv">2</span>, <span class="dv">10</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>wcss <span class="op">=</span> []</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>silhouette_scores <span class="op">=</span> []</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Find optimal K using WCSS and silhouette score</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> k_values:</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>k, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> kmeans.fit_predict(X)  <span class="co"># Assuming X is the 2D data (e.g., PCA or t-SNE)</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    wcss.append(kmeans.inertia_)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    silhouette_scores.append(silhouette_score(X, labels))</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot Elbow Curve</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>plt.plot(k_values, wcss, <span class="st">'-o'</span>)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Elbow Method for K-Means"</span>)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Number of Clusters (K)"</span>)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Within-Cluster Sum of Squares (WCSS)"</span>)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In the <strong>Elbow Method</strong> plot above, the inertia decreases sharply up to around <strong>K = 4 or K = 5</strong>, after which the curve begins to flatten. This behavior suggests that the optimal number of clusters lies between <strong>4 and 5</strong>. By combining insights from both the <strong>Elbow Method</strong> and the <strong>Silhouette Scores</strong>, the best value for <code>K</code> can be confidently selected. Visualizing the clustering results further allows for assessing their alignment with meaningful patterns within the data.</p>
</section>
<section id="visualize-clusters-for-optimal-k" class="level3">
<h3 class="anchored" data-anchor-id="visualize-clusters-for-optimal-k">Visualize Clusters for Optimal K</h3>
<div id="cell-22" class="cell" data-execution_count="99">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize Clusters for Optimal K</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>optimal_k <span class="op">=</span> <span class="dv">4</span>  <span class="co"># Replace with the K identified using the elbow point</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>optimal_k, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>labels_kmeans <span class="op">=</span> kmeans.fit_predict(X)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>plot_2D(X, labels_kmeans, <span class="st">'K-Means Clustering Results'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The visualization above displays the results of K-Means clustering applied with an optimal value of <strong>K = 4</strong>, as determined using the Elbow Method. The data has been projected onto the first two principal components (PC-1 and PC-2) for visualization, with each point color-coded based on its cluster assignment.</p>
<p>The clusters appear well-separated and exhibit distinct patterns along the two principal components:</p>
<ol type="1">
<li>The <strong>yellow cluster</strong> occupies the far right of the <strong>PC-1 axis</strong>, indicating that this group has unique characteristics that distinguish it from the others.<br>
</li>
<li>The <strong>purple cluster</strong> is concentrated at the bottom of the plot, suggesting that it shares common features that set it apart, particularly along <strong>PC-2</strong>.<br>
</li>
<li>The <strong>teal and blue clusters</strong> are more centered with slight overlap, reflecting some shared attributes while still maintaining discernible boundaries.</li>
</ol>
<p>The choice of <strong>K = 4</strong> aligns well with the structure of the exoneration dataset, where race emerged as a key factor during Exploratory Data Analysis (EDA). The racial groups initially considered included <strong>Black, Hispanic, White, Native American, and Asian</strong>. However, the <strong>Asian group</strong> was excluded due to its negligible presence in the dataset, making four clusters a logical and meaningful choice. This outcome mirrors the underlying data distribution, where the remaining racial groups are clearly represented in the clustering results.</p>
<p>These findings suggest that K-Means effectively partitions the data into four meaningful clusters based on the selected features. The clear separation of clusters along <strong>PC-1 and PC-2</strong> highlights that these principal components successfully capture the variance in the data, enabling the differentiation of racial groupings. The slight overlap between clusters may stem from shared attributes across groups or limitations in the chosen features.</p>
</section>
</section>
<section id="dbscan-density-based-spatial-clustering-of-applications-with-noise" class="level2">
<h2 class="anchored" data-anchor-id="dbscan-density-based-spatial-clustering-of-applications-with-noise">DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</h2>
<p>DBSCAN is an unsupervised clustering algorithm that groups data points based on density. Unlike K-Means, which requires specifying the number of clusters in advance, DBSCAN identifies clusters by locating dense regions in the data while marking points in less dense areas as noise. This makes it particularly effective for identifying clusters of arbitrary shapes and handling outliers. DBSCAN relies on two key parameters:</p>
<ol type="1">
<li><strong><code>eps</code></strong>: The maximum distance between two points for them to be considered part of the same neighborhood.<br>
</li>
<li><strong><code>min_samples</code></strong>: The minimum number of points required to form a dense region (a cluster).</li>
</ol>
<p>DBSCAN begins with an unvisited point and determines its neighborhood within the radius <strong><code>eps</code></strong>. If the number of points in the neighborhood meets or exceeds <strong><code>min_samples</code></strong>, a cluster is initiated. The cluster is then expanded by iteratively including points within <strong><code>eps</code></strong> distance of other points already in the cluster. Points that do not meet the density requirement are labeled as <strong>noise</strong> (outliers).</p>
<p>One of the key strengths of DBSCAN is its ability to <strong>detect clusters of varying shapes</strong> and handle datasets with <strong>noisy or irregular boundaries</strong>. Additionally, it does not force every point into a cluster, which allows for the identification of outliers—a feature particularly useful for understanding anomalies within the data. However, selecting appropriate values for <strong><code>eps</code></strong> and <strong><code>min_samples</code></strong> is critical to DBSCAN’s performance. A common strategy involves experimenting with different <strong><code>eps</code></strong> values and assessing the results using metrics such as the <strong>Silhouette Score</strong>.</p>
<p>By leveraging DBSCAN, the analysis can uncover nuanced structures in the dataset that may not be apparent with algorithms like K-Means, offering deeper insights into hidden patterns and systemic disparities.</p>
<section id="eps-hyper-parameter-tuning" class="level3">
<h3 class="anchored" data-anchor-id="eps-hyper-parameter-tuning">Eps Hyper-Parameter Tuning</h3>
<div id="cell-26" class="cell" data-execution_count="93">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> DBSCAN</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Test different eps values for DBSCAN</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>eps_values <span class="op">=</span> [<span class="fl">0.5</span>, <span class="fl">1.0</span>, <span class="fl">1.5</span>, <span class="fl">2.0</span>]</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> eps <span class="kw">in</span> eps_values:</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    dbscan <span class="op">=</span> DBSCAN(eps<span class="op">=</span>eps, min_samples<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    labels_dbscan <span class="op">=</span> dbscan.fit_predict(X)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>        sil_score <span class="op">=</span> silhouette_score(X, labels_dbscan)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"EPS: </span><span class="sc">{</span>eps<span class="sc">}</span><span class="ss">, Silhouette Score: </span><span class="sc">{</span>sil_score<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span>:</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"EPS: </span><span class="sc">{</span>eps<span class="sc">}</span><span class="ss">, Silhouette Score: Undefined (noise present)"</span>)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Visualize Clusters</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    plot_2D(X, labels_dbscan, <span class="ss">f'DBSCAN Clustering (EPS=</span><span class="sc">{</span>eps<span class="sc">}</span><span class="ss">)'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>EPS: 0.5, Silhouette Score: 0.1970450363722957
EPS: 1.0, Silhouette Score: 0.7808945570083328
EPS: 1.5, Silhouette Score: 0.7808945570083328
EPS: 2.0, Silhouette Score: 0.7808945570083328</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-10-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-10-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-10-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-10-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The <strong><code>eps</code> values</strong> for DBSCAN were selected by testing a range of values (<strong>0.5, 1.0, 1.5, and 2.0</strong>) to analyze the algorithm’s sensitivity to this key parameter. DBSCAN uses <strong><code>eps</code></strong> (neighborhood radius) and <strong><code>min_samples</code></strong> to define dense regions, where <code>eps</code> controls the size of these regions. Smaller values of <code>eps</code> result in fragmented clusters, while larger values produce fewer, broader clusters. The chosen values allow for an incremental evaluation of how clustering changes to identify the best balance between <strong>fragmentation</strong> and <strong>cohesion</strong>.</p>
<ol type="1">
<li><strong>EPS = 0.5</strong>
<ul>
<li>At <strong><code>eps = 0.5</code></strong>, the clusters are highly fragmented, with a significant number of points labeled as <strong>noise</strong> (not assigned to any cluster). The neighborhood radius is too small to form large, cohesive clusters.<br>
</li>
<li>The <strong>Silhouette Score</strong> is <strong>0.197</strong>, indicating poor clustering performance with high intra-cluster variance. The results lack meaningful structure and cohesion.</li>
</ul></li>
<li><strong>EPS = 1.0</strong>
<ul>
<li>With <strong><code>eps = 1.0</code></strong>, clustering performance improves significantly. More points are grouped into clusters, dense regions become apparent, and the number of noise points is reduced.<br>
</li>
<li>The <strong>Silhouette Score</strong> rises to <strong>0.78</strong>, reflecting well-defined clusters and clear separation between groups. At this value, the algorithm strikes a good balance between cohesive clusters and noise reduction.</li>
</ul></li>
<li><strong>EPS = 1.5</strong>
<ul>
<li>At <strong><code>eps = 1.5</code></strong>, the clustering results remain largely similar to those observed at <code>eps = 1.0</code>. Most data points are grouped into a single large cluster, with only a small number of points remaining on the periphery.<br>
</li>
<li>The <strong>Silhouette Score</strong> remains stable at <strong>0.78</strong>, but expanding the neighborhood radius further does not uncover additional structure in the data.</li>
</ul></li>
<li><strong>EPS = 2.0</strong>
<ul>
<li>When <strong><code>eps = 2.0</code></strong>, nearly all points are assigned to a single large cluster. While this eliminates noise points, it oversimplifies the data and removes meaningful structural separation.<br>
</li>
<li>Although the Silhouette Score remains consistent, the clustering lacks distinct groupings, indicating that <code>eps = 2.0</code> is too large for this dataset.</li>
</ul></li>
</ol>
<p>The analysis of DBSCAN results demonstrates that <strong><code>eps = 1.0</code></strong> provides the best clustering performance. At this value, the clusters are <strong>well-defined</strong>, noise is minimized, and the <strong>Silhouette Score</strong> achieves a peak of <strong>0.78</strong>. Smaller <code>eps</code> values, such as <strong>0.5</strong>, lead to fragmented clusters with excessive noise, while larger values, such as <strong>1.5</strong> and <strong>2.0</strong>, smooth the data excessively, reducing meaningful separation. Thus, <strong><code>eps = 1.0</code></strong> emerges as the optimal choice, balancing noise reduction with cohesive and interpretable clustering results.</p>
</section>
</section>
<section id="hierarchical-clustering" class="level2">
<h2 class="anchored" data-anchor-id="hierarchical-clustering">Hierarchical Clustering</h2>
<p>Hierarchical Clustering is an unsupervised machine learning algorithm that builds a hierarchy of clusters through an iterative process. Unlike <strong>K-Means</strong>, which requires specifying the number of clusters in advance, hierarchical clustering produces a <strong>dendrogram</strong>—a tree-like structure that shows how data points are grouped at different levels of granularity. The algorithm can follow two main approaches: <strong>agglomerative</strong> (bottom-up) and <strong>divisive</strong> (top-down). In <strong>agglomerative clustering</strong>, each data point starts as its own cluster. Clusters are progressively merged based on their similarity (distance), defined by the linkage method. This analysis uses <strong>Ward’s linkage</strong>, which minimizes intra-cluster variance at each step, resulting in well-balanced and cohesive clusters. The dendrogram serves as a visual tool to identify the optimal number of clusters by “cutting” the tree at a height where clusters are most distinct. This makes hierarchical clustering particularly effective for EDA, as it does not require prior knowledge of the number of clusters.</p>
<p>To determine the optimal clusters, the dendrogram was examined for large vertical distances, indicating well-separated groups. Ward’s linkage ensures minimal intra-cluster variance, producing compact and interpretable groupings—an essential property for datasets like the exoneration data, where clear separations between groups are critical. Hierarchical clustering with <strong>Ward’s linkage</strong> offers a flexible and interpretable approach for uncovering the dataset’s structure. By analyzing the dendrogram and selecting the appropriate height, meaningful clusters were identified. Compared to <strong>K-Means</strong> and <strong>DBSCAN</strong>, hierarchical clustering provides the additional benefit of visualizing relationships between clusters, making it a valuable method for validating and interpreting clustering results.</p>
<div id="cell-29" class="cell" data-execution_count="94">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.cluster.hierarchy <span class="im">import</span> dendrogram, linkage</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> AgglomerativeClustering</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create Dendrogram</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>linkage_matrix <span class="op">=</span> linkage(X, method<span class="op">=</span><span class="st">'ward'</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">7</span>))</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>dendrogram(linkage_matrix)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Hierarchical Clustering Dendrogram"</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Samples"</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Distance"</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply Agglomerative Clustering for chosen number of clusters</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>n_clusters <span class="op">=</span> <span class="dv">4</span>  <span class="co"># Replace with the number identified using the dendrogram</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>agglom <span class="op">=</span> AgglomerativeClustering(n_clusters<span class="op">=</span>n_clusters, linkage<span class="op">=</span><span class="st">'ward'</span>)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>labels_agglom <span class="op">=</span> agglom.fit_predict(X)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize Clusters</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>plot_2D(X, labels_agglom, <span class="st">'Hierarchical Clustering Results'</span>)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-11-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="hierarchical-clustering-results-interpretation" class="level3">
<h3 class="anchored" data-anchor-id="hierarchical-clustering-results-interpretation">Hierarchical Clustering Results Interpretation</h3>
<p>The dendrogram above visually represents the clustering hierarchy generated using <strong>Ward’s linkage</strong> in hierarchical clustering. The dendrogram shows how data points are progressively merged into clusters at varying distances. By <strong>cutting the dendrogram</strong> at an appropriate height, <strong>four clusters</strong> are identified, which align with the patterns observed in the earlier <strong>K-Means results</strong>.</p>
<p>The <strong>Hierarchical Clustering Results</strong> plot projects these clusters onto the first two principal components (PC-1 and PC-2), with data points color-coded based on their cluster assignments:<br>
1. <strong>Purple Cluster</strong>: Positioned at the <strong>top of the PC-2 axis</strong>, this group stands out distinctly, indicating unique features that strongly separate it along PC-2.<br>
2. <strong>Yellow Cluster</strong>: Spread vertically across the <strong>middle region</strong> of PC-1, this cluster shows moderate cohesion while spanning a wide range along PC-2.<br>
3. <strong>Teal Cluster</strong>: Concentrated toward the <strong>bottom-right</strong>, this group is more compact and well-defined along PC-1, suggesting strong intra-cluster similarity.<br>
4. <strong>Blue Cluster</strong>: Located in the <strong>bottom-left region</strong>, this group appears dense and cohesive, with relatively low variation along PC-2.</p>
<p>The dendrogram supports the selection of <strong>four clusters</strong>, evidenced by clear separations into branches at a distance of approximately <strong>20 units</strong>. These clusters are well-defined and consistent with the natural groupings observed in the data, particularly along PC-1 and PC-2. Compared to other clustering methods, hierarchical clustering offers a notable advantage by providing a hierarchical structure for cluster exploration. This allows for the analysis of groupings at multiple levels of granularity, enhancing the interpretability of the results. Overall, hierarchical clustering confirms the patterns identified in the <strong>K-Means results</strong>, while providing additional insights through the dendrogram. The use of <strong>Ward’s linkage</strong> ensures minimal intra-cluster variance, resulting in cohesive and interpretable clusters. These clusters align closely with the underlying structure of the data, reinforcing the conclusions drawn from the <strong>PCA</strong> and <strong>K-Means</strong> analyses.</p>
</section>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">Discussion</h2>
<p>The clustering analysis using <strong>K-Means</strong>, <strong>DBSCAN</strong>, and <strong>Hierarchical Clustering</strong> reveals meaningful patterns within the exoneration dataset. Each method identifies groupings that align with the underlying racial categories—Black, Hispanic, White, and Native American—highlighted in the earlier exploratory data analysis (EDA).</p>
<p><strong>K-Means</strong> effectively grouped the data into four clusters, which correspond closely with the primary racial groups observed. The Elbow Method confirmed that four clusters were optimal, with clear separations visible along PC-1 and PC-2. This method produced cohesive and well-defined clusters with minimal noise, reflecting the structure of the data. The results demonstrate a strong alignment with the racial breakdown, as the groupings were particularly distinct along PC-1, capturing critical variance in the dataset.</p>
<p><strong>DBSCAN</strong> yielded varying results depending on the value of the <code>eps</code> parameter. At <code>eps = 1.0</code>, the method achieved the best balance between cluster cohesion and noise reduction, with a Silhouette Score of 0.78. At this setting, the clusters were distinguishable, and the algorithm successfully identified dense regions within the data. However, increasing <code>eps</code> beyond 1.0 caused most data points to merge into a single cluster, diminishing DBSCAN’s ability to detect finer groupings. Despite this limitation, DBSCAN proved particularly useful for identifying sparse regions and outliers, which may correspond to underrepresented racial groups or anomalies within the dataset.</p>
<p><strong>Hierarchical Clustering</strong> using Ward’s linkage provided additional insights through its dendrogram, offering a clear visualization of the clustering process. The dendrogram supported the selection of four clusters, which closely matched the groupings identified by K-Means. Projecting the clusters onto the first two principal components revealed well-defined and interpretable separations. Ward’s linkage ensured minimal intra-cluster variance, producing compact clusters that effectively reflect the structure of the data.</p>
<p>While all three clustering methods revealed meaningful patterns, <strong>K-Means</strong> and <strong>Hierarchical Clustering</strong> produced the clearest and most interpretable results. Both consistently identified four clusters that align with the racial categories analyzed earlier. K-Means offered computational efficiency and well-separated groupings, while Hierarchical Clustering provided the added benefit of a dendrogram, which validated the results by illustrating the relationships between clusters. <strong>DBSCAN</strong>, while effective for detecting outliers and handling irregular data distributions, struggled to produce distinct groupings beyond specific parameter settings.</p>
<p>The clustering results demonstrate that the exoneration data can be effectively grouped into four distinct clusters that align with the racial categories of Black, Hispanic, White, and Native American. These findings reinforce the observations from the EDA, where negligible representation of certain racial groups, such as Asians, led to their exclusion from the analysis. By revealing clear and consistent patterns within the data, this analysis highlights systemic disparities in exoneration outcomes that are closely tied to racial identity. Understanding these patterns provides critical insights into racial biases within the criminal justice system, emphasizing the need for informed policy changes and further investigations.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>