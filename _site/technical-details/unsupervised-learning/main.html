<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.555">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>DSAN-5000: Project - Unsupervised Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../assets/gu-logo.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">DSAN-5000: Project</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../instructions/overview.html"> 
<span class="menu-text">Instructions</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../report/report.html"> 
<span class="menu-text">Report</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-technical-details" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Technical details</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-technical-details">    
        <li>
    <a class="dropdown-item" href="../../technical-details/data-collection/main.html">
 <span class="dropdown-text">Data-collection</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../technical-details/data-cleaning/main.html">
 <span class="dropdown-text">Data-cleaning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../technical-details/data-balancing/main.html">
 <span class="dropdown-text">Counterfactual Data Balancing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../technical-details/eda/main.html">
 <span class="dropdown-text">Exploratory Data Analysis</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../technical-details/unsupervised-learning/main.html">
 <span class="dropdown-text">Unsupervised Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../technical-details/supervised-learning/main.html">
 <span class="dropdown-text">Supervised Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../technical-details/progress-log.html">
 <span class="dropdown-text">Progress Log</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../technical-details/llm-usage-log.html">
 <span class="dropdown-text">LLM usage Log</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../bibliography.html"> 
<span class="menu-text">Bibliography</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../appendix.html"> 
<span class="menu-text">Appendix</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/dsan-5000/project-courtneyrgreen.git"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://crg123.georgetown.domains/"> 
<span class="menu-text">Back to Portfolio</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-and-motivation" id="toc-introduction-and-motivation" class="nav-link active" data-scroll-target="#introduction-and-motivation">Introduction and Motivation</a></li>
  <li><a href="#data-preprocessing" id="toc-data-preprocessing" class="nav-link" data-scroll-target="#data-preprocessing">Data Preprocessing</a></li>
  <li><a href="#dimensionality-reduction" id="toc-dimensionality-reduction" class="nav-link" data-scroll-target="#dimensionality-reduction">Dimensionality Reduction</a>
  <ul class="collapse">
  <li><a href="#pca-principal-component-analysis" id="toc-pca-principal-component-analysis" class="nav-link" data-scroll-target="#pca-principal-component-analysis">PCA (Principal Component Analysis)</a>
  <ul class="collapse">
  <li><a href="#explained-variance-ratio" id="toc-explained-variance-ratio" class="nav-link" data-scroll-target="#explained-variance-ratio">Explained Variance Ratio</a></li>
  </ul></li>
  <li><a href="#t-sne-t-distributed-stochastic-neighbor-embedding" id="toc-t-sne-t-distributed-stochastic-neighbor-embedding" class="nav-link" data-scroll-target="#t-sne-t-distributed-stochastic-neighbor-embedding">t-SNE (t-distributed Stochastic Neighbor Embedding)</a>
  <ul class="collapse">
  <li><a href="#comparison-of-pca-and-t-sne-results" id="toc-comparison-of-pca-and-t-sne-results" class="nav-link" data-scroll-target="#comparison-of-pca-and-t-sne-results">Comparison of PCA and t-SNE Results</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#clustering-methods" id="toc-clustering-methods" class="nav-link" data-scroll-target="#clustering-methods">Clustering Methods</a>
  <ul class="collapse">
  <li><a href="#k-means" id="toc-k-means" class="nav-link" data-scroll-target="#k-means">K-Means</a>
  <ul class="collapse">
  <li><a href="#elbow-method" id="toc-elbow-method" class="nav-link" data-scroll-target="#elbow-method">Elbow-Method</a></li>
  <li><a href="#visualize-clusters-for-optimal-k" id="toc-visualize-clusters-for-optimal-k" class="nav-link" data-scroll-target="#visualize-clusters-for-optimal-k">Visualize Clusters for Optimal K</a></li>
  </ul></li>
  <li><a href="#dbscan-density-based-spatial-clustering-of-applications-with-noise" id="toc-dbscan-density-based-spatial-clustering-of-applications-with-noise" class="nav-link" data-scroll-target="#dbscan-density-based-spatial-clustering-of-applications-with-noise">DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</a>
  <ul class="collapse">
  <li><a href="#eps-hyper-parameter-tuning" id="toc-eps-hyper-parameter-tuning" class="nav-link" data-scroll-target="#eps-hyper-parameter-tuning">Eps Hyper-Parameter Tuning</a></li>
  </ul></li>
  <li><a href="#hierarchical-clustering" id="toc-hierarchical-clustering" class="nav-link" data-scroll-target="#hierarchical-clustering">Hierarchical Clustering</a>
  <ul class="collapse">
  <li><a href="#hierarchical-clustering-results-interpretation" id="toc-hierarchical-clustering-results-interpretation" class="nav-link" data-scroll-target="#hierarchical-clustering-results-interpretation">Hierarchical Clustering Results Interpretation</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a>
  <ul class="collapse">
  <li><a href="#comparison-of-methods" id="toc-comparison-of-methods" class="nav-link" data-scroll-target="#comparison-of-methods">Comparison of Methods</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Unsupervised Learning</h1>
</div>



<div class="quarto-title-meta column-page-left">

    
  
    
  </div>
  


</header>


<section id="introduction-and-motivation" class="level1">
<h1>Introduction and Motivation</h1>
<p>This analysis employs <strong>unsupervised learning</strong> techniques—including <strong>Principal Component Analysis (PCA)</strong>, <strong>t-Distributed Stochastic Neighbor Embedding (t-SNE)</strong>, <strong>K-Means</strong>, <strong>DBSCAN</strong>, and <strong>Hierarchical Clustering</strong>—to examine the <strong>Illinois exoneration dataset</strong>. The primary objective is to identify patterns and hidden structures within the data, particularly focusing on how case characteristics, demographic variables (such as race and county), and the number of years lost to wrongful convictions intersect.</p>
<p>The analysis is structured as follows:<br>
1. <strong>Dimensionality Reduction</strong>: Methods such as PCA and t-SNE are utilized to project high-dimensional data into lower-dimensional spaces, simplifying the visualization of complex relationships while preserving key structural and variance-based insights.</p>
<ol start="2" type="1">
<li><p><strong>Clustering</strong>: Clustering techniques—<strong>K-Means</strong>, <strong>DBSCAN</strong>, and <strong>Hierarchical Clustering</strong>—are applied to uncover natural groupings within the dataset and assess whether these clusters align with demographic features like race or case-related factors.</p></li>
<li><p><strong>Evaluation and Interpretation</strong>: The performance of each method is evaluated, and clustering results are compared to draw meaningful interpretations. Visualizations are integrated throughout the analysis to enhance clarity and support findings.</p></li>
</ol>
<p>The motivation for this analysis stems from the critical need to uncover systemic patterns in wrongful conviction data. By applying unsupervised learning methods, the investigation aims to reveal relationships and disparities between demographic factors and case outcomes that are not immediately apparent. These insights contribute to a deeper understanding of biases and inequities within exoneration cases and support broader efforts for justice system reform.</p>
</section>
<section id="data-preprocessing" class="level1">
<h1>Data Preprocessing</h1>
<p>The data preprocessing stage prepares the <strong>Illinois exoneration dataset</strong> for dimensionality reduction and clustering. This step involves selecting relevant features, encoding categorical variables, and standardizing numerical features to ensure compatibility with unsupervised learning algorithms.</p>
<section id="feature-selection" class="level4">
<h4 class="anchored" data-anchor-id="feature-selection">Feature Selection</h4>
<p>The features chosen for this analysis include a combination of <strong>numerical</strong> and <strong>categorical</strong> variables. The numerical variables—<strong>age</strong>, <strong>sentence in years</strong>, and <strong>years lost</strong>—were selected to provide quantitative insights into exoneration cases, such as the age at conviction, the length of imprisonment, and the total number of years lost. The categorical variable <strong>race</strong> was included to capture demographic patterns and be used as a <strong>color vector</strong> during visualization to assess how the identified clusters align with racial groupings.</p>
<p>The selected features are as follows:<br>
- <strong>Numerical</strong>: <code>age</code>, <code>sentence_in_years</code>, <code>years_lost</code><br>
- <strong>Categorical</strong>: <code>race</code></p>
</section>
<section id="standardization-and-encoding" class="level4">
<h4 class="anchored" data-anchor-id="standardization-and-encoding">Standardization and Encoding</h4>
<p>To ensure that numerical features contribute equally to the analysis, they were standardized using <strong>StandardScaler</strong>. Standardization adjusts each numerical feature to have a mean of 0 and a standard deviation of 1, preventing variables like sentence lengths from dominating the clustering process. The categorical variable <strong>race</strong> was encoded using <strong>Label Encoding</strong>, which assigns a numerical value to each category (e.g., race). This transformation ensures compatibility with algorithms such as <strong>K-Means</strong>, <strong>DBSCAN</strong>, and <strong>Hierarchical Clustering</strong> that require numerical inputs for processing.</p>
<div id="cell-3" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler, LabelEncoder</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'../../data/processed-data/illinois_exoneration_data.csv'</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Select features of interest</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">'age'</span>, <span class="st">'sentence_in_years'</span>, <span class="st">'years_lost'</span>, <span class="st">'race'</span>, <span class="st">'county'</span>]</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df[features].dropna()</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Encode categorical variables</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>le_race <span class="op">=</span> LabelEncoder()</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>le_county <span class="op">=</span> LabelEncoder()</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'race_encoded'</span>] <span class="op">=</span> le_race.fit_transform(df[<span class="st">'race'</span>])</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardize numerical variables</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>scaled_features <span class="op">=</span> scaler.fit_transform(df[[<span class="st">'age'</span>, <span class="st">'sentence_in_years'</span>, <span class="st">'years_lost'</span>]])</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> scaled_features  <span class="co"># Already standardized</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="dimensionality-reduction" class="level1">
<h1>Dimensionality Reduction</h1>
<p>The objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.</p>
<section id="pca-principal-component-analysis" class="level2">
<h2 class="anchored" data-anchor-id="pca-principal-component-analysis">PCA (Principal Component Analysis)</h2>
<div id="cell-5" class="cell" data-execution_count="102">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------------------------------------------</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Unsupervised Learning Implementation</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Principal Component Analysis (PCA) and clustering methods used</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># in this code are based on demos and labs provided by:</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Hickman, J. (2024.). Principal Components Analysis and clustering.</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Georgetown University Centralized Lecture Content.</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Retrieved from: https://jfh.georgetown.domains/centralized-lecture-content/content/machine-learning/unsupervised-learning/dimensionality-reduction/PCA/notes.html#pca-vs-clustering</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------------------------------------------</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Define utility plotting function</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_2D(X,color_vector, plot_title):</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    ax.scatter(X[:,<span class="dv">0</span>], X[:,<span class="dv">1</span>],c<span class="op">=</span>color_vector, alpha<span class="op">=</span><span class="fl">0.5</span>) <span class="co">#, c=y</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    ax.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">'PC-1 '</span>, ylabel<span class="op">=</span><span class="st">'PC-2'</span>,</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span> plot_title)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    ax.grid()</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># fig.savefig("test.png")</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Define variance plot function to visualize variance explained by PCA components</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_variance_explained(pca):</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    explained_variance_ratio <span class="op">=</span> pca.explained_variance_ratio_</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot explained variance ratio</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    plt.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(explained_variance_ratio) <span class="op">+</span> <span class="dv">1</span>), explained_variance_ratio, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Number of components'</span>)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Explained variance ratio'</span>)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Explained Variance Ratio by Component'</span>)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot cumulative explained variance</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    cumulative_variance <span class="op">=</span> np.cumsum(explained_variance_ratio)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>    plt.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(cumulative_variance) <span class="op">+</span> <span class="dv">1</span>), cumulative_variance, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Number of components'</span>)</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Cumulative explained variance'</span>)</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Cumulative Explained Variance by Component'</span>)</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="explained-variance-ratio" class="level3">
<h3 class="anchored" data-anchor-id="explained-variance-ratio">Explained Variance Ratio</h3>
<p>The explained variance ratio in PCA indicates how much of the total variance in the dataset is captured by each principal component (PC). It plays a crucial role in determining the optimal number of components to use in PCA. Each principal component captures a portion of the total variance, with the first component (PC-1) explaining the most variance, followed by the second (PC-2), and so on. By examining the variance distribution across components, we can identify which components contribute the most information to the dataset. The cumulative variance is obtained by summing the explained variance ratios of successive components. This cumulative measure helps determine how many components are required to retain a significant portion of the total variance, such as 90% or 95%. Fewer components result in a simpler and more efficient representation of the data, while still preserving most of its structure.</p>
<div id="cell-7" class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply PCA</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA()</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>X_pca <span class="op">=</span> pca.fit_transform(X)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Print variance explained and cumulative variance by each principal component</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Variance explained by each principal component:"</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pca.explained_variance_ratio_[:<span class="dv">10</span>])</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Cumulative variance explained by each principal component:"</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.cumsum(pca.explained_variance_ratio_)[:<span class="dv">10</span>])</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the variance explained</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>plot_variance_explained(pca)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Variance explained by each principal component:
[0.59465697 0.29997716 0.10536587]

Cumulative variance explained by each principal component:
[0.59465697 0.89463413 1.        ]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-4-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-4-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The first plot, <strong>“Explained Variance Ratio by Component,”</strong> shows the proportion of variance captured by each principal component. The steep decline in this plot indicates that the first component (PC-1) explains the largest portion of the variance, followed by the second component (PC-2). After these first two components, the amount of additional variance explained by subsequent components decreases significantly. This behavior suggests that most of the dataset’s structure can be captured by the first two components.</p>
<p>The second plot, <strong>“Cumulative Explained Variance by Component,”</strong> illustrates the total variance explained as additional components are included. The curve rises sharply at first, with the first two components capturing approximately 90% of the total variance. Beyond the second component, the curve begins to flatten, showing diminishing returns. This flattening indicates that including more components does not add much new information to the representation of the data.</p>
<p>Together, these plots highlight the importance of the first two principal components. By focusing on these components, we can reduce the dimensionality of the data while retaining most of its variance. This simplification improves computational efficiency, reduces model complexity, and makes data visualization more interpretable without sacrificing critical information.</p>
<div id="cell-9" class="cell" data-execution_count="101">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>X_pca_2 <span class="op">=</span> pca.fit_transform(X)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5: Plot 2D results</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>plot_2D(X_pca_2, df[<span class="st">'race_encoded'</span>], <span class="st">'Principal Component Analysis Results'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-5-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The plot, <strong>“Principal Component Analysis Results,”</strong> displays the data reduced to two dimensions (PC-1 and PC-2), which are the two principal components that capture the most variance in the dataset. Each point represents an individual data observation, with colors corresponding to the <code>race_encoded</code> variable.</p>
<ul>
<li><strong>PC-1 (x-axis)</strong> captures the largest portion of the variance (around ~60%).<br>
</li>
<li><strong>PC-2 (y-axis)</strong> explains the next largest portion of variance (~30%).</li>
</ul>
<p>From the plot, the data points appear somewhat clustered along the PC-1 axis, indicating that much of the variance in the dataset is captured in this direction. However, there is also a spread along PC-2, suggesting that the second principal component adds additional separation. The vertical “striping” and overlapping points suggest that race (color-coded) does not perfectly align with the variance captured by the first two components. This implies that the numerical features (<code>age</code>, <code>sentence_in_years</code>, <code>years_lost</code>) alone may not fully separate or distinguish racial categories.</p>
<p>In summary, while PCA effectively reduces the data to two dimensions, the clustering of points indicates that race-based patterns may not be strongly linear within the numerical features. Further investigation, such as applying clustering techniques (e.g., K-Means) or nonlinear dimensionality reduction methods like t-SNE, could provide additional insights into underlying relationships.</p>
</section>
</section>
<section id="t-sne-t-distributed-stochastic-neighbor-embedding" class="level2">
<h2 class="anchored" data-anchor-id="t-sne-t-distributed-stochastic-neighbor-embedding">t-SNE (t-distributed Stochastic Neighbor Embedding)</h2>
<div id="cell-12" class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test different perplexity values</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>perplexity_values <span class="op">=</span> [<span class="dv">5</span>, <span class="dv">30</span>, <span class="dv">50</span>, <span class="dv">100</span>]</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> perplexity <span class="kw">in</span> perplexity_values:</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Running t-SNE with perplexity=</span><span class="sc">{</span>perplexity<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    tsne <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>, perplexity<span class="op">=</span>perplexity, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    X_tsne <span class="op">=</span> tsne.fit_transform(X)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    plot_2D(X_tsne, df[<span class="st">'race_encoded'</span>], <span class="ss">f't-SNE Results (Perplexity=</span><span class="sc">{</span>perplexity<span class="sc">}</span><span class="ss">)'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Running t-SNE with perplexity=5
Running t-SNE with perplexity=30
Running t-SNE with perplexity=50
Running t-SNE with perplexity=100</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-6-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-6-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-6-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-6-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The t-SNE visualizations were generated with perplexity values of <strong>5, 30, 50, and 100</strong> to explore how the parameter influences the clustering structure. Perplexity determines how local or global the relationships are in the data, with lower values focusing on small neighborhoods and higher values capturing broader global patterns.</p>
<p>At <strong>perplexity = 5</strong>, the plot shows very fragmented and overly localized clusters. While small neighborhoods are highlighted, the data appears disjointed, making it difficult to identify meaningful global groupings. This suggests that a perplexity of 5 is too low to capture a coherent structure.</p>
<p>At <strong>perplexity = 30</strong>, the visualization becomes more organized. There is a noticeable balance between local and global structure, with clear regional groupings emerging. Smaller clusters appear alongside broader trends, providing an interpretable and balanced representation of the data.</p>
<p>At <strong>perplexity = 50</strong>, the clustering becomes more cohesive and distinct. The structure of the data is well-defined, and the groupings are clearer compared to perplexity = 30. The balance between small-scale patterns and global structure is maintained, making this value an ideal choice for visualizing race-based patterns in the data.</p>
<p>At <strong>perplexity = 100</strong>, the plot emphasizes the global structure at the expense of local details. The clusters become stretched horizontally, and smaller, fine-grained groupings are smoothed out. While it highlights broad relationships, it sacrifices the clarity of smaller clusters that may hold important insights.</p>
<p><strong>In conclusion</strong>, a perplexity value of <strong>50</strong> was chosen because it produces the clearest and most cohesive clusters while preserving both local details and global structure. This value strikes the best balance for identifying meaningful groupings in the data and visualizing patterns related to race.</p>
<section id="comparison-of-pca-and-t-sne-results" class="level3">
<h3 class="anchored" data-anchor-id="comparison-of-pca-and-t-sne-results">Comparison of PCA and t-SNE Results</h3>
<div id="cell-15" class="cell" data-execution_count="89">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply t-SNE with perplexity=50</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>tsne <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>, perplexity<span class="op">=</span><span class="dv">50</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>X_tsne <span class="op">=</span> tsne.fit_transform(X)  <span class="co"># X is your standardized input data</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Side-by-side comparison: PCA vs t-SNE</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">6</span>))</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co"># PCA Plot</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].scatter(X_pca[:, <span class="dv">0</span>], X_pca[:, <span class="dv">1</span>], c<span class="op">=</span>df[<span class="st">'race_encoded'</span>], alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'PCA Results'</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'PC-1'</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'PC-2'</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="co"># t-SNE Plot</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].scatter(X_tsne[:, <span class="dv">0</span>], X_tsne[:, <span class="dv">1</span>], c<span class="op">=</span>df[<span class="st">'race_encoded'</span>], alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'t-SNE Results'</span>)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'t-SNE Component 1'</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'t-SNE Component 2'</span>)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The two plots above compare the results of <strong>Principal Component Analysis (PCA)</strong> and <strong>t-SNE (perplexity=50)</strong> when applied to the same dataset.</p>
<p>In the <strong>PCA Results</strong> (left plot), the data is reduced to two principal components, which capture the directions of maximum variance. The points appear somewhat aligned along the vertical axis (PC-1), indicating that the majority of the variance lies in that direction. However, the plot does not show clear or well-defined clusters, suggesting that PCA is capturing global variance patterns but struggles to preserve local neighborhood structures. The visual separation by race (color-coded) is not particularly clear in the PCA output.</p>
<p>In contrast, the <strong>t-SNE Results</strong> (right plot) provide a more nuanced and detailed visualization. By focusing on both local and global relationships, t-SNE produces distinct clusters and patterns that are more clearly separated. The clusters show greater cohesion, suggesting that t-SNE is better at preserving the local structure of the data. While there is still overlap among the points, the t-SNE output reveals a clearer structure and grouping compared to PCA, particularly with race-based color encoding.</p>
<p><strong>Overall</strong>, t-SNE (with perplexity=50) performs better at uncovering patterns and potential clusters in the dataset, whereas PCA primarily captures the global variance but fails to separate the groups as effectively. This highlights the advantage of t-SNE for visualizing complex, high-dimensional data where local relationships are important.</p>
</section>
</section>
</section>
<section id="clustering-methods" class="level1">
<h1>Clustering Methods</h1>
<p>Apply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.</p>
<p>We apply <strong>K-Means</strong>, <strong>DBSCAN</strong>, and <strong>Hierarchical Clustering</strong> to identify clusters in the data.</p>
<section id="k-means" class="level2">
<h2 class="anchored" data-anchor-id="k-means">K-Means</h2>
<p>K-Means is a widely used unsupervised clustering algorithm that partitions data into a predefined number of clusters, denoted as <strong>K</strong>. The algorithm works iteratively to minimize the <strong>Within-Cluster Sum of Squares (WCSS)</strong>, which measures the variance within each cluster. K-Means starts by randomly selecting <code>K</code> initial cluster centroids and assigns each data point to the closest centroid based on the Euclidean distance. It then recalculates the centroids as the mean of the data points within each cluster and repeats the assignment process until the centroids stabilize or a convergence criterion is met.</p>
<p>The primary purpose of K-Means is to group data into cohesive clusters where intra-cluster distances are minimized, and inter-cluster distances are maximized. It is particularly effective for spherical or well-separated clusters. However, K-Means requires the number of clusters (<code>K</code>) to be specified in advance, which necessitates hyperparameter tuning.</p>
<section id="elbow-method" class="level3">
<h3 class="anchored" data-anchor-id="elbow-method">Elbow-Method</h3>
<p>To determine the optimal number of clusters, we use the <strong>Elbow Method</strong> and the <strong>Silhouette Score</strong>. The <strong>Elbow Method</strong> involves plotting the WCSS against the number of clusters and identifying the “elbow point,” where the rate of decrease in WCSS slows down. This point suggests the optimal number of clusters, as adding more clusters beyond this point yields diminishing returns in variance reduction. Additionally, the <strong>Silhouette Score</strong> evaluates the quality of clustering by measuring how similar each data point is to its own cluster compared to others. A higher Silhouette Score indicates better-defined and more cohesive clusters.</p>
<div id="cell-19" class="cell" data-execution_count="97">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> silhouette_score</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>, category<span class="op">=</span><span class="pp">UserWarning</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Range for number of clusters</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>k_values <span class="op">=</span> <span class="bu">range</span>(<span class="dv">2</span>, <span class="dv">10</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>wcss <span class="op">=</span> []</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>silhouette_scores <span class="op">=</span> []</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Find optimal K using WCSS and silhouette score</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> k_values:</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>k, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> kmeans.fit_predict(X)  <span class="co"># Assuming X is the 2D data (e.g., PCA or t-SNE)</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    wcss.append(kmeans.inertia_)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    silhouette_scores.append(silhouette_score(X, labels))</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot Elbow Curve</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>plt.plot(k_values, wcss, <span class="st">'-o'</span>)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Elbow Method for K-Means"</span>)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Number of Clusters (K)"</span>)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Within-Cluster Sum of Squares (WCSS)"</span>)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In the Elbow Method plot above, the WCSS decreases significantly up to around <strong>K=4 or K=5</strong>, after which the curve begins to flatten, suggesting that 4–5 clusters might be optimal. By combining insights from both the Elbow Method and Silhouette Scores, we can confidently select the best value for <code>K</code> and visualize the clustering results to assess their alignment with meaningful patterns in the data.</p>
</section>
<section id="visualize-clusters-for-optimal-k" class="level3">
<h3 class="anchored" data-anchor-id="visualize-clusters-for-optimal-k">Visualize Clusters for Optimal K</h3>
<div id="cell-22" class="cell" data-execution_count="99">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize Clusters for Optimal K</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>optimal_k <span class="op">=</span> <span class="dv">4</span>  <span class="co"># Replace with the K identified using the elbow point</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>optimal_k, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>labels_kmeans <span class="op">=</span> kmeans.fit_predict(X)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>plot_2D(X, labels_kmeans, <span class="st">'K-Means Clustering Results'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The visualization above shows the results of K-Means clustering applied with an optimal value of <strong>K = 4</strong>, as determined using the Elbow Method. The data is projected onto the first two principal components (PC-1 and PC-2) for visualization, and each point is color-coded according to its cluster assignment.</p>
<p>The clusters are well-separated and exhibit distinct patterns along the two principal components. Notably: 1. The <strong>yellow cluster</strong> occupies the far right of the PC-1 axis, indicating that this group has unique characteristics that distinguish it from the other clusters. 2. The <strong>purple cluster</strong> is concentrated at the bottom of the plot, suggesting that it shares common features that separate it from the other groups, particularly along PC-2. 3. The <strong>teal and blue clusters</strong> are more centered and slightly overlap but still maintain discernible boundaries, reflecting some similarities in their features.</p>
<p>The choice of <strong>K = 4</strong> aligns well with the nature of the <strong>exoneration data</strong>, where race plays a significant role. In the EDA, the key racial groups considered were <strong>Black, Hispanic, White, Native American, and Asian</strong>. However, the Asian group had such a negligible presence in the dataset that it had to be excluded, making <strong>four clusters</strong> a logical choice. This result mirrors the underlying data distribution, where the primary racial groups remaining are well-represented in the clustering results.</p>
<p>These findings indicate that K-Means successfully partitions the data into four meaningful clusters based on the selected features. The clear separation of clusters along PC-1 and PC-2 reflects that these principal components effectively capture the variance in the data and help distinguish racial groupings. The slight overlap between clusters may be due to shared attributes among groups or the limitations of the chosen features.</p>
<p>Further analysis could involve examining the cluster centroids to identify the defining characteristics of each group. Additionally, assessing how these clusters align with the race variable (<code>race_encoded</code>) could provide deeper insights into systemic patterns within the exoneration dataset.</p>
</section>
</section>
<section id="dbscan-density-based-spatial-clustering-of-applications-with-noise" class="level2">
<h2 class="anchored" data-anchor-id="dbscan-density-based-spatial-clustering-of-applications-with-noise">DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</h2>
<p>DBSCAN is an unsupervised clustering algorithm that groups data points based on density. Unlike K-Means, which requires a predefined number of clusters, DBSCAN identifies clusters by locating dense regions in the data and marking points in less dense areas as noise. It is particularly effective for identifying clusters of arbitrary shapes and handling outliers.</p>
<p>DBSCAN relies on two key parameters:<br>
1. <strong><code>eps</code></strong>: The maximum distance between two points to be considered part of the same neighborhood.<br>
2. <strong><code>min_samples</code></strong>: The minimum number of points required to form a dense region (a cluster).</p>
<p>The algorithm works as follows:<br>
- DBSCAN starts with an unvisited point and determines its neighborhood within the radius <code>eps</code>.<br>
- If the number of points in this neighborhood exceeds <code>min_samples</code>, a cluster is formed.<br>
- The algorithm expands this cluster by iteratively including points within <code>eps</code> distance of other points in the cluster.<br>
- Points that do not belong to any cluster are labeled as <strong>noise</strong> (outliers).</p>
<p>The advantage of DBSCAN is its ability to discover clusters of varying shapes and handle noisy data. However, selecting the optimal values for <code>eps</code> and <code>min_samples</code> is critical to its performance. A common approach is to experiment with different <code>eps</code> values and evaluate the results using a metric like the <strong>Silhouette Score</strong>. Unlike K-Means, DBSCAN does not assign all data points to clusters—points in sparse areas are labeled as noise, which can be useful for identifying outliers.</p>
<p>To apply DBSCAN, we iterate over multiple values of <code>eps</code> to determine which one produces the most cohesive clusters. Visualizing the clusters and assessing the Silhouette Score helps confirm the quality of the results. In the context of the exoneration dataset, DBSCAN is particularly useful for detecting patterns in data with irregular boundaries and identifying anomalies, such as racial groups with limited representation.</p>
<section id="eps-hyper-parameter-tuning" class="level3">
<h3 class="anchored" data-anchor-id="eps-hyper-parameter-tuning">Eps Hyper-Parameter Tuning</h3>
<div id="cell-26" class="cell" data-execution_count="93">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> DBSCAN</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Test different eps values for DBSCAN</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>eps_values <span class="op">=</span> [<span class="fl">0.5</span>, <span class="fl">1.0</span>, <span class="fl">1.5</span>, <span class="fl">2.0</span>]</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> eps <span class="kw">in</span> eps_values:</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    dbscan <span class="op">=</span> DBSCAN(eps<span class="op">=</span>eps, min_samples<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    labels_dbscan <span class="op">=</span> dbscan.fit_predict(X)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>        sil_score <span class="op">=</span> silhouette_score(X, labels_dbscan)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"EPS: </span><span class="sc">{</span>eps<span class="sc">}</span><span class="ss">, Silhouette Score: </span><span class="sc">{</span>sil_score<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span>:</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"EPS: </span><span class="sc">{</span>eps<span class="sc">}</span><span class="ss">, Silhouette Score: Undefined (noise present)"</span>)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Visualize Clusters</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    plot_2D(X, labels_dbscan, <span class="ss">f'DBSCAN Clustering (EPS=</span><span class="sc">{</span>eps<span class="sc">}</span><span class="ss">)'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>EPS: 0.5, Silhouette Score: 0.1970450363722957
EPS: 1.0, Silhouette Score: 0.7808945570083328
EPS: 1.5, Silhouette Score: 0.7808945570083328
EPS: 2.0, Silhouette Score: 0.7808945570083328</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-10-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-10-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-10-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-10-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The <code>eps</code> values for DBSCAN were selected by testing a range of values (<strong>0.5, 1.0, 1.5, and 2.0</strong>) to examine the algorithm’s sensitivity to its key parameter. DBSCAN uses <code>eps</code> (neighborhood radius) and <code>min_samples</code> to define dense regions, with <code>eps</code> controlling the size of these regions. Smaller <code>eps</code> values result in more fragmented clusters, while larger values lead to fewer, broader clusters. These specific <code>eps</code> values were chosen to incrementally observe how the clustering changes and identify the best balance between fragmentation and cohesion.</p>
<ol type="1">
<li><strong>EPS = 0.5</strong>
<ul>
<li>At <code>eps=0.5</code>, the clusters are very fragmented, with many points labeled as <strong>noise</strong> (points not assigned to any cluster). This fragmentation occurs because the neighborhood radius is too small to form large, cohesive clusters.<br>
</li>
<li>The <strong>Silhouette Score</strong> is <strong>0.197</strong>, indicating poor clustering performance and high intra-cluster variance. The data points lack cohesion, and the results do not form meaningful groups.</li>
</ul></li>
<li><strong>EPS = 1.0</strong>
<ul>
<li>With <code>eps=1.0</code>, the clustering improves significantly, and more points are assigned to clusters. The dense regions start to emerge, and noise points are minimized.<br>
</li>
<li>The <strong>Silhouette Score</strong> improves to <strong>0.78</strong>, reflecting better-defined clusters and greater separation between groups. This value of <code>eps</code> represents a good balance where clusters form without excessive noise, and cohesion is maintained.</li>
</ul></li>
<li><strong>EPS = 1.5</strong>
<ul>
<li>At <code>eps=1.5</code>, the clustering remains largely the same as at <code>eps=1.0</code>. Most points are grouped into a single large cluster, while a small number of points remain on the periphery.<br>
</li>
<li>The Silhouette Score remains stable at <strong>0.78</strong>, but the clustering results show diminishing returns—expanding the neighborhood size does not yield additional structure in the data.</li>
</ul></li>
<li><strong>EPS = 2.0</strong>
<ul>
<li>When <code>eps=2.0</code>, the algorithm assigns almost all points to a single large cluster. While this eliminates noise points, it also removes meaningful structure in the data.<br>
</li>
<li>The Silhouette Score remains consistent, but visually, the clustering lacks separation, indicating that <code>eps=2.0</code> is too large for this dataset.</li>
</ul></li>
</ol>
<p>The analysis of DBSCAN results shows that <strong><code>eps=1.0</code></strong> provides the best clustering performance. At this value, the clusters are well-defined, with minimal noise and a Silhouette Score of <strong>0.78</strong>. Smaller values of <code>eps</code> (e.g., 0.5) lead to fragmented clusters with significant noise, while larger values (e.g., 1.5 and 2.0) over-smooth the data, resulting in fewer meaningful clusters. Thus, <code>eps=1.0</code> is chosen as the optimal value for DBSCAN, balancing noise reduction with cohesive clustering.</p>
</section>
</section>
<section id="hierarchical-clustering" class="level2">
<h2 class="anchored" data-anchor-id="hierarchical-clustering">Hierarchical Clustering</h2>
<p>Hierarchical Clustering is an unsupervised machine learning algorithm that builds a hierarchy of clusters through an iterative process. Unlike K-Means, which requires a predefined number of clusters, hierarchical clustering creates a tree-like structure called a <strong>dendrogram</strong> that represents how data points are grouped together at different levels of granularity. The algorithm can be categorized into two main approaches: <strong>agglomerative</strong> (bottom-up) and <strong>divisive</strong> (top-down).</p>
<p>In <strong>agglomerative clustering</strong>, the process starts with each data point as its own cluster. Pairs of clusters are then progressively merged based on their similarity (distance) until a single cluster containing all the data points is formed. The <strong>linkage method</strong> determines how the distance between clusters is measured. For this analysis, <strong>Ward’s linkage</strong> was used, which minimizes the variance between clusters during the merging process. Ward’s method is particularly effective for producing well-balanced and cohesive clusters.</p>
<p>The purpose of hierarchical clustering is to provide a clear visual representation of the clustering process through the dendrogram. By analyzing the dendrogram, we can identify the optimal number of clusters by “cutting” the tree at a certain height where the clusters are most distinct. This approach does not require prior knowledge of the number of clusters, making it particularly useful for exploratory analysis.</p>
<p>To determine the optimal number of clusters, we examined the dendrogram for large vertical distances, which indicate well-separated clusters. Ward’s linkage helps ensure that the resulting clusters have minimal intra-cluster variance, leading to more compact and interpretable groupings. This makes hierarchical clustering especially robust for datasets like ours, where clear group separations are important.</p>
<p>In summary, hierarchical clustering using <strong>Ward’s linkage</strong> provided a flexible and interpretable way to explore the clustering structure. By analyzing the dendrogram and cutting at the appropriate height, we were able to identify meaningful clusters within the data. Compared to K-Means and DBSCAN, hierarchical clustering offers the advantage of visualizing relationships between clusters, making it a valuable method for validating clustering results.</p>
<div id="cell-29" class="cell" data-execution_count="94">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.cluster.hierarchy <span class="im">import</span> dendrogram, linkage</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> AgglomerativeClustering</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create Dendrogram</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>linkage_matrix <span class="op">=</span> linkage(X, method<span class="op">=</span><span class="st">'ward'</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">7</span>))</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>dendrogram(linkage_matrix)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Hierarchical Clustering Dendrogram"</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Samples"</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Distance"</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply Agglomerative Clustering for chosen number of clusters</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>n_clusters <span class="op">=</span> <span class="dv">4</span>  <span class="co"># Replace with the number identified using the dendrogram</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>agglom <span class="op">=</span> AgglomerativeClustering(n_clusters<span class="op">=</span>n_clusters, linkage<span class="op">=</span><span class="st">'ward'</span>)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>labels_agglom <span class="op">=</span> agglom.fit_predict(X)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize Clusters</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>plot_2D(X, labels_agglom, <span class="st">'Hierarchical Clustering Results'</span>)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-11-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="hierarchical-clustering-results-interpretation" class="level3">
<h3 class="anchored" data-anchor-id="hierarchical-clustering-results-interpretation">Hierarchical Clustering Results Interpretation</h3>
<p>The dendrogram above visually represents the clustering hierarchy created using <strong>Ward’s linkage</strong> in hierarchical clustering. The dendrogram displays how data points are merged into clusters at different distances. By cutting the dendrogram at an appropriate height, we identify <strong>four clusters</strong>, which align with the patterns observed in the previous K-Means results.</p>
<p>The <strong>Hierarchical Clustering Results</strong> plot shows the clusters projected onto the first two principal components (PC-1 and PC-2). The data points are color-coded based on their assigned clusters:<br>
1. <strong>Purple Cluster</strong>: Located at the top of the PC-2 axis, this group stands out distinctly, indicating it possesses features that separate it strongly along PC-2.<br>
2. <strong>Yellow Cluster</strong>: Spread vertically across the middle region of PC-1, this group shows moderate cohesion while spanning a wide range along PC-2.<br>
3. <strong>Teal Cluster</strong>: Concentrated toward the bottom-right, this cluster is more compact and defined along PC-1, suggesting strong intra-cluster similarity.<br>
4. <strong>Blue Cluster</strong>: Positioned in the bottom-left region, this group appears dense and well-defined, with relatively low variation in PC-2.</p>
<p>The dendrogram supports the selection of <strong>four clusters</strong>, as indicated by the clear separation into branches at a distance of approximately 20 units. The clusters are well-defined and consistent with the natural groupings in the data, particularly along PC-1 and PC-2. Compared to other clustering methods, hierarchical clustering provides a clear advantage through its hierarchical structure, which allows for exploration of clusters at different levels of granularity.</p>
<p>Overall, hierarchical clustering confirms the patterns observed in the K-Means results, while offering additional insights through the dendrogram. The use of <strong>Ward’s linkage</strong> ensures minimal intra-cluster variance, producing cohesive and interpretable clusters. These clusters align with the underlying structure of the data, reinforcing the conclusions drawn from the PCA and K-Means analysis.</p>
</section>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">Discussion</h2>
<p>The clustering analysis using <strong>K-Means</strong>, <strong>DBSCAN</strong>, and <strong>Hierarchical Clustering</strong> reveals meaningful patterns within the exoneration dataset. Each method identifies groupings that align with the underlying racial categories, which include Black, Hispanic, White, and Native American groups, as analyzed in the earlier exploratory data analysis (EDA).</p>
<ol type="1">
<li><p><strong>K-Means</strong>:<br>
K-Means successfully grouped the data into four clusters, which matches the primary racial groups observed in the dataset. The <strong>Elbow Method</strong> indicated that four clusters were optimal, with clear separations along PC-1 and PC-2. This method produced cohesive and interpretable clusters with minimal noise, effectively capturing the structure of the data. The results align well with the racial breakdown, as groups were well-separated, particularly along PC-1.</p></li>
<li><p><strong>DBSCAN</strong>:<br>
DBSCAN produced varying results depending on the <code>eps</code> parameter. At <strong>eps = 1.0</strong>, the clustering achieved the best balance between cohesion and noise reduction, with a Silhouette Score of <strong>0.78</strong>. However, increasing <code>eps</code> beyond 1.0 caused most data points to merge into a single cluster, reducing the method’s ability to detect finer groupings. DBSCAN was particularly useful for identifying outliers and sparse regions in the dataset, which could correspond to smaller or underrepresented racial groups.</p></li>
<li><p><strong>Hierarchical Clustering</strong>:<br>
Hierarchical clustering using <strong>Ward’s linkage</strong> produced a clear dendrogram, which supported the selection of four clusters. The resulting groupings closely matched those identified by K-Means, with well-defined clusters along PC-1 and PC-2. The dendrogram provided an intuitive visualization of the clustering process, confirming that the data could be effectively separated into four primary clusters.</p></li>
</ol>
<section id="comparison-of-methods" class="level3">
<h3 class="anchored" data-anchor-id="comparison-of-methods">Comparison of Methods</h3>
<p>While all three methods identified meaningful clusters, K-Means and Hierarchical Clustering provided the clearest and most interpretable results. Both methods consistently produced <strong>four clusters</strong>, aligning with the racial categories explored in the dataset. K-Means was computationally efficient and produced well-separated groups, while Hierarchical Clustering offered additional insights through the dendrogram. DBSCAN, although effective at detecting outliers, struggled to form distinct clusters beyond certain parameter values.</p>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>The clustering results demonstrate that the exoneration data can be effectively grouped into four distinct clusters, which align closely with the racial categories: Black, Hispanic, White, and Native American. This finding supports the patterns observed during the EDA, where certain racial groups, such as Asians, were negligible and excluded from analysis. K-Means and Hierarchical Clustering were the most reliable methods, as they produced consistent and interpretable groupings that reflect the underlying data structure. These results have significant real-world implications, highlighting systemic disparities in exoneration outcomes that are strongly correlated with racial identity. Understanding these patterns can inform policy changes and further investigations into racial biases within the criminal justice system.</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>